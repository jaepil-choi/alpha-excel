# Alpha-Database êµ¬í˜„ ê°€ì´ë“œ (Implementation Guide)

## 1. ê°œìš”

ì´ ë¬¸ì„œëŠ” alpha-databaseì˜ **êµ¬í˜„ ì„¸ë¶€ì‚¬í•­**ì„ ë‹¤ë£¹ë‹ˆë‹¤. ì•„í‚¤í…ì²˜ ê²°ì •ì˜ **"ì–´ë–»ê²Œ(How)"**ì— ì´ˆì ì„ ë§ì¶¥ë‹ˆë‹¤.

**í•µì‹¬ ì›ì¹™**: alpha-databaseëŠ” **ë°ì´í„° CRUDì™€ ì¿¼ë¦¬**ì—ë§Œ ì§‘ì¤‘í•©ë‹ˆë‹¤. ë°ì´í„° ìˆ˜ì§‘, ê³„ì‚°, Expression ì§ë ¬í™”ëŠ” ë‹¤ë¥¸ ì»´í¬ë„ŒíŠ¸ì˜ ì±…ì„ì…ë‹ˆë‹¤.

---

## êµ¬í˜„ í˜„í™© (Implementation Status)

### âœ… Phase 1: Config-Driven Data Loading (COMPLETE)
- âœ… ConfigLoader (ë…ë¦½ì )
- âœ… DataLoader (pivoting)
- âœ… DataSource facade
- âœ… BaseReader interface
- âœ… ParquetReader (DuckDB backend)
- âœ… Plugin architecture (register_reader)
- âœ… Alpha-Canvas integration (dependency injection)
- âœ… 100% test coverage (40 tests passing)
- âœ… Experiment 20 validated (identical to old DataLoader)
- âœ… Showcase 18 (integration demonstration)

### âœ… Phase 1.5: Expression Serialization (COMPLETE - 2025-01-23)
- âœ… SerializationVisitor (Expression â†’ JSON dict)
- âœ… DeserializationVisitor (dict â†’ Expression)
- âœ… DependencyExtractor (field lineage)
- âœ… Convenience wrappers (to_dict, from_dict, get_field_dependencies)
- âœ… All 14 Expression types supported
- âœ… 33 comprehensive tests passing
- âœ… Round-trip validation complete
- âœ… Showcase 19 (serialization demonstration)

### ğŸ”„ Phase 2: Data Storage (PLANNED)
- â³ DataWriter (field, alpha, factor)
- â³ MetaTable (catalog)
- â³ LineageTracker (explicit dependencies)

### ğŸ“ Phase 3: Documentation & Migration (PLANNED)
- â³ End-to-end examples
- â³ Migration guide for users
- â³ Performance benchmarks

---

## 2. P0: Config-Driven Data Loading êµ¬í˜„ (âœ… COMPLETE)

### 2.1. ConfigLoader êµ¬í˜„ (ë…ë¦½ì )

**ì¤‘ìš”**: alpha-databaseëŠ” alpha-canvasì˜ configì— ì˜ì¡´í•˜ì§€ **ì•ŠìŠµë‹ˆë‹¤**. ìì²´ ConfigLoaderë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

**Location**: `alpha_database/core/config.py`

**ì±…ì„**:
- `config/data.yaml` íŒŒì¼ íŒŒì‹±
- Field ì •ì˜ (query, time_col, asset_col, value_col, reader type ë“±) ë¡œë“œ
- ì„¤ì • ê²€ì¦

**alpha-canvas configì™€ì˜ ì°¨ì´**:
- alpha-database configëŠ” **ë°ì´í„° ì†ŒìŠ¤**ë§Œ ì •ì˜
- alpha-canvas configëŠ” **ê³„ì‚° ë¡œì§**ë„ í¬í•¨í•  ìˆ˜ ìˆìŒ
- ë‘ configëŠ” ë…ë¦½ì ìœ¼ë¡œ ìœ ì§€

### 2.2. DataLoader êµ¬í˜„

**Location**: `alpha_database/core/data_loader.py`

**ì±…ì„**:
- Long í¬ë§· DataFrameì„ Wide í¬ë§· DataArrayë¡œ í”¼ë²—íŒ…
- (date, security_id, value) â†’ (time, asset) ë³€í™˜
- xarray ì¢Œí‘œ ì„¤ì •

### 2.3. DataSource Facade êµ¬í˜„

```python
# alpha_database/core/data_source.py

from typing import Optional, Dict
import xarray as xr
from .config import ConfigLoader
from .data_loader import DataLoader
from ..readers import BaseReader, ParquetReader, CSVReader, ExcelReader

class DataSource:
    """í†µí•© ë°ì´í„° ì†ŒìŠ¤ facade with plugin support."""
    
    def __init__(self, config_path: str = 'config/data.yaml'):
        self._config = ConfigLoader(config_path)
        self._data_loader = DataLoader()
        self._readers: Dict[str, BaseReader] = self._init_core_readers()
    
    def _init_core_readers(self) -> Dict[str, BaseReader]:
        """Initialize core readers (built-in)."""
        return {
            'parquet': ParquetReader(),
            'csv': CSVReader(),
            'excel': ExcelReader(),
        }
    
    def register_reader(self, reader_type: str, reader: BaseReader):
        """Register custom reader (plugin).
        
        Args:
            reader_type: Name to use in config (e.g., 'fnguide_excel')
            reader: BaseReader instance
        """
        if reader_type in self._readers:
            raise ValueError(f"Reader '{reader_type}' already registered")
        self._readers[reader_type] = reader
    
    def load_field(
        self,
        field_name: str,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None
    ) -> xr.DataArray:
        """í•„ë“œ ë°ì´í„° ë¡œë“œ (Long â†’ Wide ë³€í™˜).
        
        Args:
            field_name: Field name from config
            start_date: Start date (YYYY-MM-DD)
            end_date: End date (YYYY-MM-DD)
        
        Returns:
            xarray.DataArray with (time, asset) dimensions
        """
        # 1. Configì—ì„œ í•„ë“œ ì •ì˜ ë¡œë“œ
        field_config = self._config.get_field(field_name)
        
        # 2. Reader ì„ íƒ (plugin ì§€ì›)
        reader_type = field_config.get('reader', 'parquet')
        if reader_type not in self._readers:
            raise ValueError(
                f"Reader '{reader_type}' not found. "
                f"Available: {list(self._readers.keys())}"
            )
        reader = self._readers[reader_type]
        
        # 3. íŒŒë¼ë¯¸í„° ì¤€ë¹„
        params = {
            'start_date': start_date,
            'end_date': end_date
        }
        
        # 4. Readerë¡œ ë°ì´í„° ë¡œë“œ (Long format)
        df_long = reader.read(
            query=field_config['query'],
            params=params
        )
        
        # 5. Long â†’ Wide í”¼ë²—íŒ…
        data_array = self._data_loader.pivot_to_xarray(
            df=df_long,
            time_col=field_config['time_col'],
            asset_col=field_config['asset_col'],
            value_col=field_config['value_col']
        )
        
        return data_array
```

### 2.4. BaseReader ì¸í„°í˜ì´ìŠ¤

```python
# alpha_database/readers/base.py

from abc import ABC, abstractmethod
import pandas as pd
from typing import Dict, Any

class BaseReader(ABC):
    """ëª¨ë“  Readerì˜ ê³µí†µ ì¸í„°í˜ì´ìŠ¤."""
    
    @abstractmethod
    def read(self, query: str, params: Dict[str, Any]) -> pd.DataFrame:
        """ë°ì´í„° ì½ê¸° (Long í¬ë§· ë°˜í™˜).
        
        Args:
            query: SQL template, file path, or other query format
            params: Runtime parameters (start_date, end_date, etc.)
        
        Returns:
            Long-format DataFrame with (time_col, asset_col, value_col)
        """
        pass
```

### 2.5. ParquetReader êµ¬í˜„

```python
# alpha_database/readers/parquet.py

import duckdb
import pandas as pd
from typing import Dict, Any
from .base import BaseReader

class ParquetReader(BaseReader):
    """DuckDBë¥¼ ì‚¬ìš©í•œ Parquet reader."""
    
    def read(self, query: str, params: Dict[str, Any]) -> pd.DataFrame:
        """Parquet íŒŒì¼ ì¿¼ë¦¬ ì‹¤í–‰.
        
        Args:
            query: SQL with {start_date}, {end_date} placeholders
            params: {'start_date': '2024-01-01', 'end_date': '2024-12-31'}
        
        Returns:
            Long-format DataFrame
        """
        # íŒŒë¼ë¯¸í„° ì¹˜í™˜
        formatted_query = query.format(**params)
        
        # DuckDB ì¿¼ë¦¬ ì‹¤í–‰
        conn = duckdb.connect(':memory:')
        df = conn.execute(formatted_query).fetchdf()
        conn.close()
        
        return df
```

### 2.6. Plugin Architecture

**Core vs Plugin**:

**Core Readers** (alpha-database íŒ¨í‚¤ì§€):
- ParquetReader, CSVReader, ExcelReader
- ëª¨ë“  ì„¤ì¹˜ì— í¬í•¨
- `alpha_database/readers/` ë””ë ‰í† ë¦¬

**Plugin Readers** (ë³„ë„ íŒ¨í‚¤ì§€):
- `alpha-database-fnguide` (official plugin by alpha-database team)
- `alpha-database-bloomberg` (official plugin by alpha-database team)
- User-defined readers (community or custom)

**Official Plugin ì˜ˆì‹œ: alpha-database-fnguide**

```python
# In alpha-database-fnguide package (ë³„ë„ ì„¤ì¹˜)
# Location: alpha_database_fnguide/fnguide_reader.py

from alpha_database.readers import BaseReader
import pandas as pd

class FnGuideExcelReader(BaseReader):
    """FnGuide íŠ¹ìˆ˜ í¬ë§· Excel reader (official plugin)."""
    
    def read(self, query: str, params: Dict[str, Any]) -> pd.DataFrame:
        """FnGuide Excel íŒŒì¼ ì½ê¸°."""
        file_path = query
        
        # FnGuide-specific: header row skip
        header_row = params.get('header_row', 3)
        sheet_name = params.get('sheet_name', 0)
        
        df = pd.read_excel(
            file_path,
            sheet_name=sheet_name,
            header=header_row
        )
        
        # FnGuide-specific: column mapping
        date_col = params.get('date_col', 'ê²°ì‚°ì¼')
        security_col = params.get('security_col', 'ì¢…ëª©ì½”ë“œ')
        
        # Long formatìœ¼ë¡œ ë³€í™˜
        # ... implementation ...
        
        return df_long
```

**Plugin ì‚¬ìš© (User code)**:

```python
# 1. Install plugin
# pip install alpha-database-fnguide

# 2. Import and register
from alpha_database import DataSource
from alpha_database_fnguide import FnGuideExcelReader

ds = DataSource(config_path='config/data.yaml')
ds.register_reader('fnguide_excel', FnGuideExcelReader())

# 3. Use in config
# config/data.yaml:
# fnguide_financials:
#   reader: fnguide_excel
#   file_path: 'data/fnguide.xlsx'
#   header_row: 3
```

**User-defined Reader ì˜ˆì‹œ**:

```python
# User implements custom reader for proprietary format
class MyCustomReader(BaseReader):
    def read(self, query: str, params: dict) -> pd.DataFrame:
        # Custom logic
        ...

# Register
ds.register_reader('my_custom', MyCustomReader())
```

### 2.8. Alpha-Canvas í†µí•© (âœ… IMPLEMENTED)

**Status**: âœ… Complete (Breaking Change - No Backward Compatibility)

**Implementation Date**: 2025-01-23

**Changes Made**:

```python
# In alpha_canvas/core/facade.py (IMPLEMENTED)

class AlphaCanvas:
    def __init__(
        self,
        data_source: 'DataSource',        # MANDATORY
        start_date: str,                  # MANDATORY
        end_date: Optional[str] = None,   # OPTIONAL
        config_dir: str = 'config',
        universe: Optional[Union[Expression, xr.DataArray]] = None
    ):
        """Initialize AlphaCanvas with DataSource injection.
        
        BREAKING CHANGE:
        - data_source: MANDATORY (no default)
        - start_date: MANDATORY (no default)
        - time_index, asset_index: REMOVED
        """
        self._data_source = data_source
        self.start_date = start_date
        self.end_date = end_date
        
        # Lazy panel initialization
        self._panel = None
        
        # Initialize evaluator with DataSource
        empty_ds = xr.Dataset()
        self._evaluator = EvaluateVisitor(empty_ds, data_source=data_source)
        self._evaluator._start_date = start_date
        self._evaluator._end_date = end_date
        
        # ... rest of initialization
```

```python
# In alpha_canvas/core/visitor.py (IMPLEMENTED)

class EvaluateVisitor:
    def __init__(self, data_source_ds: xr.Dataset, data_source=None):
        """Initialize with DataSource.
        
        Args:
            data_source_ds: xarray.Dataset for cached data
            data_source: Optional DataSource from alpha_database
        """
        self._data = data_source_ds
        self._data_source = data_source  # Changed from _data_loader
        self._start_date = None  # Set by AlphaCanvas
        self._end_date = None
    
    def visit_field(self, node: Field) -> xr.DataArray:
        """Field ë…¸ë“œ ë°©ë¬¸ (ë°ì´í„° ë¡œë”©)."""
        # Check cache
        if node.name in self._data:
            result = self._data[node.name]
        else:
            # Load via DataSource (MANDATORY)
            if self._data_source is None:
                raise RuntimeError(
                    f"Field '{node.name}' not found and no DataSource available"
                )
            
            result = self._data_source.load_field(
                node.name,
                start_date=self._start_date,
                end_date=self._end_date
            )
            
            # Cache
            self._data = self._data.assign({node.name: result})
        
        # Apply INPUT MASKING
        if self._universe_mask is not None:
            result = result.where(self._universe_mask, np.nan)
        
        self._cache_signal_weights_and_returns(f"Field_{node.name}", result)
        return result
```

**Migration Example**:

```python
# OLD (REMOVED):
rc = AlphaCanvas(
    time_index=pd.date_range('2024-01-01', periods=252),
    asset_index=['AAPL', 'GOOGL', 'MSFT']
)

# NEW (REQUIRED):
from alpha_database import DataSource

ds = DataSource('config')
rc = AlphaCanvas(
    data_source=ds,
    start_date='2024-01-01',
    end_date='2024-12-31'
)
```

**Validation Results**:
- âœ… 40/40 tests passing (100% success rate)
- âœ… Experiment 20: 100% identical results to old DataLoader
- âœ… Showcase 18: Full integration demonstrated
- âœ… TDD Red-Green cycle complete

---

## 3. P1: Data Storage êµ¬í˜„

### 3.1. DataWriter êµ¬í˜„ (Single Interface)

```python
# alpha_database/writers/data_writer.py

import pandas as pd
import xarray as xr
from pathlib import Path
from typing import List, Dict, Optional, Union, Literal
from datetime import datetime

class DataWriter:
    """ë‹¨ì¼ ë°ì´í„° ì €ì¥ ì¸í„°í˜ì´ìŠ¤ (field, alpha, factor í†µí•©)."""
    
    def __init__(self, base_path: str = './data'):
        self.base_path = Path(base_path)
        self.base_path.mkdir(parents=True, exist_ok=True)
    
    def write(
        self,
        dataset_name: str,
        data: Union[xr.DataArray, Dict[str, xr.DataArray]],
        data_type: Literal['field', 'alpha', 'factor'],
        dependencies: List[str],  # Explicit! User-provided
        metadata: Optional[Dict] = None,
        field_name: Optional[str] = None  # Required for 'field' type
    ):
        """í†µí•© ì €ì¥ ë©”ì„œë“œ.
        
        Args:
            dataset_name: Dataset name (e.g., 'fundamental', 'momentum_v1')
            data: DataArray or dict of DataArrays (for alpha)
            data_type: 'field', 'alpha', or 'factor'
            dependencies: Explicit list of field names (user-provided!)
            metadata: Optional metadata dict
            field_name: Field name (required for 'field' type)
        """
        if data_type == 'field':
            if field_name is None:
                raise ValueError("field_name required for data_type='field'")
            self._write_field(dataset_name, field_name, data, dependencies, metadata)
        
        elif data_type == 'alpha':
            self._write_alpha(dataset_name, data, dependencies, metadata)
        
        elif data_type == 'factor':
            self._write_factor(dataset_name, data, dependencies, metadata)
        
        else:
            raise ValueError(f"Unknown data_type: {data_type}")
    
    def _write_field(
        self,
        dataset_name: str,
        field_name: str,
        data: xr.DataArray,
        dependencies: List[str],
        metadata: Optional[Dict]
    ):
        """í•„ë“œ ì €ì¥ (schema evolution ì§€ì›)."""
        dataset_path = self.base_path / f"{dataset_name}.parquet"
        
        # 1. Convert to long format
        df_new = self._to_long_format(data, field_name)
        
        # 2. Merge with existing data (if exists) - Schema Evolution
        if dataset_path.exists():
            df_existing = pd.read_parquet(dataset_path)
            
            # Upsert: drop old dates, append new data
            df_existing = df_existing[
                ~df_existing['date'].isin(df_new['date'])
            ]
            df_merged = pd.concat([df_existing, df_new], ignore_index=True)
        else:
            df_merged = df_new
        
        # 3. Save
        df_merged.to_parquet(dataset_path, index=False)
        
        # 4. Update meta table
        from ..catalog import MetaTable
        meta = MetaTable()
        meta.upsert(
            name=f"{dataset_name}.{field_name}",
            type='field',
            location=str(dataset_path),
            dependencies=dependencies,  # User-provided!
            description=metadata.get('description', '') if metadata else '',
            tags=metadata.get('tags', []) if metadata else []
        )
    
    def _write_alpha(
        self,
        alpha_id: str,
        data: Dict[str, xr.DataArray],
        dependencies: List[str],
        metadata: Optional[Dict]
    ):
        """ì•ŒíŒŒ ì €ì¥ (auto-versioning).
        
        Args:
            alpha_id: Alpha identifier
            data: {'signal': DataArray, 'weights': DataArray, 'returns': DataArray}
            dependencies: User-provided list
            metadata: Must include 'expression' (pre-serialized by alpha-canvas)
        """
        # 1. Auto-version
        version = self._get_next_version(alpha_id)
        alpha_name = f"{alpha_id}_v{version}"
        alpha_dir = self.base_path / alpha_name
        alpha_dir.mkdir(parents=True, exist_ok=True)
        
        # 2. Save data files
        for key, array in data.items():
            if array is not None:
                array.to_netcdf(alpha_dir / f'{key}.nc')
        
        # 3. Save metadata (including pre-serialized Expression)
        meta_data = {
            'alpha_id': alpha_id,
            'version': version,
            'dependencies': dependencies,  # User-provided!
            'created': datetime.now().isoformat(),
            **(metadata or {})
        }
        
        import json
        with open(alpha_dir / 'metadata.json', 'w') as f:
            json.dump(meta_data, f, indent=2)
        
        # 4. Update meta table
        from ..catalog import MetaTable
        meta = MetaTable()
        meta.upsert(
            name=alpha_name,
            type='alpha',
            location=str(alpha_dir),
            dependencies=dependencies,  # User-provided!
            description=metadata.get('description', '') if metadata else '',
            tags=metadata.get('tags', []) if metadata else [],
            version=version
        )
    
    def _write_factor(
        self,
        factor_id: str,
        data: xr.DataArray,
        dependencies: List[str],
        metadata: Optional[Dict]
    ):
        """íŒ©í„° ìˆ˜ìµë¥  ì €ì¥ (time series)."""
        factor_dir = self.base_path / factor_id
        factor_dir.mkdir(parents=True, exist_ok=True)
        
        # Save time series
        data.to_netcdf(factor_dir / 'returns.nc')
        
        # Save metadata
        meta_data = {
            'factor_id': factor_id,
            'dependencies': dependencies,  # User-provided!
            'created': datetime.now().isoformat(),
            **(metadata or {})
        }
        
        import json
        with open(factor_dir / 'metadata.json', 'w') as f:
            json.dump(meta_data, f, indent=2)
        
        # Update meta table
        from ..catalog import MetaTable
        meta = MetaTable()
        meta.upsert(
            name=factor_id,
            type='factor',
            location=str(factor_dir),
            dependencies=dependencies,  # User-provided!
            description=metadata.get('description', '') if metadata else '',
            tags=metadata.get('tags', []) if metadata else []
        )
    
    def _get_next_version(self, alpha_id: str) -> int:
        """ë‹¤ìŒ ë²„ì „ ë²ˆí˜¸ ì°¾ê¸°."""
        existing = list(self.base_path.glob(f"{alpha_id}_v*"))
        if not existing:
            return 1
        
        versions = [
            int(p.name.split('_v')[1])
            for p in existing
        ]
        return max(versions) + 1
    
    def _to_long_format(self, data: xr.DataArray, field_name: str) -> pd.DataFrame:
        """Wide â†’ Long ë³€í™˜."""
        df = data.to_dataframe(name=field_name).reset_index()
        return df
```

### 3.2. Alpha-Canvas Expression ì§ë ¬í™” (Visitor Pattern)

**ì¤‘ìš”**: ì´ ì½”ë“œëŠ” **alpha-canvas**ì— êµ¬í˜„ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. alpha-databaseëŠ” ì§ë ¬í™”ëœ ê²°ê³¼(dict)ë§Œ ì €ì¥í•©ë‹ˆë‹¤.

**ì„¤ê³„ ì›ì¹™**: **Visitor Pattern**ì„ ì‚¬ìš©í•˜ì—¬ Expression ì§ë ¬í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ëŠ” Expression í´ë˜ìŠ¤ë¥¼ ì§ë ¬í™” ë¡œì§ìœ¼ë¡œë¶€í„° ë¶„ë¦¬í•©ë‹ˆë‹¤.

```python
# In alpha_canvas/core/serialization.py (NEW FILE)

from typing import Dict, Any, List
from .expression import Expression, Field, Constant
from ..ops.timeseries import TsMean, TsAny
from ..ops.crosssection import Rank, CsQuantile
from ..ops.logical import And, Or

class SerializationVisitor:
    """Expression treeë¥¼ dictë¡œ ì§ë ¬í™”í•˜ëŠ” visitor.
    
    ì´ visitorëŠ” Expression treeë¥¼ ìˆœíšŒí•˜ë©° ê° ë…¸ë“œë¥¼
    JSON-serializable dictë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    
    def visit_field(self, node: Field) -> Dict[str, Any]:
        """Field ë…¸ë“œ ì§ë ¬í™”."""
        return {
            'type': 'Field',
            'name': node.name
        }
    
    def visit_constant(self, node: Constant) -> Dict[str, Any]:
        """Constant ë…¸ë“œ ì§ë ¬í™”."""
        return {
            'type': 'Constant',
            'value': node.value
        }
    
    def visit_ts_mean(self, node: TsMean) -> Dict[str, Any]:
        """TsMean ë…¸ë“œ ì§ë ¬í™”."""
        return {
            'type': 'TsMean',
            'child': node.child.accept(self),  # Recursive
            'window': node.window
        }
    
    def visit_ts_any(self, node: TsAny) -> Dict[str, Any]:
        """TsAny ë…¸ë“œ ì§ë ¬í™”."""
        return {
            'type': 'TsAny',
            'child': node.child.accept(self),
            'window': node.window
        }
    
    def visit_rank(self, node: Rank) -> Dict[str, Any]:
        """Rank ë…¸ë“œ ì§ë ¬í™”."""
        return {
            'type': 'Rank',
            'child': node.child.accept(self)
        }
    
    def visit_cs_quantile(self, node: CsQuantile) -> Dict[str, Any]:
        """CsQuantile ë…¸ë“œ ì§ë ¬í™”."""
        return {
            'type': 'CsQuantile',
            'child': node.child.accept(self),
            'q': node.q
        }
    
    # ... implement for all other Expression types
    # (And, Or, Add, Sub, Mul, Div, etc.)


class DeserializationVisitor:
    """Dictë¥¼ Expression treeë¡œ ì—­ì§ë ¬í™”í•˜ëŠ” visitor."""
    
    @staticmethod
    def from_dict(data: Dict[str, Any]) -> Expression:
        """Dictë¥¼ Expressionìœ¼ë¡œ ì¬êµ¬ì„±.
        
        Args:
            data: ì§ë ¬í™”ëœ Expression dict
        
        Returns:
            ì¬êµ¬ì„±ëœ Expression ê°ì²´
        """
        expr_type = data['type']
        
        if expr_type == 'Field':
            return Field(data['name'])
        
        elif expr_type == 'Constant':
            return Constant(data['value'])
        
        elif expr_type == 'TsMean':
            child = DeserializationVisitor.from_dict(data['child'])
            return TsMean(child, window=data['window'])
        
        elif expr_type == 'TsAny':
            child = DeserializationVisitor.from_dict(data['child'])
            return TsAny(child, window=data['window'])
        
        elif expr_type == 'Rank':
            child = DeserializationVisitor.from_dict(data['child'])
            return Rank(child)
        
        elif expr_type == 'CsQuantile':
            child = DeserializationVisitor.from_dict(data['child'])
            return CsQuantile(child, q=data['q'])
        
        # ... handle all Expression types
        
        else:
            raise ValueError(f"Unknown expression type: {expr_type}")


class DependencyExtractor:
    """Expression treeì—ì„œ Field dependenciesë¥¼ ì¶”ì¶œí•˜ëŠ” visitor."""
    
    def __init__(self):
        self.dependencies: List[str] = []
    
    def visit_field(self, node: Field) -> None:
        """Field ë…¸ë“œ ë°©ë¬¸ ì‹œ ì˜ì¡´ì„± ì¶”ê°€."""
        self.dependencies.append(node.name)
    
    def visit_constant(self, node: Constant) -> None:
        """ConstantëŠ” ì˜ì¡´ì„± ì—†ìŒ."""
        pass
    
    def visit_ts_mean(self, node: TsMean) -> None:
        """TsMeanì˜ child ìˆœíšŒ."""
        node.child.accept(self)
    
    def visit_rank(self, node: Rank) -> None:
        """Rankì˜ child ìˆœíšŒ."""
        node.child.accept(self)
    
    # ... implement for all operators
    
    @staticmethod
    def extract(expr: Expression) -> List[str]:
        """Expressionì—ì„œ Field dependencies ì¶”ì¶œ.
        
        Args:
            expr: Expression tree
        
        Returns:
            List of unique field names
        """
        extractor = DependencyExtractor()
        expr.accept(extractor)
        return list(set(extractor.dependencies))  # Deduplicate
```

**Helper Functions for User Convenience**:

```python
# In alpha_canvas/core/expression.py (ADDED)

class Expression:
    """Base Expression class."""
    
    # ... existing methods ...
    
    def to_dict(self) -> Dict[str, Any]:
        """Serialize Expression to dict (convenience wrapper).
        
        Returns:
            JSON-serializable dict representation
        """
        from .serialization import SerializationVisitor
        return self.accept(SerializationVisitor())
    
    @staticmethod
    def from_dict(data: Dict[str, Any]) -> 'Expression':
        """Deserialize dict to Expression (convenience wrapper).
        
        Args:
            data: Serialized Expression dict
        
        Returns:
            Reconstructed Expression object
        """
        from .serialization import DeserializationVisitor
        return DeserializationVisitor.from_dict(data)
    
    def get_field_dependencies(self) -> List[str]:
        """Extract Field dependencies (convenience wrapper).
        
        Returns:
            List of field names this Expression depends on
        """
        from .serialization import DependencyExtractor
        return DependencyExtractor.extract(self)
```

**ì´ì **:
1. âœ… **Separation of Concerns**: Expression í´ë˜ìŠ¤ëŠ” ìˆœìˆ˜ ë„ë©”ì¸ ë¡œì§ì—ë§Œ ì§‘ì¤‘
2. âœ… **í™•ì¥ì„±**: ìƒˆë¡œìš´ ì§ë ¬í™” í˜•ì‹(YAML, Binary) ì¶”ê°€ ì‹œ ìƒˆ visitorë§Œ ì¶”ê°€
3. âœ… **ìœ ì§€ë³´ìˆ˜ì„±**: ì§ë ¬í™” ë¡œì§ì´ í•œ ê³³ì— ì§‘ì¤‘ë¨
4. âœ… **í…ŒìŠ¤íŠ¸ ìš©ì´ì„±**: Visitorë¥¼ ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
5. âœ… **ê¸°ì¡´ ì¸í”„ë¼ í™œìš©**: EvaluateVisitorì™€ ë™ì¼í•œ íŒ¨í„´ ì‚¬ìš©

### 3.3. ì‚¬ìš© íŒ¨í„´

```python
# User code (in Jupyter notebook)

from alpha_canvas import AlphaCanvas, Field, Expression
from alpha_canvas.ops import TsMean, Rank
from alpha_canvas.portfolio import DollarNeutralScaler
from alpha_database import DataWriter

# 1. Create alpha in alpha-canvas
rc = AlphaCanvas(start_date='2024-01-01', end_date='2024-12-31')
expr = Rank(TsMean(Field('returns'), window=5))
result = rc.evaluate(expr, scaler=DollarNeutralScaler())

# 2. alpha-canvasê°€ Expression ì§ë ¬í™” ë° ì˜ì¡´ì„± ì¶”ì¶œ
expr_dict = expr.to_dict()  # alpha-canvas method!
dependencies = expr.get_field_dependencies()  # alpha-canvas method!
# Returns: ['returns']

# 3. alpha-databaseì— ì €ì¥ (pre-serialized data)
writer = DataWriter(base_path='./alphas')

writer.write(
    dataset_name='momentum_ma5_rank',
    data_type='alpha',
    data={
        'signal': rc.get_signal(step=2),
        'weights': rc.get_weights(step=2),
        'returns': rc.get_port_return(step=2)
    },
    dependencies=dependencies,  # From alpha-canvas!
    metadata={
        'expression': expr_dict,  # Pre-serialized by alpha-canvas!
        'expression_str': str(expr),
        'description': 'Momentum strategy with 5-day MA and rank',
        'tags': ['momentum', 'mean-reversion']
    }
)
```

### 3.4. Meta Table êµ¬í˜„

```python
# alpha_database/catalog/meta_table.py

import pandas as pd
from pathlib import Path
from typing import Optional, List, Dict
from datetime import datetime

class MetaTable:
    """ì¤‘ì•™ ë©”íƒ€ë°ì´í„° ë ˆì§€ìŠ¤íŠ¸ë¦¬."""
    
    def __init__(self, path: str = './data/meta_table.parquet'):
        self.path = Path(path)
        self.path.parent.mkdir(parents=True, exist_ok=True)
        
        if not self.path.exists():
            self._init_table()
    
    def _init_table(self):
        """ë¹ˆ í…Œì´ë¸” ì´ˆê¸°í™”."""
        df = pd.DataFrame({
            'name': pd.Series(dtype='str'),
            'type': pd.Series(dtype='str'),
            'location': pd.Series(dtype='str'),
            'dependencies': pd.Series(dtype='object'),  # List[str]
            'created': pd.Series(dtype='datetime64[ns]'),
            'updated': pd.Series(dtype='datetime64[ns]'),
            'description': pd.Series(dtype='str'),
            'tags': pd.Series(dtype='object'),  # List[str]
            'version': pd.Series(dtype='Int64')
        })
        df.to_parquet(self.path, index=False)
    
    def upsert(
        self,
        name: str,
        type: str,
        location: str,
        dependencies: List[str],  # User-provided!
        description: str = '',
        tags: List[str] = None,
        version: Optional[int] = None
    ):
        """ë ˆì½”ë“œ ì¶”ê°€ ë˜ëŠ” ì—…ë°ì´íŠ¸."""
        df = pd.read_parquet(self.path)
        
        now = datetime.now()
        
        # Check if exists
        existing = df[df['name'] == name]
        
        if len(existing) > 0:
            # Update
            df.loc[df['name'] == name, 'updated'] = now
            df.loc[df['name'] == name, 'location'] = location
            df.loc[df['name'] == name, 'dependencies'] = [dependencies]
            df.loc[df['name'] == name, 'description'] = description
            if tags:
                df.loc[df['name'] == name, 'tags'] = [tags]
        else:
            # Insert
            new_row = {
                'name': name,
                'type': type,
                'location': location,
                'dependencies': [dependencies],
                'created': now,
                'updated': now,
                'description': description,
                'tags': [tags] if tags else [[]],
                'version': version
            }
            df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)
        
        df.to_parquet(self.path, index=False)
    
    def query(self, type: Optional[str] = None, **kwargs) -> pd.DataFrame:
        """í…Œì´ë¸” ì¿¼ë¦¬."""
        df = pd.read_parquet(self.path)
        
        if type:
            df = df[df['type'] == type]
        
        return df
    
    def find_dependents(self, field_name: str) -> List[str]:
        """íŠ¹ì • í•„ë“œì— ì˜ì¡´í•˜ëŠ” ëª¨ë“  ì—”í‹°í‹° ì°¾ê¸°."""
        df = pd.read_parquet(self.path)
        
        dependents = []
        for idx, row in df.iterrows():
            if field_name in row['dependencies']:
                dependents.append(row['name'])
        
        return dependents
    
    def search(self, tags__contains: Optional[str] = None) -> pd.DataFrame:
        """Tag ê²€ìƒ‰."""
        df = pd.read_parquet(self.path)
        
        if tags__contains:
            mask = df['tags'].apply(
                lambda tags: tags__contains in tags if tags else False
            )
            df = df[mask]
        
        return df
```

---

## 4. P2: Lineage Tracking êµ¬í˜„ (Explicit Dependencies Only)

### 4.1. LineageTracker êµ¬í˜„

```python
# alpha_database/catalog/lineage.py

import pandas as pd
from pathlib import Path
from typing import List, Set, Dict
from .meta_table import MetaTable

class LineageTracker:
    """ì˜ì¡´ì„± ê·¸ë˜í”„ ì¶”ì  (user-provided dependencies only)."""
    
    def __init__(self, meta_table_path: str = './data/meta_table.parquet'):
        self.meta = MetaTable(meta_table_path)
        self.df = pd.read_parquet(meta_table_path)
    
    def find_dependents(self, field_name: str) -> List[str]:
        """íŠ¹ì • í•„ë“œì— ì˜ì¡´í•˜ëŠ” ëª¨ë“  ì—”í‹°í‹° (1-level).
        
        Note:
            User-provided dependenciesë§Œ ì‚¬ìš© (Expression íŒŒì‹± ì—†ìŒ)
        """
        return self.meta.find_dependents(field_name)
    
    def find_dependencies(self, entity_name: str) -> List[str]:
        """íŠ¹ì • ì—”í‹°í‹°ê°€ ì˜ì¡´í•˜ëŠ” ëª¨ë“  í•„ë“œ (1-level, explicit only).
        
        Args:
            entity_name: Entity to analyze
        
        Returns:
            List of field names (user-provided dependencies only)
        """
        row = self.df[self.df['name'] == entity_name]
        if len(row) == 0:
            return []
        return row.iloc[0]['dependencies']
    
    def get_impact(self, field_name: str) -> List[str]:
        """Impact analysis: ë°ì´í„° ë³€ê²½ ì‹œ ì˜í–¥ë°›ëŠ” ì—”í‹°í‹°.
        
        Returns:
            List of entity names that depend on field_name
        """
        # Simple 1-level impact (no recursive traversal)
        return self.find_dependents(field_name)
    
    def get_lineage(self, entity_name: str) -> Dict[str, List[str]]:
        """ì „ì²´ ê³„ë³´ ê²½ë¡œ ë°˜í™˜ (explicit links only).
        
        Returns:
            Dict mapping entity to its user-provided dependencies
        """
        lineage = {}
        lineage[entity_name] = self.find_dependencies(entity_name)
        return lineage
```

**ì¤‘ìš”**: ì´ êµ¬í˜„ì€ ë§¤ìš° ë‹¨ìˆœí•©ë‹ˆë‹¤. Expression íŒŒì‹±ì´ë‚˜ ì¬ê·€ì  ì˜ì¡´ì„± ê³„ì‚°ì„ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì‚¬ìš©ìê°€ `write()` ì‹œ ì œê³µí•œ `dependencies`ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

---

## 5. í…ŒìŠ¤íŠ¸ ì „ëµ

### 5.1. Unit Tests

**DataSource**:
- Config íŒŒì‹± í…ŒìŠ¤íŠ¸
- Reader ì„ íƒ ë¡œì§ í…ŒìŠ¤íŠ¸
- Plugin ë“±ë¡ í…ŒìŠ¤íŠ¸
- íŒŒë¼ë¯¸í„° ì¹˜í™˜ í…ŒìŠ¤íŠ¸

**Readers**:
- Long í¬ë§· ë°˜í™˜ ê²€ì¦
- íŒŒë¼ë¯¸í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
- ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸

**DataWriter**:
- ë°ì´í„° ì €ì¥ ê²€ì¦ (field, alpha, factor)
- Schema evolution í…ŒìŠ¤íŠ¸
- Meta table ì—…ë°ì´íŠ¸ ê²€ì¦
- Versioning í…ŒìŠ¤íŠ¸

**MetaTable**:
- CRUD ì‘ì—… í…ŒìŠ¤íŠ¸
- ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
- Dependency ê²€ìƒ‰ í…ŒìŠ¤íŠ¸

### 5.2. Integration Tests

**Alpha-Canvas í†µí•©**:
- DataSource injection í…ŒìŠ¤íŠ¸
- Field ë¡œë”© ê²€ì¦
- Backward compatibility ê²€ì¦

**End-to-End**:
```python
def test_e2e_alpha_save_and_load():
    # 1. Create alpha in alpha-canvas
    rc = AlphaCanvas(data_source=ds, ...)
    expr = Rank(TsMean(Field('returns'), window=5))
    result = rc.evaluate(expr, scaler=scaler)
    
    # 2. alpha-canvasê°€ ì§ë ¬í™”
    expr_dict = expr.to_dict()
    deps = expr.get_field_dependencies()
    
    # 3. alpha-databaseì— ì €ì¥
    writer.write(..., dependencies=deps, metadata={'expression': expr_dict})
    
    # 4. ì¬ë¡œë“œ ë° ì¬í˜„
    reader = DataReader(...)
    alpha_data = reader.read('momentum_v1')
    expr_reconstructed = Expression.from_dict(alpha_data['metadata']['expression'])
    
    # 5. ì¬ì‹¤í–‰
    result2 = rc.evaluate(expr_reconstructed, scaler=scaler)
    
    # 6. ê²€ì¦
    assert result.equals(result2)
```

### 5.3. Performance Tests

**DataLoader**:
- ëŒ€ìš©ëŸ‰ Parquet ì¿¼ë¦¬ ì„±ëŠ¥
- í”¼ë²—íŒ… ì„±ëŠ¥
- DuckDB ìµœì í™” ê²€ì¦

**DataWriter**:
- ëŒ€ìš©ëŸ‰ ì €ì¥ ì„±ëŠ¥
- Schema evolution ì„±ëŠ¥

---

## 6. ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ

### 6.1. Phase 1: DataSource êµ¬í˜„

1. `alpha_database/core/` êµ¬í˜„
2. Reader íŒ¨ë°€ë¦¬ êµ¬í˜„
3. Plugin ì‹œìŠ¤í…œ êµ¬í˜„
4. í…ŒìŠ¤íŠ¸ ì‘ì„± ë° ê²€ì¦

### 6.2. Phase 2: Alpha-Canvas í†µí•©

1. `AlphaCanvas.__init__()` ìˆ˜ì • (data_source íŒŒë¼ë¯¸í„°)
2. `EvaluateVisitor.visit_field()` ìˆ˜ì •
3. **Expression ì§ë ¬í™” ë©”ì„œë“œ ì¶”ê°€** (`to_dict()`, `from_dict()`, `get_field_dependencies()`)
4. Backward compatibility ìœ ì§€
5. ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ëª¨ë‘ í†µê³¼ í™•ì¸

### 6.3. Phase 3: Storage êµ¬í˜„

1. DataWriter êµ¬í˜„ (ë‹¨ì¼ ì¸í„°í˜ì´ìŠ¤)
2. Meta Table êµ¬í˜„
3. LineageTracker êµ¬í˜„ (explicit only)
4. í…ŒìŠ¤íŠ¸ ì‘ì„±

### 6.4. Phase 4: ê²€ì¦ ë° ë§ˆì´ê·¸ë ˆì´ì…˜

1. End-to-end í…ŒìŠ¤íŠ¸
2. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
3. alpha-canvas ë‚´ì¥ loader ì œê±° (Phase 3)

---

## 7. ì•„í‚¤í…ì²˜ ë‹¨ìˆœí™” ìš”ì•½

### ì œì™¸ëœ êµ¬í˜„

**1. Data Fetching (CCXTFetcher, APIFetcher)**
- **ì´ìœ **: ETL íŒŒì´í”„ë¼ì¸ì˜ ì—­í• 
- **ëŒ€ì•ˆ**: ì‚¬ìš©ìê°€ ì§ì ‘ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

**2. Expression Auto-Serialization**
- **ì´ìœ **: alpha-canvasì˜ ì±…ì„
- **ëŒ€ì•ˆ**: alpha-canvasê°€ `to_dict()` ë©”ì„œë“œ ì œê³µ, alpha-databaseëŠ” ê²°ê³¼ë§Œ ì €ì¥

**3. Automatic Dependency Extraction**
- **ì´ìœ **: Expression íŒŒì‹±ì€ ë³µì¡í•˜ê³  ê°•ê²°í•© ìœ ë°œ
- **ëŒ€ì•ˆ**: ì‚¬ìš©ìê°€ `write()` ì‹œ ëª…ì‹œì ìœ¼ë¡œ `dependencies` ì œê³µ, alpha-canvasê°€ `get_field_dependencies()` helper ì œê³µ

**4. Triple-Cache Management**
- **ì´ìœ **: ê³„ì‚° ìºì‹œëŠ” alpha-canvasì˜ ì±…ì„
- **ëŒ€ì•ˆ**: alpha-canvasê°€ ìºì‹œ ê´€ë¦¬, alpha-databaseëŠ” ìµœì¢… ê²°ê³¼ë§Œ ì €ì¥

**5. Specialized Readers (Built-in FnGuide, Bloomberg ë“±)**
- **ì´ìœ **: ìœ ì§€ë³´ìˆ˜ ë¶€ë‹´, í™•ì¥ì„± ì œí•œ
- **ëŒ€ì•ˆ**: Plugin architecture

### êµ¬í˜„ëœ í•µì‹¬ ê¸°ëŠ¥

1. âœ… **DataSource Facade** (with plugin support)
2. âœ… **Core Readers** (Parquet, CSV, Excel)
3. âœ… **Single DataWriter** (field, alpha, factor í†µí•©)
4. âœ… **Meta Table** (simple Parquet table)
5. âœ… **LineageTracker** (explicit dependencies only)

---

## 8. ë‹¤ìŒ ë‹¨ê³„ (Next Steps)

### 8.1. Phase 1 ì™„ë£Œ ê²€ì¦ âœ…

**ì™„ë£Œëœ ì‘ì—…**:
- âœ… ConfigLoader, DataLoader, DataSource êµ¬í˜„
- âœ… BaseReader, ParquetReader êµ¬í˜„
- âœ… Plugin architecture êµ¬í˜„
- âœ… AlphaCanvas í†µí•© (breaking change)
- âœ… 40 tests passing (100% success rate)
- âœ… Experiment 20 validated
- âœ… Showcase 17 & 18 completed
- âœ… Committed: `feat: integrate DataSource into AlphaCanvas`

### 8.2. Phase 2 ì‹œì‘ ì „ ì •ë¦¬ ì‘ì—…

**ê¶Œì¥ ì‘ì—… ìˆœì„œ**:

1. **Old DataLoader ì œê±°** (alpha-canvas ë‚´ë¶€):
   - `src/alpha_canvas/core/data_loader.py` ì‚­ì œ
   - ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ (DataSourceë¡œ ì™„ì „ ëŒ€ì²´)
   - ëª¨ë“  í…ŒìŠ¤íŠ¸ ì—¬ì „íˆ í†µê³¼í•˜ëŠ”ì§€ í™•ì¸

**Note**: Remaining showcases (1-16) ì—…ë°ì´íŠ¸ëŠ” ì„ íƒì‚¬í•­ì…ë‹ˆë‹¤. ShowcasesëŠ” temporal completenessë¥¼ ìœ„í•œ ê²ƒì´ë©°, í•„ìˆ˜ ì‘ì—…ì´ ì•„ë‹™ë‹ˆë‹¤. README.md ì—…ë°ì´íŠ¸ë„ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

### 8.3. Phase 2: Data Storage êµ¬í˜„ (ë‹¤ìŒ í° ì‘ì—…)

**Phase 2 ëª©í‘œ**:
- DataWriter êµ¬í˜„ (field, alpha, factor ì €ì¥)
- MetaTable êµ¬í˜„ (ì¹´íƒˆë¡œê·¸)
- LineageTracker êµ¬í˜„ (ì˜ì¡´ì„± ì¶”ì )

**Phase 2 ì „ì œì¡°ê±´**:
- âœ… **Expression ì§ë ¬í™”** (COMPLETED - 2025-01-23):
  - âœ… `SerializationVisitor` (Expression â†’ dict)
  - âœ… `DeserializationVisitor` (dict â†’ Expression)
  - âœ… `DependencyExtractor` (Extract Field dependencies)
  - âœ… ëª¨ë“  14ê°œ Expression íƒ€ì… ì§€ì› (Field, Constant, TsMean, TsAny, Rank, CsQuantile, comparison operators, logical operators)
  - âœ… Convenience wrappers: `Expression.to_dict()`, `Expression.from_dict()`, `Expression.get_field_dependencies()`
  - âœ… 33 tests passing, round-trip validation complete
  - âœ… Showcase 19 demonstrating all capabilities

**Phase 2 ì‹œì‘ ì „ í™•ì¸ì‚¬í•­**:
- [x] Phase 1 ì™„ì „íˆ ê²€ì¦ë¨
- [ ] Old DataLoader ì œê±°ë¨
- [ ] Documentation ì—…ë°ì´íŠ¸ë¨
- [x] Expression ì§ë ¬í™” êµ¬í˜„ ì™„ë£Œë¨ (2025-01-23)

### 8.4. ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì‘ì—… (Quick Wins)

**Option 1: Cleanup & Documentation** (ì¶”ì²œ)
1. Old DataLoader ì œê±°
2. Showcase ì—…ë°ì´íŠ¸ (1-16)
3. README ì—…ë°ì´íŠ¸
4. Commit: `chore: remove old DataLoader and update showcases`

**Option 2: Expression Serialization** âœ… COMPLETED (2025-01-23)
1. âœ… `SerializationVisitor` êµ¬í˜„ (Expression â†’ dict)
2. âœ… `DeserializationVisitor` êµ¬í˜„ (dict â†’ Expression)
3. âœ… `DependencyExtractor` êµ¬í˜„ (Field dependencies)
4. âœ… ëª¨ë“  14ê°œ Expression íƒ€ì…ì— visitor ë©”ì„œë“œ ì¶”ê°€
5. âœ… Convenience wrappers êµ¬í˜„ (`to_dict()`, `from_dict()`, `get_field_dependencies()`)
6. âœ… Unit tests ì‘ì„± (33 tests passing)
7. âœ… Commit: `feat: implement visitor-based Expression serialization`

**Option 3: Phase 2 ì‹œì‘** (ë°”ë¡œ ì§„í–‰)
1. DataWriter êµ¬í˜„ ì‹œì‘
2. Field ì €ì¥ ê¸°ëŠ¥ë¶€í„° êµ¬í˜„
3. MetaTable skeleton êµ¬í˜„
4. TDDë¡œ ì§„í–‰

---

**Implementation Version**: 2.0 (Simplified)  
**Last Updated**: 2025-01-23 (Phase 1 Complete)  
**Core Principle**: **alpha-databaseëŠ” ë°ì´í„° CRUDì™€ ì¿¼ë¦¬ì—ë§Œ ì§‘ì¤‘í•©ë‹ˆë‹¤.**
