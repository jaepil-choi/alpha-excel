# 3. Implementation Guide

ì´ ë¬¸ì„œëŠ” alpha-canvasì˜ êµ¬ì²´ì ì¸ êµ¬í˜„ ë°©ë²•ë¡ , ì¸í„°í˜ì´ìŠ¤ ì„¤ê³„, ê·¸ë¦¬ê³  ê°œë°œ í‘œì¤€ì„ ì •ì˜í•©ë‹ˆë‹¤.

## 3.0. êµ¬í˜„ í˜„í™© ìš”ì•½ (Implementation Status Summary)

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-01-23

### âœ… **ì™„ë£Œëœ í•µì‹¬ ê¸°ëŠ¥**

| Feature | Status | Location | Description |
|---------|--------|----------|-------------|
| **F1: Data Retrieval** | âœ… DONE | `core/config.py`, `core/data_loader.py` | Config-driven Parquet data loading |
| **F2: CsQuantile** | âœ… DONE | `ops/classification.py` | Independent/dependent sort bucketing |
| **F2: Signal Assignment** | âœ… DONE | `core/expression.py` | Lazy evaluation `expr[mask] = value` |
| **F3: Triple-Cache** | âœ… DONE | `core/visitor.py` | Signal, weight, port_return caching |
| **F5: Portfolio (Weights)** | âœ… DONE | `portfolio/` | Strategy pattern weight scalers |
| **F6: Backtesting** | âœ… DONE | `core/visitor.py`, `core/facade.py` | Shift-mask workflow, position returns |
| **Universe Masking** | âœ… DONE | `core/facade.py`, `core/visitor.py` | Double masking strategy |
| **Boolean Expressions** | âœ… DONE | `core/expression.py`, `ops/logical.py` | Comparison + logical operators |
| **DataAccessor** | âœ… DONE | `utils/accessor.py` | `rc.data['field']` interface |

### ğŸ“Š **í…ŒìŠ¤íŠ¸ ë° ê²€ì¦ í˜„í™©**

- âœ… **176ê°œ** ë‹¨ìœ„/í†µí•© í…ŒìŠ¤íŠ¸ (`tests/`)
- âœ… **18ê°œ** ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸ (`experiments/`)
- âœ… **16ê°œ** showcase ì˜ˆì œ (`showcase/`)
- âœ… Fama-French 2Ã—3 factor ì¬í˜„ ê²€ì¦
- âœ… ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (vectorized operations)

### ğŸ“‹ **alpha-labìœ¼ë¡œ ì´ê´€ ì˜ˆì •**

alpha-canvasëŠ” **compute engine**ì— ì§‘ì¤‘í•˜ë©°, ë‹¤ìŒ ê¸°ëŠ¥ë“¤ì€ **alpha-lab íŒ¨í‚¤ì§€**ì—ì„œ ì œê³µë©ë‹ˆë‹¤:
- PnL ë¶„ì„ ë° ì„±ê³¼ ì§€í‘œ (Sharpe, drawdown, turnover, etc.)
- ì‹œê°í™” (heatmaps, PnL curves, attribution charts)
- Step ë¹„êµ ë° ì§„í™” ë¶„ì„
- Factor exposure ë° IC ë¶„ì„

**ì„¤ê³„ ê·¼ê±°**: PRD Section 1.9 ë° 2.1ì— ëª…ì‹œëœ loose coupling ì•„í‚¤í…ì²˜ ì›ì¹™ ì¤€ìˆ˜

---

## 3.1. í”„ë¡œì íŠ¸ êµ¬ì¡°

```text
alpha-canvas/
â”œâ”€â”€ config/                      # íƒ€ì…ë³„ ì„¤ì • íŒŒì¼
â”‚   â”œâ”€â”€ data.yaml               # ë°ì´í„° í•„ë“œ ì •ì˜
â”‚   â”œâ”€â”€ db.yaml                 # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì • (ì„ íƒì )
â”‚   â””â”€â”€ compute.yaml            # ê³„ì‚° ê´€ë ¨ ì„¤ì • (ì„ íƒì )
â”œâ”€â”€ src/
â”‚   â””â”€â”€ alpha_canvas/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ core/
â”‚       â”‚   â”œâ”€â”€ facade.py       # AlphaCanvas (rc) í¼ì‚¬ë“œ í´ë˜ìŠ¤
â”‚       â”‚   â”œâ”€â”€ expression.py   # Expression ì»´í¬ì§“ íŠ¸ë¦¬
â”‚       â”‚   â”œâ”€â”€ visitor.py      # EvaluateVisitor íŒ¨í„´ (íƒ€ì… ê²€ì‚¬ í¬í•¨)
â”‚       â”‚   â””â”€â”€ config.py       # ConfigLoader
â”‚       â”œâ”€â”€ ops/                # ì—°ì‚°ì (ts_mean, rank, etc.)
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ timeseries.py   # ts_mean, ts_sum, etc. (ë‹¤í˜•ì„± ì—°ì‚°ì)
â”‚       â”‚   â”œâ”€â”€ crosssection.py # cs_rank ë“± (Panel ì „ìš© ì—°ì‚°ì)
â”‚       â”‚   â”œâ”€â”€ classification.py # cs_quantile, cs_cut (ë¶„ë¥˜ê¸°/ì¶• ìƒì„±)
â”‚       â”‚   â”œâ”€â”€ transform.py    # group_neutralize, etc.
â”‚       â”‚   â””â”€â”€ tensor.py       # ë¯¸ë˜ í™•ì¥ìš© (MVPì—ì„œëŠ” ë¹„ì–´ìˆìŒ)
â”‚       â”œâ”€â”€ portfolio/          # í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„± (weight scaling)
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ base.py         # WeightScaler ì¶”ìƒ ë² ì´ìŠ¤ í´ë˜ìŠ¤
â”‚       â”‚   â””â”€â”€ strategies.py   # GrossNetScaler, DollarNeutralScaler, LongOnlyScaler
â”‚       â””â”€â”€ utils/
â”‚           â”œâ”€â”€ accessor.py     # Property ì ‘ê·¼ì (data accessor)
â”‚           â””â”€â”€ mask.py         # ë§ˆìŠ¤í¬ í—¬í¼
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ alpha_lab/              # ë¶„ì„ íŒ¨í‚¤ì§€ (ë³„ë„ íŒ¨í‚¤ì§€, ë¯¸êµ¬í˜„)
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ alpha_database/         # ì˜ì†ì„± íŒ¨í‚¤ì§€ (ë³„ë„ íŒ¨í‚¤ì§€, ë¯¸êµ¬í˜„)
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ experiments/                # ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ tests/                      # í…ŒìŠ¤íŠ¸
â””â”€â”€ docs/
    â””â”€â”€ vibe_coding/
        â”œâ”€â”€ prd.md
        â”œâ”€â”€ architecture.md
        â””â”€â”€ implementation.md   # ì´ ë¬¸ì„œ
```

## 3.2. í•µì‹¬ ì¸í„°í˜ì´ìŠ¤ ì„¤ê³„

### 3.2.1. ì´ˆê¸°í™” ë° ì„¤ì •

```python
from alpha_canvas import AlphaCanvas

# config/ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  YAML íŒŒì¼ì„ ìë™ ë¡œë“œ
rc = AlphaCanvas()

# ë˜ëŠ” íŠ¹ì • config ë””ë ‰í† ë¦¬ ì§€ì •
rc = AlphaCanvas(config_dir='./custom_config')
```

**êµ¬í˜„ ìš”êµ¬ì‚¬í•­:**

- `AlphaCanvas.__init__()` ë‚´ë¶€ì—ì„œ `ConfigLoader`ë¥¼ ìƒì„±í•˜ê³  `config/` ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  `.yaml` íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
- `ConfigLoader`ëŠ” `data.yaml`, `db.yaml` ë“±ì„ ê°ê° íŒŒì‹±í•˜ì—¬ ë‚´ë¶€ dictì— ì €ì¥í•©ë‹ˆë‹¤.

### 3.2.2. ì½”ì–´ ë°ì´í„° ëª¨ë¸ êµ¬í˜„ (Core Data Model Implementation)

#### A. `AlphaCanvas.add_data()` êµ¬í˜„ (`facade.py`)

```python
def add_data(self, name: str, data: Union[Expression, xr.DataArray]) -> None:
    """ë°ì´í„° ë³€ìˆ˜ë¥¼ Datasetì— ì¶”ê°€ (Expression ë˜ëŠ” DataArray ì§€ì›)"""
    
    # Case 1: Expression í‰ê°€ (ì¼ë°˜ì ì¸ ê²½ë¡œ)
    if isinstance(data, Expression):
        self.rules[name] = data  # Expression ì €ì¥ (ì¬í‰ê°€ ê°€ëŠ¥í•˜ë„ë¡)
        result_array = self._evaluator.evaluate(data)  # Visitorë¡œ í‰ê°€
        self.db = self.db.assign({name: result_array})  # data_varsì— ì¶”ê°€
    
    # Case 2: DataArray ì§ì ‘ ì£¼ì… (Open Toolkit: Inject)
    elif isinstance(data, xr.DataArray):
        # ì™¸ë¶€ì—ì„œ ìƒì„±í•œ ë°ì´í„° ì£¼ì… (Visitor ê±´ë„ˆë›°ê¸°)
        self.db = self.db.assign({name: data})
    
    else:
        raise TypeError(f"data must be Expression or DataArray, got {type(data)}")
```

**í•µì‹¬ ì‚¬í•­:**

- `xarray.Dataset.assign()`ì„ ì‚¬ìš©í•˜ì—¬ Data Variableë¡œ ì¶”ê°€
- `Expression`ê³¼ `DataArray` ëª¨ë‘ ì§€ì› (ì˜¤ë²„ë¡œë”©)
- Open Toolkit ì² í•™: ì™¸ë¶€ ê³„ì‚° ê²°ê³¼ë¥¼ seamlessly inject

#### B. `rc.db` í”„ë¡œí¼í‹° (Open Toolkit: Eject)

```python
@property
def db(self) -> xr.Dataset:
    """ìˆœìˆ˜ xarray.Dataset ë°˜í™˜ (Jupyter ejectìš©)"""
    return self._dataset  # ë‚´ë¶€ Datasetì„ ê·¸ëŒ€ë¡œ ë…¸ì¶œ
```

**í•µì‹¬ ì‚¬í•­:**

- ë˜í•‘ ì—†ì´ ìˆœìˆ˜ `xarray.Dataset` ë°˜í™˜
- ì‚¬ìš©ìëŠ” `pure_ds = rc.db`ë¡œ êº¼ë‚´ì„œ scipy/statsmodels ì‚¬ìš© ê°€ëŠ¥

#### C. ìœ ë‹ˆë²„ìŠ¤ ë§ˆìŠ¤í‚¹ (Universe Masking) âœ… **IMPLEMENTED**

**ìš”êµ¬ì‚¬í•­**: ì´ˆê¸°í™” ì‹œ ìœ ë‹ˆë²„ìŠ¤ë¥¼ ì„¤ì •í•˜ê³ , ëª¨ë“  ë°ì´í„°ì™€ ì—°ì‚°ì— ìë™ ì ìš©

```python
# AlphaCanvas ì´ˆê¸°í™” with universe
rc = AlphaCanvas(
    start_date='2024-01-01',
    end_date='2024-12-31',
    universe=price > 5.0  # Boolean DataArray
)

# ë˜ëŠ” Expressionìœ¼ë¡œ ì„¤ì •
rc = AlphaCanvas(
    start_date='2024-01-01',
    end_date='2024-12-31',
    universe=Field('univ500')  # Field Expression (ë¯¸ë˜ í™•ì¥)
)

# ìœ ë‹ˆë²„ìŠ¤ í™•ì¸ (read-only)
print(f"Universe coverage: {rc.universe.sum().values} positions")
```

**êµ¬í˜„ ì„¸ë¶€ì‚¬í•­**:

**1. AlphaCanvasì— universe íŒŒë¼ë¯¸í„° ì¶”ê°€**:
```python
class AlphaCanvas:
    def __init__(
        self,
        config_dir='config',
        start_date=None,
        end_date=None,
        time_index=None,
        asset_index=None,
        universe: Optional[Union[Expression, xr.DataArray]] = None  # NEW
    ):
        # ... ê¸°ì¡´ ì´ˆê¸°í™” ...
        
        # Universe mask ì´ˆê¸°í™” (ë¶ˆë³€)
        self._universe_mask: Optional[xr.DataArray] = None
        if universe is not None:
            self._set_initial_universe(universe)
    
    def _set_initial_universe(self, universe: Union[Expression, xr.DataArray]) -> None:
        """ìœ ë‹ˆë²„ìŠ¤ ë§ˆìŠ¤í¬ë¥¼ ì´ˆê¸°í™” ì‹œ í•œ ë²ˆë§Œ ì„¤ì • (ë¶ˆë³€)."""
        # Expression í‰ê°€ (e.g., Field('univ500'))
        if isinstance(universe, Expression):
            universe_data = self._evaluator.evaluate(universe)
        else:
            universe_data = universe
        
        # Shape ê²€ì¦
        expected_shape = (
            len(self._panel.db.coords['time']), 
            len(self._panel.db.coords['asset'])
        )
        if universe_data.shape != expected_shape:
            raise ValueError(
                f"Universe mask shape {universe_data.shape} doesn't match "
                f"data shape {expected_shape}"
            )
        
        # Dtype ê²€ì¦
        if universe_data.dtype != bool:
            raise TypeError(f"Universe must be boolean, got {universe_data.dtype}")
        
        # ë¶ˆë³€ ì €ì¥
        self._universe_mask = universe_data
        
        # Evaluatorì— ì „íŒŒ (ìë™ ì ìš© ìœ„í•´)
        self._evaluator._universe_mask = self._universe_mask
    
    @property
    def universe(self) -> Optional[xr.DataArray]:
        """ìœ ë‹ˆë²„ìŠ¤ ë§ˆìŠ¤í¬ ì¡°íšŒ (read-only)."""
        return self._universe_mask
```

**2. EvaluateVisitorì— ì´ì¤‘ ë§ˆìŠ¤í‚¹ êµ¬í˜„**:
```python
class EvaluateVisitor:
    def __init__(self, data_source: xr.Dataset, data_loader=None):
        self._data = data_source
        self._data_loader = data_loader
        self._universe_mask: Optional[xr.DataArray] = None  # AlphaCanvasê°€ ì„¤ì •
        self._cache: Dict[int, Tuple[str, xr.DataArray]] = {}
        self._step_counter = 0
    
    def visit_field(self, node) -> xr.DataArray:
        """Field ë…¸ë“œ ë°©ë¬¸ with INPUT MASKING."""
        # í•„ë“œ ë¡œë“œ (ìºì‹œ ë˜ëŠ” DataLoader)
        if node.name in self._data:
            result = self._data[node.name]
        else:
            if self._data_loader is None:
                raise RuntimeError(f"Field '{node.name}' not found")
            result = self._data_loader.load_field(node.name)
            self._data = self._data.assign({node.name: result})
        
        # INPUT MASKING: í•„ë“œ ê²€ìƒ‰ ì‹œ ìœ ë‹ˆë²„ìŠ¤ ì ìš©
        if self._universe_mask is not None:
            result = result.where(self._universe_mask, np.nan)
        
        self._cache_result(f"Field_{node.name}", result)
        return result
    
    def visit_operator(self, node) -> xr.DataArray:
        """ì—°ì‚°ì ë°©ë¬¸ with OUTPUT MASKING."""
        # 1. ìˆœíšŒ: ìì‹ í‰ê°€ (ì´ë¯¸ ë§ˆìŠ¤í‚¹ë¨)
        child_result = node.child.accept(self)
        
        # 2. ìœ„ì„: ì—°ì‚°ìì˜ compute() í˜¸ì¶œ
        result = node.compute(child_result)
        
        # 3. OUTPUT MASKING: ì—°ì‚° ê²°ê³¼ì— ìœ ë‹ˆë²„ìŠ¤ ì ìš©
        if self._universe_mask is not None:
            result = result.where(self._universe_mask, np.nan)
        
        # 4. ìºì‹±
        operator_name = node.__class__.__name__
        self._cache_result(operator_name, result)
        
        return result
```

**3. add_data()ì—ì„œ ì£¼ì… ë°ì´í„° ë§ˆìŠ¤í‚¹**:
```python
def add_data(self, name: str, data: Union[Expression, xr.DataArray]) -> None:
    """ë°ì´í„° ì¶”ê°€ with ìœ ë‹ˆë²„ìŠ¤ ë§ˆìŠ¤í‚¹."""
    if isinstance(data, Expression):
        # Expression ê²½ë¡œ - Evaluatorê°€ ìë™ ë§ˆìŠ¤í‚¹
        self.rules[name] = data
        result = self._evaluator.evaluate(data)
        self._panel.add_data(name, result)
        
        # Evaluator ì¬ë™ê¸°í™” ì‹œ ìœ ë‹ˆë²„ìŠ¤ ë³´ì¡´
        self._evaluator = EvaluateVisitor(self._panel.db, self._data_loader)
        if self._universe_mask is not None:
            self._evaluator._universe_mask = self._universe_mask
    
    elif isinstance(data, xr.DataArray):
        # DataArray ì§ì ‘ ì£¼ì… - ì—¬ê¸°ì„œ ë§ˆìŠ¤í‚¹
        if self._universe_mask is not None:
            data = data.where(self._universe_mask, float('nan'))
        
        self._panel.add_data(name, data)
        self._evaluator = EvaluateVisitor(self._panel.db, self._data_loader)
        if self._universe_mask is not None:
            self._evaluator._universe_mask = self._universe_mask
```

**í•µì‹¬ ì‚¬í•­**:
- **ë¶ˆë³€ì„±**: ìœ ë‹ˆë²„ìŠ¤ëŠ” ì´ˆê¸°í™” ì‹œ í•œ ë²ˆë§Œ ì„¤ì •, ë³€ê²½ ë¶ˆê°€
- **ì´ì¤‘ ë§ˆìŠ¤í‚¹**: Field ì…ë ¥ + Operator ì¶œë ¥ ëª¨ë‘ ë§ˆìŠ¤í‚¹ (ì‹ ë¢° ì²´ì¸)
- **Open Toolkit**: ì£¼ì…ëœ DataArrayë„ ìë™ ë§ˆìŠ¤í‚¹
- **ì„±ëŠ¥**: 13.6% ì˜¤ë²„í—¤ë“œ (xarray lazy evaluationìœ¼ë¡œ ë¬´ì‹œ ê°€ëŠ¥)

---

#### D. `rc.data` Accessor êµ¬í˜„ (Selector Interface) âœ… **IMPLEMENTED**

**ì„¤ê³„ ì² í•™**: Expression ê¸°ë°˜ í•„ë“œ ì ‘ê·¼ìœ¼ë¡œ ì§€ì—° í‰ê°€ ë° ìœ ë‹ˆë²„ìŠ¤ ì•ˆì „ì„± ë³´ì¥

```python
from alpha_canvas.core.expression import Field


class DataAccessor:
    """rc.data accessor that returns Field Expressions.
    
    This enables Expression-based data access:
        rc.data['field_name'] â†’ Field('field_name')
        rc.data['size'] == 'small' â†’ Equals(Field('size'), 'small')
    
    Field Expressions remain lazy until explicitly evaluated,
    ensuring universe masking through the Visitor pattern.
    """
    
    def __getitem__(self, field_name: str) -> Field:
        """Return Field Expression for the given field name.
        
        Args:
            field_name: Name of the field to access
            
        Returns:
            Field Expression wrapping the field name
            
        Raises:
            TypeError: If field_name is not a string
        """
        if not isinstance(field_name, str):
            raise TypeError(
                f"Field name must be string, got {type(field_name).__name__}"
            )
        
        return Field(field_name)
    
    def __getattr__(self, name: str):
        """Prevent attribute access - only item access allowed.
        
        This ensures a single, consistent access pattern.
        """
        raise AttributeError(
            f"DataAccessor does not support attribute access. "
            f"Use rc.data['{name}'] instead of rc.data.{name}"
        )
```

**AlphaCanvas í†µí•©**:

```python
class AlphaCanvas:
    def __init__(self, ...):
        # ... existing init ...
        self._data_accessor = DataAccessor()  # Create once, reuse
    
    @property
    def data(self) -> DataAccessor:
        """Access data fields as Field Expressions."""
        return self._data_accessor
```

**í•µì‹¬ ì‚¬í•­:**

- âœ… **Expression ë°˜í™˜**: `rc.data['size']` â†’ `Field('size')` (lazy)
- âœ… **Lazy í‰ê°€**: ëª…ì‹œì  `rc.evaluate()` í˜¸ì¶œ ì „ê¹Œì§€ í‰ê°€ ì•ˆ ë¨
- âœ… **ìœ ë‹ˆë²„ìŠ¤ ì•ˆì „**: ëª¨ë“  Expressionì€ Visitorë¥¼ í†µí•´ í‰ê°€ë˜ì–´ ìœ ë‹ˆë²„ìŠ¤ ë§ˆìŠ¤í‚¹ ë³´ì¥
- âœ… **Composable**: `ts_mean(rc.data['returns'], 10)` ê°™ì€ ì²´ì´ë‹ ê°€ëŠ¥
- âœ… **Item access only**: `rc.data['field']`ë§Œ ì§€ì›, `rc.data.field`ëŠ” ì—ëŸ¬
- âœ… **í†µí•©**: Phase 7A Boolean Expressionê³¼ ì™„ë²½ í†µí•©

**ì‚¬ìš© íŒ¨í„´**:

```python
# âœ… Correct pattern (Expression-based)
mask = rc.data['size'] == 'small'  # Returns Equals Expression
result = rc.evaluate(mask)         # Evaluates with universe masking

# âœ… Complex pattern
mask = (rc.data['size'] == 'small') & (rc.data['momentum'] == 'high')
result = rc.evaluate(mask)

# âŒ Wrong pattern (not supported)
mask = rc.data.size == 'small'  # AttributeError
```

### 3.2.3. Interface A: Formula-based (Excel-like)

```python
from alpha_canvas.ops import ts_mean, rank, group_neutralize, Field

# 1. ê°„ë‹¨í•œ í—¬í¼ í•¨ìˆ˜ ìŠ¤íƒ€ì¼ (ì¦‰ì‹œ í‰ê°€)
returns_10d = rc.ts_mean('returns', 10)  
# rc.db['returns_10d']ì— DataArray ì €ì¥

# 2. ë³µì¡í•œ Expression ì •ì˜ (ì§€ì—° í‰ê°€)
alpha_expr = group_neutralize(
    rank(ts_mean(Field('returns'), 10)),
    group_by='subindustry'
)

# 3. Expressionì„ ë³€ìˆ˜ë¡œ ë“±ë¡
rc.add_data_var('alpha1', alpha_expr)

# 4. ë°ì´í„° ì ‘ê·¼ (evaluated data)
alpha1_data = rc.db['alpha1']  # xarray.DataArray (T, N)
```

**êµ¬í˜„ ìš”êµ¬ì‚¬í•­:**

- `Field('returns')`: `ConfigLoader`ì—ì„œ `config/data.yaml`ì˜ `returns` ì •ì˜ë¥¼ ì°¸ì¡°í•˜ëŠ” Leaf Expression
- `ts_mean()`, `rank()` ë“±: Composite Expression ë…¸ë“œë¥¼ ìƒì„±
- `rc.add_data_var()`: Expressionì„ `rc.rules`ì— ë“±ë¡í•˜ê³ , `EvaluateVisitor`ë¡œ í‰ê°€í•˜ì—¬ `rc.db`ì— ì €ì¥

### 3.2.3. Interface B: Selector-based (NumPy-like)

```python
# 1. ì‹œê·¸ë„ ìº”ë²„ìŠ¤ ì´ˆê¸°í™”
rc.init_signal_canvas('my_alpha')  
# rc.db['my_alpha']ì— (T, N) ì˜í–‰ë ¬ ìƒì„±

# 2. ë°ì´í„° ë“±ë¡
rc.add_data('mcap', Field('market_cap'))
rc.add_data('ret', Field('returns'))
rc.add_data('vol', Field('volume'))

# 3. ë¶„ë¥˜ ë°ì´í„° ì •ì˜ - ë ˆì´ë¸” ê¸°ë°˜ ë²„í‚·
rc.add_data('size', cs_quantile(rc.data['mcap'], bins=3, labels=['small', 'mid', 'big']))
rc.add_data('momentum', cs_quantile(rc.data['ret'], bins=2, labels=['low', 'high']))
rc.add_data('surge', ts_any(rc.data['ret'] > 0.3, window=252))  # Boolean

# 4. ë¹„êµ ì—°ì‚°ìœ¼ë¡œ Boolean ë§ˆìŠ¤í¬ ìƒì„±
mask_long = (rc.data['size'] == 'small') & (rc.data['momentum'] == 'high') & (rc.data['surge'] == True)
mask_short = (rc.data['size'] == 'big') & (rc.data['momentum'] == 'low')

# 5. NumPy-style í• ë‹¹
rc['my_alpha'][mask_long] = 1.0
rc['my_alpha'][mask_short] = -1.0

# ë˜ëŠ” í˜„ì¬ í™œì„± ìº”ë²„ìŠ¤ì— ì§ì ‘ í• ë‹¹
rc[mask_long] = 1.0

# 6. ìµœì¢… ì‹œê·¸ë„ ì ‘ê·¼ (evaluated data)
my_alpha = rc.db['my_alpha']  # xarray.DataArray (T, N)
```

**êµ¬í˜„ ìš”êµ¬ì‚¬í•­:**

- `rc.add_data('size', expr)`: Expressionì„ í‰ê°€í•˜ê³  `rc.db.assign({'size': result})`ë¡œ data_varsì— ì¶”ê°€
- `rc.data['size'] == 'small'`:
  1. `rc.data['size']` â†’ `Field('size')` Expression ë°˜í™˜
  2. `Field('size') == 'small'` â†’ `Equals(Field('size'), 'small')` Expression ë°˜í™˜
  3. Expressionì€ lazyí•˜ê²Œ ìœ ì§€, `rc.evaluate(expr)`ë¡œ í‰ê°€
- `rc[mask] = value`: `xr.where(mask, value, rc.db[current_canvas])`ë¡œ í• ë‹¹ (ë¯¸êµ¬í˜„)

### 3.2.4. Interface C: Selective Traceability (Integer-Based Steps)

```python
# ë³µì¡í•œ Expression ì •ì˜
complex_alpha = group_neutralize(
    rank(ts_mean(Field('returns'), 5)),
    group_by='subindustry'
)

rc.add_data_var('complex_alpha', complex_alpha)

# Expression íŠ¸ë¦¬ êµ¬ì¡° (depth-first ìˆœì„œ):
# step 0: Field('returns')
# step 1: ts_mean(Field('returns'), 5)
# step 2: rank(...)
# step 3: group_neutralize(...)

# 1. íŠ¹ì • ë‹¨ê³„ë§Œ ì¶”ì 
pnl_step1 = rc.trace_pnl('complex_alpha', step=1)
# {'sharpe': 0.7, 'total_pnl': 150, 'cumulative_returns': [...]}

# 2. ëª¨ë“  ë‹¨ê³„ ì¶”ì 
pnl_all = rc.trace_pnl('complex_alpha')  # step=None (default)
# {
#   0: {'step_name': 'Field_returns', 'sharpe': 0.5, ...},
#   1: {'step_name': 'ts_mean', 'sharpe': 0.7, ...},
#   2: {'step_name': 'rank', 'sharpe': 0.6, ...},
#   3: {'step_name': 'group_neutralize', 'sharpe': 0.8, ...}
# }

# 3. ì¤‘ê°„ ë°ì´í„° ì§ì ‘ ì ‘ê·¼
intermediate = rc.get_intermediate('complex_alpha', step=1)
# xarray.DataArray (T, N) - ts_mean ì ìš© í›„ ë°ì´í„°

# 4. ë³µì¡í•œ Expression ì˜ˆì‹œ (ë³‘ë ¬ ì—°ì‚°)
combo_alpha = ts_mean(Field('returns'), 3) + rank(Field('market_cap'))
rc.add_data_var('combo', combo_alpha)

# step 0: Field('returns')
# step 1: ts_mean(Field('returns'), 3)
# step 2: Field('market_cap')
# step 3: rank(Field('market_cap'))
# step 4: add(step1, step3)

pnl_step4 = rc.trace_pnl('combo', step=4)  # ìµœì¢… ê²°ê³¼
```

**êµ¬í˜„ ìš”êµ¬ì‚¬í•­:**

- `EvaluateVisitor.cache` êµ¬ì¡°: `dict[str, dict[int, tuple[str, xr.DataArray]]]`
  - ì™¸ë¶€ í‚¤: ë³€ìˆ˜ëª… (e.g., `'complex_alpha'`)
  - ë‚´ë¶€ í‚¤: ì •ìˆ˜ step ì¸ë±ìŠ¤ (0ë¶€í„° ì‹œì‘)
  - ê°’: `(ë…¸ë“œ_ì´ë¦„, DataArray)` íŠœí”Œ (ë””ë²„ê¹…ìš© ë©”íƒ€ë°ì´í„° í¬í•¨)
- `EvaluateVisitor._step_counter`: í˜„ì¬ step ì¸ë±ìŠ¤ë¥¼ ì¶”ì í•˜ëŠ” ë‚´ë¶€ ì¹´ìš´í„°
- `EvaluateVisitor`ëŠ” Expression íŠ¸ë¦¬ë¥¼ **ê¹Šì´ ìš°ì„  íƒìƒ‰(depth-first)** ìœ¼ë¡œ ìˆœíšŒí•˜ë©° ê° ë…¸ë“œì˜ ë°˜í™˜ê°’ì„ ìºì‹œì— ì €ì¥
- `rc.trace_pnl(var, step=None)`:
  - `step=None`: ëª¨ë“  ë‹¨ê³„ì˜ ìºì‹œ ë°ì´í„°ë¥¼ `PnLTracer`ì— ì „ë‹¬
  - `step=1`: í•´ë‹¹ ë‹¨ê³„ë§Œ ì „ë‹¬
- `rc.get_intermediate(var, step)`: `rc._evaluator.cache[var][step][1]` ë°˜í™˜ (DataArray ë¶€ë¶„)

#### Dual-Cache for Weight Tracking âœ… **IMPLEMENTED**

**Phase 16ì—ì„œ ì¶”ê°€ëœ ê¸°ëŠ¥**: Weight scalingì´ Visitor ìºì‹±ì— í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤.

**ì„¤ê³„ ì² í•™**:
- **Every step scaled**: ëª¨ë“  ì—°ì‚° ë‹¨ê³„ì—ì„œ signalê³¼ weightë¥¼ ë™ì‹œ ìºì‹±
- **Scaler in evaluate()**: `rc.evaluate(expr, scaler=...)` APIë¡œ on-the-fly ìŠ¤ì¼€ì¼ëŸ¬ êµì²´
- **Separate caches**: Signal ìºì‹œëŠ” ì˜ì†ì , weight ìºì‹œëŠ” ê°±ì‹  ê°€ëŠ¥
- **Research-friendly**: ë™ì¼ signalì— ì—¬ëŸ¬ ìŠ¤ì¼€ì¼ë§ ì „ëµ ì‰½ê²Œ ë¹„êµ

**ê¸°ë³¸ ì‚¬ìš©ë²•**:

```python
from alpha_canvas.core.expression import Field
from alpha_canvas.ops.timeseries import TsMean
from alpha_canvas.ops.crosssection import Rank
from alpha_canvas.portfolio.strategies import DollarNeutralScaler, GrossNetScaler

# Step 1: Weight scalingê³¼ í•¨ê»˜ í‰ê°€
expr = Rank(TsMean(Field('returns'), window=5))
result = rc.evaluate(expr, scaler=DollarNeutralScaler())

# Step 2: Signal ìºì‹œ ì ‘ê·¼ (ì˜ì†ì )
for step in range(len(rc._evaluator._signal_cache)):
    name, signal = rc._evaluator.get_cached_signal(step)
    print(f"Step {step}: {name}, shape={signal.shape}")
    print(f"  Signal stats: mean={signal.mean():.4f}, std={signal.std():.4f}")

# Step 3: Weight ìºì‹œ ì ‘ê·¼ (ì¡°ê±´ë¶€)
for step in range(len(rc._evaluator._weight_cache)):
    name, weights = rc._evaluator.get_cached_weights(step)
    if weights is not None:
        long_sum = weights.where(weights > 0, 0.0).sum(dim='asset').mean()
        short_sum = weights.where(weights < 0, 0.0).sum(dim='asset').mean()
        gross = abs(weights).sum(dim='asset').mean()
        net = weights.sum(dim='asset').mean()
        
        print(f"Step {step}: {name}")
        print(f"  Long: {long_sum:.4f}, Short: {short_sum:.4f}")
        print(f"  Gross: {gross:.4f}, Net: {net:.4f}")
```

**í¸ì˜ ë©”ì„œë“œ**:

```python
# rc.get_weights() í¸ì˜ ë©”ì„œë“œ ì‚¬ìš©
result = rc.evaluate(expr, scaler=DollarNeutralScaler())

# íŠ¹ì • stepì˜ weight ì¡°íšŒ
weights_step_0 = rc.get_weights(0)  # Field('returns') weights
weights_step_1 = rc.get_weights(1)  # TsMean result weights
weights_step_2 = rc.get_weights(2)  # Rank result weights

# Validation
print(f"Final weights shape: {weights_step_2.shape}")
print(f"Final gross exposure: {abs(weights_step_2).sum(dim='asset').mean():.2f}")
print(f"Final net exposure: {weights_step_2.sum(dim='asset').mean():.2f}")
```

**íš¨ìœ¨ì ì¸ Scaler êµì²´**:

```python
# ì´ˆê¸° í‰ê°€ with DollarNeutralScaler
result = rc.evaluate(expr, scaler=DollarNeutralScaler())
weights_neutral = rc.get_weights(2)  # Final step weights

# Scaler êµì²´ (Signal ìºì‹œ ì¬ì‚¬ìš©!)
result = rc.evaluate(expr, scaler=GrossNetScaler(2.0, 0.3))
weights_long_bias = rc.get_weights(2)  # Final step weights

# ë¹„êµ
print("Dollar Neutral:")
print(f"  Net: {weights_neutral.sum(dim='asset').mean():.4f}")  # ~0.0

print("Net Long Bias:")
print(f"  Net: {weights_long_bias.sum(dim='asset').mean():.4f}")  # ~0.3

# í•µì‹¬: Signalì€ ì¬í‰ê°€í•˜ì§€ ì•ŠìŒ (íš¨ìœ¨ì !)
# Weightë§Œ ì¬ê³„ì‚°ë¨
```

**ìºì‹œ êµ¬ì¡°**:

```python
# EvaluateVisitor ë‚´ë¶€ êµ¬ì¡°
class EvaluateVisitor:
    def __init__(self, ...):
        # Dual-cache architecture
        self._signal_cache: Dict[int, Tuple[str, xr.DataArray]] = {}  # ì˜ì†ì 
        self._weight_cache: Dict[int, Tuple[str, Optional[xr.DataArray]]] = {}  # ê°±ì‹  ê°€ëŠ¥
        self._scaler: Optional[WeightScaler] = None  # í˜„ì¬ scaler
        self._step_counter = 0
    
    def evaluate(self, expr, scaler: Optional[WeightScaler] = None):
        """Evaluate with optional weight caching."""
        # scalerê°€ ë³€ê²½ë˜ë©´ weight_cacheë§Œ ì¬ì„¤ì •
        # signal_cacheëŠ” í•­ìƒ ìƒˆë¡œ í‰ê°€ (but same scaler â†’ reuse)
        ...
    
    def recalculate_weights_with_scaler(self, scaler: WeightScaler):
        """Signal ìºì‹œ ì¬ì‚¬ìš©, weightë§Œ ì¬ê³„ì‚° (on-demand ì „ëµ ë¹„êµìš©)."""
        self._scaler = scaler
        self._weight_cache = {}
        
        for step_idx in sorted(self._signal_cache.keys()):
            name, signal = self._signal_cache[step_idx]
            try:
                weights = scaler.scale(signal)
                self._weight_cache[step_idx] = (name, weights)
            except Exception:
                self._weight_cache[step_idx] = (name, None)
```

**ì„¤ê³„ ê·¼ê±°**:

1. **Every step scaled**: 
   - PnL ë¶„ì„ì„ ìœ„í•´ ê° ì—°ì‚° ë‹¨ê³„ì˜ ì˜í–¥ì„ ì¶”ì  ê°€ëŠ¥
   - ì–´ëŠ ì—°ì‚°ìê°€ ì„±ëŠ¥ì— ê¸°ì—¬/ì•…í™”ì‹œí‚¤ëŠ”ì§€ íŒŒì•…

2. **Scaler in evaluate()**:
   - ìŠ¤ì¼€ì¼ë§ ì „ëµì„ ëŸ°íƒ€ì„ì— ì‰½ê²Œ êµì²´
   - ë™ì¼ Expressionì— ì—¬ëŸ¬ ì „ëµ ë¹„êµ (ì—°êµ¬ ì›Œí¬í”Œë¡œìš°)

3. **Separate caches**:
   - Signal ìºì‹œ: ì˜ì†ì  (scaler ë³€ê²½ ì‹œì—ë„ ìœ ì§€)
   - Weight ìºì‹œ: ê°±ì‹  ê°€ëŠ¥ (scaler ë³€ê²½ ì‹œ ì¬ê³„ì‚°)
   - íš¨ìœ¨ì„±: Signal ì¬í‰ê°€ íšŒí”¼

4. **No backward compat í•„ìš”**:
   - MVP ê°œë°œ ë‹¨ê³„ì—ì„œ clean design ìš°ì„ 
   - Weight ìºì‹œê°€ Noneì´ë©´ scalerê°€ ì§€ì • ì•ˆ ëœ ê²ƒ

**ë¯¸ë˜ í™•ì¥: On-Demand Weight Recalculation**:

```python
# í˜„ì¬ëŠ” evaluate() í˜¸ì¶œ ì‹œë§ˆë‹¤ signalë„ ì¬í‰ê°€
# ë¯¸ë˜ ìµœì í™”: signal ìºì‹œê°€ ìˆìœ¼ë©´ ì¬ì‚¬ìš©í•˜ê³  weightë§Œ ì¬ê³„ì‚°

# Option 1: í˜„ì¬ ë°©ì‹ (ë§¤ë²ˆ ì¬í‰ê°€)
result = rc.evaluate(expr, scaler=DollarNeutralScaler())  # Signal + Weight ê³„ì‚°
result = rc.evaluate(expr, scaler=GrossNetScaler(2.0, 0.3))  # Signal ì¬í‰ê°€ + Weight ê³„ì‚°

# Option 2: ë¯¸ë˜ ìµœì í™” (signal ìºì‹œ ì¬ì‚¬ìš©)
result = rc.evaluate(expr, scaler=DollarNeutralScaler())  # Signal + Weight ê³„ì‚°
rc._evaluator.recalculate_weights_with_scaler(GrossNetScaler(2.0, 0.3))  # Weightë§Œ ì¬ê³„ì‚°
weights_new = rc.get_weights(2)  # ìƒˆë¡œìš´ weight ì¡°íšŒ

# í˜„ì¬ëŠ” evaluate()ê°€ í•­ìƒ signal ì¬í‰ê°€í•˜ì§€ë§Œ,
# ë¯¸ë˜ì—ëŠ” signal ìºì‹œê°€ validí•˜ë©´ ì¬ì‚¬ìš© ê°€ëŠ¥
```

**Performance Impact**:

- **Signal í‰ê°€**: ë³€ê²½ ì—†ìŒ (ê¸°ì¡´ ì„±ëŠ¥ ìœ ì§€)
- **Weight ê³„ì‚°**: Stepë‹¹ 7-40ms (Phase 15 ê²€ì¦)
- **Scaler êµì²´**: Signal ìºì‹œ ì¬ì‚¬ìš© ì‹œ weight ê³„ì‚°ë§Œ (ë§¤ìš° íš¨ìœ¨ì )
- **ë©”ëª¨ë¦¬**: 2ë°° ìºì‹œ (signal + weight), í•˜ì§€ë§Œ PnL ë¶„ì„ ê°€ì¹˜ê°€ cost ìƒíšŒ

**Use Cases**:

1. **Step-by-step PnL decomposition**:
   ```python
   for step in range(num_steps):
       signal = rc._evaluator.get_cached_signal(step)[1]
       weights = rc.get_weights(step)
       if weights is not None:
           # Calculate PnL for this step
           step_pnl = (weights * returns.shift(time=1)).sum(dim='asset')
           print(f"Step {step} PnL: {step_pnl.sum().values:.2f}")
   ```

2. **Scaling strategy comparison**:
   ```python
   strategies = {
       'neutral': DollarNeutralScaler(),
       'long_10pct': GrossNetScaler(2.0, 0.2),
       'long_only': LongOnlyScaler(1.0)
   }
   
   for name, scaler in strategies.items():
       result = rc.evaluate(expr, scaler=scaler)
       weights = rc.get_weights(-1)  # Final step
       # Compare performance metrics
   ```

3. **Attribution analysis**:
   - Signal generation (ì—°ì‚°ìë“¤ì˜ ê¸°ì—¬)
   - Weight scaling (ìŠ¤ì¼€ì¼ë§ ì „ëµì˜ ì˜í–¥)
   - ê°ê°ì˜ PnL ê¸°ì—¬ë„ ë¶„ë¦¬ ë¶„ì„

#### Triple-Cache for Backtesting âœ… **IMPLEMENTED**

**Phase 17ì—ì„œ ì¶”ê°€ëœ ê¸°ëŠ¥**: Portfolio return computationì´ triple-cacheì— í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤.

**ì„¤ê³„ ì² í•™**:
- **Automatic execution**: Scaler ì œê³µ ì‹œ ë°±í…ŒìŠ¤íŠ¸ ìë™ ì‹¤í–‰
- **Position-level returns**: `(T, N)` shape ë³´ì¡´ìœ¼ë¡œ winner/loser ë¶„ì„ ê°€ëŠ¥
- **Shift-mask workflow**: Forward-looking bias ë°©ì§€, NaN pollution ë°©ì§€
- **On-demand aggregation**: Daily/cumulative PnLì€ í•„ìš” ì‹œ ê³„ì‚°

**Return Data Auto-Loading**:

```python
# config/data.yamlì— 'returns' í•„ë“œ í•„ìˆ˜ ì •ì˜
returns:
  db_type: parquet
  table: PRICEVOLUME
  index_col: date
  security_col: security_id
  value_col: return
  query: >
    SELECT date, security_id, return 
    FROM read_parquet('data/fake/pricevolume.parquet')
    WHERE date >= :start_date AND date <= :end_date

# AlphaCanvas ì´ˆê¸°í™” ì‹œ ìë™ ë¡œë“œ
rc = AlphaCanvas(start_date='2024-01-01', end_date='2024-12-31')
# ë‚´ë¶€ì ìœ¼ë¡œ _load_returns_data() í˜¸ì¶œ
# rc._returnsì— ì €ì¥, rc._evaluator._returns_dataì— ì „ë‹¬
```

**ê¸°ë³¸ ì‚¬ìš©ë²•**:

```python
from alpha_canvas.core.expression import Field
from alpha_canvas.ops.timeseries import TsMean
from alpha_canvas.ops.crosssection import Rank
from alpha_canvas.portfolio.strategies import DollarNeutralScaler

# Step 1: Scalerì™€ í•¨ê»˜ í‰ê°€ (ë°±í…ŒìŠ¤íŠ¸ ìë™ ì‹¤í–‰)
expr = Rank(TsMean(Field('price'), window=5))
result = rc.evaluate(expr, scaler=DollarNeutralScaler())

# Step 2: Position-level returns ì ‘ê·¼ (winner/loser ë¶„ì„)
port_return = rc.get_port_return(step=2)  # (T, N) shape
total_contrib = port_return.sum(dim='time')
best_stock = total_contrib.argmax(dim='asset')
worst_stock = total_contrib.argmin(dim='asset')

print(f"Best performer: {assets[best_stock]}")
print(f"Worst performer: {assets[worst_stock]}")

# Step 3: Daily PnL ë° ì„±ê³¼ ì§€í‘œ
daily_pnl = rc.get_daily_pnl(step=2)  # (T,) aggregated
mean_pnl = daily_pnl.mean().values
std_pnl = daily_pnl.std().values
sharpe = mean_pnl / std_pnl * np.sqrt(252)

print(f"Mean daily PnL: {mean_pnl:.6f}")
print(f"Sharpe ratio: {sharpe:.2f}")

# Step 4: Cumulative PnL (cumsum ì‚¬ìš©)
cum_pnl = rc.get_cumulative_pnl(step=2)
final_pnl = cum_pnl.isel(time=-1).values

print(f"Final cumulative PnL: {final_pnl:.6f}")
```

**Shift-Mask Workflow (Forward-Bias Prevention)**:

```python
# ë‚´ë¶€ êµ¬í˜„ (_compute_portfolio_returns):
def _compute_portfolio_returns(self, weights: xr.DataArray) -> xr.DataArray:
    # Step 1: Shift weights (ì–´ì œì˜ signalë¡œ ì˜¤ëŠ˜ ê±°ë˜)
    weights_shifted = weights.shift(time=1)
    
    # Step 2: Re-mask with current universe (ì¤‘ìš”! NaN pollution ë°©ì§€)
    if self._universe_mask is not None:
        final_weights = weights_shifted.where(self._universe_mask)
    else:
        final_weights = weights_shifted
    
    # Step 3: Mask returns
    if self._universe_mask is not None:
        returns_masked = self._returns_data.where(self._universe_mask)
    else:
        returns_masked = self._returns_data
    
    # Step 4: Element-wise multiply (shape (T, N) ë³´ì¡´!)
    port_return = final_weights * returns_masked
    
    return port_return  # ì¦‰ì‹œ ì§‘ê³„í•˜ì§€ ì•ŠìŒ
```

**Re-Maskingì˜ ì¤‘ìš”ì„±**:

```python
# ë¬¸ì œ ìƒí™©:
# - ì¢…ëª©ì´ universeì—ì„œ í‡´ì¶œ â†’ weightê°€ NaN
# - NaN * return = NaN â†’ daily PnLë„ NaN (pollution)

# í•´ê²°ì±…: Shift í›„ í˜„ì¬ universeë¡œ re-mask
# - í‡´ì¶œëœ ì¢…ëª©ì˜ í¬ì§€ì…˜ ì²­ì‚°
# - NaN pollution ë°©ì§€
# - PnL ê³„ì‚° ì •ìƒ ì‘ë™

# Example:
# t=0: Stock A in universe, weight=0.5
# t=1: Stock A exits universe
# Without re-mask: weight[t=1] = 0.5 (shifted) â†’ NaN * return â†’ NaN
# With re-mask: weight[t=1] = NaN (liquidated) â†’ sum(skipna=True) â†’ valid PnL
```

**Efficient Scaler Comparison**:

```python
# ì´ˆê¸° í‰ê°€
result = rc.evaluate(expr, scaler=DollarNeutralScaler())
pnl1 = rc.get_cumulative_pnl(2).isel(time=-1).values

# Scaler êµì²´ (Signal ìºì‹œ ì¬ì‚¬ìš©!)
result = rc.evaluate(expr, scaler=GrossNetScaler(2.0, 0.5))
pnl2 = rc.get_cumulative_pnl(2).isel(time=-1).values

print(f"Dollar Neutral PnL: {pnl1:.6f}")
print(f"Net Long Bias PnL: {pnl2:.6f}")

# í•µì‹¬: Signal ì¬í‰ê°€ ì—†ìŒ, weightì™€ port_returnë§Œ ì¬ê³„ì‚° (íš¨ìœ¨ì !)
```

**ìºì‹œ êµ¬ì¡° (Triple-Cache)**:

```python
class EvaluateVisitor:
    def __init__(self, ...):
        # Triple-cache architecture
        self._signal_cache: Dict[int, Tuple[str, xr.DataArray]] = {}  # ì˜ì†ì 
        self._weight_cache: Dict[int, Tuple[str, Optional[xr.DataArray]]] = {}  # ê°±ì‹  ê°€ëŠ¥
        self._port_return_cache: Dict[int, Tuple[str, Optional[xr.DataArray]]] = {}  # ê°±ì‹  ê°€ëŠ¥ (NEW!)
        
        self._returns_data: Optional[xr.DataArray] = None  # Return data (í•„ìˆ˜)
        self._scaler: Optional[WeightScaler] = None
        self._step_counter = 0
```

**On-Demand Aggregation**:

```python
# Position-level returnsì€ í•­ìƒ (T, N) shapeìœ¼ë¡œ ìºì‹±
# Daily/cumulative PnLì€ í•„ìš” ì‹œ on-demand ê³„ì‚° (ë¹ ë¦„, <1ms)

def get_daily_pnl(self, step: int) -> Optional[xr.DataArray]:
    port_return = self.get_port_return(step)
    if port_return is None:
        return None
    # On-demand aggregation
    daily_pnl = port_return.sum(dim='asset')  # (T,)
    return daily_pnl

def get_cumulative_pnl(self, step: int) -> Optional[xr.DataArray]:
    daily_pnl = self.get_daily_pnl(step)
    if daily_pnl is None:
        return None
    # Cumsum (not cumprod) for time-invariance
    cumulative_pnl = daily_pnl.cumsum(dim='time')
    return cumulative_pnl
```

**Cumsum vs Cumprod**:

```python
# Cumsum: Time-invariant (ê³µì •í•œ ì „ëµ ë¹„êµ)
returns_example = [0.02, 0.03, -0.01]
cumsum_result = sum(returns_example)  # 0.04 (ìˆœì„œ ë¬´ê´€)

# Cumprod: Time-dependent (ê¸´ ì „ëµì— ìœ ë¦¬, ë¶ˆê³µì •)
cumprod_result = (1.02 * 1.03 * 0.99) - 1  # 0.0405 (ë³µë¦¬ íš¨ê³¼)

# ì„ íƒ: Cumsumì„ ê¸°ë³¸ ë©”íŠ¸ë¦­ìœ¼ë¡œ ì‚¬ìš©
# â†’ ì—°êµ¬ ëª©ì ì˜ ê³µì •í•œ ë¹„êµ
```

**Step-by-Step PnL Decomposition**:

```python
# ê° ì—°ì‚° stepë³„ PnL ë¹„êµ (ì–´ëŠ ì—°ì‚°ìê°€ ì„±ëŠ¥ì— ê¸°ì—¬/ì•…í™”?)
expr = Rank(TsMean(Field('price'), window=5))
result = rc.evaluate(expr, scaler=DollarNeutralScaler())

for step in range(len(rc._evaluator._signal_cache)):
    signal_name, _ = rc._evaluator.get_cached_signal(step)
    cum_pnl = rc.get_cumulative_pnl(step)
    
    if cum_pnl is not None:
        final_pnl = cum_pnl.isel(time=-1).values
        print(f"Step {step} ({signal_name}): PnL = {final_pnl:+.6f}")

# Example output:
# Step 0 (Field_price): PnL = -0.101889
# Step 1 (TsMean): PnL = -0.051532
# Step 2 (Rank): PnL = -0.054569
# â†’ TsMeanì´ ì„±ëŠ¥ì„ ê°œì„ í–ˆì§€ë§Œ, Rankê°€ ì•½ê°„ ì•…í™”ì‹œí‚´
```

**Winner/Loser Attribution Analysis**:

```python
# Position-level returnsì„ í™œìš©í•œ ê¸°ì—¬ë„ ë¶„ì„
port_return = rc.get_port_return(final_step)

# ì¢…ëª©ë³„ ì´ ê¸°ì—¬ë„
total_contrib = port_return.sum(dim='time')

# Top/Bottom ì¢…ëª© ì‹ë³„
sorted_contrib = total_contrib.sortby(total_contrib, ascending=False)

print("Top 5 Contributors:")
for i in range(5):
    asset = sorted_contrib.asset.values[i]
    contrib = sorted_contrib.sel(asset=asset).values
    print(f"  {i+1}. {asset}: {contrib:+.6f}")

print("\nBottom 5 Contributors:")
for i in range(-5, 0):
    asset = sorted_contrib.asset.values[i]
    contrib = sorted_contrib.sel(asset=asset).values
    print(f"  {i+6}. {asset}: {contrib:+.6f}")
```

**Performance Impact**:

- **Return loading**: ~5ms per field (ê°™ì€ Parquet ë¡œë“œ ì„±ëŠ¥)
- **Portfolio return computation**: ~7ms for (252, 100) dataset (Experiment 19)
- **Daily PnL aggregation**: <1ms (on-demand)
- **Cumulative PnL**: <1ms (on-demand cumsum)
- **ë©”ëª¨ë¦¬**: 3ë°° ìºì‹œ (signal + weight + port_return), í•˜ì§€ë§Œ position-level ë¶„ì„ ê°€ì¹˜ ë†’ìŒ

**Design Rationale**:

1. **Position-level caching**: 
   - Winner/loser ë¶„ì„ì„ ìœ„í•´ (T, N) shape ë³´ì¡´ í•„ìˆ˜
   - ì¢…ëª©ë³„ ê¸°ì—¬ë„, factor ê²€ì¦ì— critical
   
2. **On-demand aggregation**:
   - Daily/cumulative PnLì€ ë¹ ë¦„ (<1ms)
   - ìºì‹± ë¶ˆí•„ìš”, ë©”ëª¨ë¦¬ ì ˆì•½
   
3. **Shift-mask workflow**:
   - Forward-bias ë°©ì§€ (realistic backtest)
   - Re-maskingìœ¼ë¡œ NaN pollution ë°©ì§€
   
4. **Triple-cache efficiency**:
   - Scaler êµì²´ ì‹œ signal ì¬ì‚¬ìš©
   - Weightì™€ port_returnë§Œ ì¬ê³„ì‚°
   - ì „ëµ ë¹„êµ íš¨ìœ¨ì 

### 3.2.5. í•µì‹¬ í™œìš© íŒ¨í„´: íŒ©í„° ìˆ˜ìµë¥  ê³„ì‚°

#### A. ë…ë¦½ ì´ì¤‘ ì •ë ¬ (Independent Double Sort) - Fama-French SMB

```python
from alpha_canvas.ops import cs_quantile, Field

# 1. ë°ì´í„° ë“±ë¡
rc.add_data('mcap', Field('market_cap'))
rc.add_data('btm', Field('book_to_market'))

# 2. ë…ë¦½ ì •ë ¬: ì „ì²´ ìœ ë‹ˆë²„ìŠ¤ì—ì„œ ê°ê° quantile ê³„ì‚°
rc.add_axis('size', cs_quantile(rc.data.mcap, bins=2, labels=['small', 'big']))
rc.add_axis('value', cs_quantile(rc.data.btm, bins=3, labels=['low', 'mid', 'high']))

# 3. SMB í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„±
rc.init_signal_canvas('smb')
rc[rc.axis.size['small']] = 1.0   # Long all small stocks
rc[rc.axis.size['big']] = -1.0    # Short all big stocks

# 4. íŒ©í„° ìˆ˜ìµë¥  ì¶”ì 
smb_returns = rc.trace_pnl('smb')
print(f"SMB Sharpe: {smb_returns['sharpe']:.2f}")
```

#### B. ì¢…ì† ì´ì¤‘ ì •ë ¬ (Dependent Double Sort) - Fama-French HML

```python
# 1. ì²« ë²ˆì§¸ ì •ë ¬: Size
rc.add_axis('size', cs_quantile(rc.data.mcap, bins=2, labels=['small', 'big']))

# 2. ì¢…ì† ì •ë ¬: ê° Size ê·¸ë£¹ ë‚´ì—ì„œ Value quantile ê³„ì‚°
rc.add_axis('value', cs_quantile(rc.data.btm, bins=3, labels=['low', 'mid', 'high'],
                                   group_by='size'))
# group_by='size'ëŠ” rc.rules['size']ë¥¼ ì°¸ì¡°í•˜ì—¬
# 'small' ê·¸ë£¹ê³¼ 'big' ê·¸ë£¹ ë‚´ì—ì„œ ê°ê° ë…ë¦½ì ìœ¼ë¡œ quantileì„ ê³„ì‚°

# 3. HML í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„± (ê° Size ê·¸ë£¹ ë‚´ì—ì„œ High-Low)
rc.init_signal_canvas('hml')
rc[rc.axis.value['high']] = 1.0   # Long high B/M (value) in both size groups
rc[rc.axis.value['low']] = -1.0   # Short low B/M (growth) in both size groups

# 4. íŒ©í„° ìˆ˜ìµë¥  ì¶”ì 
hml_returns = rc.trace_pnl('hml')
print(f"HML Sharpe: {hml_returns['sharpe']:.2f}")
```

#### C. ë¡œìš°ë ˆë²¨ ë§ˆìŠ¤í¬ í™œìš© (Advanced Custom Logic)

```python
# ìœ ë™ì„± í•„í„°ë§ëœ ìœ ë‹ˆë²„ìŠ¤ì—ì„œ ëª¨ë©˜í…€ íŒ©í„° êµ¬ì„±
rc.add_data('volume', Field('volume'))
rc.add_data('returns', Field('returns'))

# 1. ê³ ìœ ë™ì„± ë§ˆìŠ¤í¬ ìƒì„±
high_liquidity = rc.data.volume > rc.data.volume.quantile(0.5)

# 2. ë§ˆìŠ¤í¬ ì ìš©ëœ quantile ê³„ì‚°
rc.add_axis('momentum', cs_quantile(rc.data.returns, bins=5, labels=['q1','q2','q3','q4','q5'],
                                      mask=high_liquidity))
# mask=Falseì¸ ì¢…ëª©ì€ NaNìœ¼ë¡œ ì²˜ë¦¬ë¨

# 3. ë¡±-ìˆ í¬íŠ¸í´ë¦¬ì˜¤
rc.init_signal_canvas('momentum_factor')
rc[rc.axis.momentum['q5']] = 1.0
rc[rc.axis.momentum['q1']] = -1.0

mom_returns = rc.trace_pnl('momentum_factor')
```

#### D. ë‹¤ì°¨ì› íŒ©í„° ì¡°í•© (Multi-Factor Strategy)

```python
# ë…ë¦½ ì •ë ¬ë¡œ 3ê°œ íŒ©í„° ì¶• ìƒì„±
rc.add_axis('size', cs_quantile(rc.data.mcap, bins=3, labels=['small','mid','big']))
rc.add_axis('momentum', cs_quantile(rc.data.mom, bins=5, labels=['q1','q2','q3','q4','q5']))
rc.add_axis('quality', cs_quantile(rc.data.roe, bins=3, labels=['low','mid','high']))

# ë³µì¡í•œ ë‹¤ì°¨ì› ì„ íƒ
rc.init_signal_canvas('multi_factor')

# Small & High Momentum & High Quality
long_mask = (rc.axis.size['small'] & 
             rc.axis.momentum['q5'] & 
             rc.axis.quality['high'])

# Big & Low Momentum & Low Quality
short_mask = (rc.axis.size['big'] & 
              rc.axis.momentum['q1'] & 
              rc.axis.quality['low'])

rc[long_mask] = 1.0
rc[short_mask] = -1.0

multi_returns = rc.trace_pnl('multi_factor')
```

**ì„¤ê³„ ì˜ë„:**

- ì´ íŒ¨í„´ë“¤ì´ alpha-canvasì˜ **í•µì‹¬ í™œìš© ì‚¬ë¡€**ì…ë‹ˆë‹¤.
- **Fama-French ì¬í˜„**: `group_by`ë¡œ ì¢…ì† ì •ë ¬ì„ ê°„ê²°í•˜ê²Œ í‘œí˜„
- **ìœ ì—°ì„±**: `mask`ë¡œ ì»¤ìŠ¤í…€ ìœ ë‹ˆë²„ìŠ¤ í•„í„°ë§ ê°€ëŠ¥
- ë“€ì–¼ ì¸í„°í˜ì´ìŠ¤(Formula + Selector)ì˜ ì¡°í•©ìœ¼ë¡œ ë³µì¡í•œ ë‹¤ì°¨ì› íŒ©í„° í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ ê°„ê²°í•˜ê²Œ í‘œí˜„í•©ë‹ˆë‹¤.
- ë ˆì´ë¸” ê¸°ë°˜ ì„ íƒ(`'small'`, `'q5'`, `'high'`)ìœ¼ë¡œ ê°€ë…ì„±ê³¼ ìœ ì§€ë³´ìˆ˜ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤.

## 3.3. ì—°ì‚°ì êµ¬í˜„ íŒ¨í„´ (Operator Implementation Pattern)

### 3.3.1. ì±…ì„ ë¶„ë¦¬ ì›ì¹™

**í•µì‹¬ ì›ì¹™:** ì—°ì‚°ìëŠ” ìì‹ ì˜ ê³„ì‚° ë¡œì§ì„ ì†Œìœ í•˜ê³ , VisitorëŠ” ìˆœíšŒ ë° ìºì‹±ë§Œ ë‹´ë‹¹í•©ë‹ˆë‹¤.

**ì˜ëª»ëœ íŒ¨í„´ (Anti-Pattern):**

```python
# âŒ BAD: Visitorê°€ ê³„ì‚° ë¡œì§ì„ í¬í•¨
class EvaluateVisitor:
    def visit_ts_mean(self, node):
        child_result = node.child.accept(self)
        # Visitor ì•ˆì— rolling ê³„ì‚° ë¡œì§ì´ ë“¤ì–´ê° (ì˜ëª»ë¨!)
        result = child_result.rolling(time=node.window, min_periods=node.window).mean()
        self._cache_result("TsMean", result)
        return result
```

**ì˜¬ë°”ë¥¸ íŒ¨í„´ (Correct Pattern):**

```python
# âœ… GOOD: ì—°ì‚°ìê°€ ê³„ì‚° ë¡œì§ì„ ì†Œìœ 
@dataclass
class TsMean(Expression):
    child: Expression
    window: int
    
    def accept(self, visitor):
        """Visitor ì¸í„°í˜ì´ìŠ¤: ìˆœíšŒë¥¼ ìœ„í•œ ì§„ì…ì """
        return visitor.visit_ts_mean(self)
    
    def compute(self, child_result: xr.DataArray) -> xr.DataArray:
        """í•µì‹¬ ê³„ì‚° ë¡œì§: ì—°ì‚°ìê°€ ì†Œìœ """
        return child_result.rolling(
            time=self.window,
            min_periods=self.window
        ).mean()

# âœ… GOOD: VisitorëŠ” ìˆœíšŒ ë° ìºì‹±ë§Œ ë‹´ë‹¹
class EvaluateVisitor:
    def visit_ts_mean(self, node: TsMean) -> xr.DataArray:
        """íŠ¸ë¦¬ ìˆœíšŒ ë° ìƒíƒœ ìˆ˜ì§‘"""
        # 1. ìˆœíšŒ: ìì‹ ë…¸ë“œ í‰ê°€
        child_result = node.child.accept(self)
        
        # 2. ê³„ì‚° ìœ„ì„: ì—°ì‚°ìì—ê²Œ ë§¡ê¹€
        result = node.compute(child_result)
        
        # 3. ìƒíƒœ ìˆ˜ì§‘: ê²°ê³¼ ìºì‹±
        self._cache_result("TsMean", result)
        
        return result
```

### 3.3.2. ì—°ì‚°ì êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

ëª¨ë“  ì—°ì‚°ìëŠ” ë‹¤ìŒ êµ¬ì¡°ë¥¼ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤:

```python
@dataclass
class OperatorName(Expression):
    """ì—°ì‚°ì ì„¤ëª….
    
    Args:
        child: ìì‹ Expression (í•„ìš”ì‹œ)
        param1: ì—°ì‚°ì íŒŒë¼ë¯¸í„° 1
        param2: ì—°ì‚°ì íŒŒë¼ë¯¸í„° 2
    
    Returns:
        ì—°ì‚° ê²°ê³¼ DataArray
    """
    child: Expression  # ìì‹ ë…¸ë“œ (ìˆëŠ” ê²½ìš°)
    param1: type1      # ì—°ì‚°ì íŒŒë¼ë¯¸í„°ë“¤
    param2: type2
    
    def accept(self, visitor) -> xr.DataArray:
        """Visitor ì¸í„°í˜ì´ìŠ¤."""
        return visitor.visit_operator_name(self)
    
    def compute(self, *inputs: xr.DataArray) -> xr.DataArray:
        """í•µì‹¬ ê³„ì‚° ë¡œì§.
        
        Args:
            *inputs: ìì‹ ë…¸ë“œë“¤ì˜ í‰ê°€ ê²°ê³¼
        
        Returns:
            ì´ ì—°ì‚°ì˜ ê²°ê³¼ DataArray
        
        Note:
            ì´ ë©”ì„œë“œëŠ” ìˆœìˆ˜ í•¨ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤ (ë¶€ì‘ìš© ì—†ìŒ).
            Visitor ì°¸ì¡° ì—†ì´ ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤.
        """
        # ì‹¤ì œ ê³„ì‚° ë¡œì§
        result = ...  # xarray/numpy ì—°ì‚°
        return result
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**

- [ ] `accept()` ë©”ì„œë“œ: Visitor ì¸í„°í˜ì´ìŠ¤ ì œê³µ
- [ ] `compute()` ë©”ì„œë“œ: í•µì‹¬ ê³„ì‚° ë¡œì§ ìº¡ìŠí™”
- [ ] `compute()`ëŠ” ìˆœìˆ˜ í•¨ìˆ˜ (ë¶€ì‘ìš© ì—†ìŒ)
- [ ] `compute()`ëŠ” Visitor ë…ë¦½ì  (ì§ì ‘ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥)
- [ ] Docstringìœ¼ë¡œ Args/Returns ëª…í™•íˆ ë¬¸ì„œí™”

### 3.3.3. Visitor êµ¬í˜„ íŒ¨í„´

ëª¨ë“  `visit_*()` ë©”ì„œë“œëŠ” ë™ì¼í•œ 3ë‹¨ê³„ íŒ¨í„´ì„ ë”°ë¦…ë‹ˆë‹¤:

```python
def visit_operator_name(self, node: OperatorName) -> xr.DataArray:
    """ì—°ì‚°ì ë…¸ë“œ ë°©ë¬¸: ìˆœíšŒ ë° ìºì‹±.
    
    Args:
        node: ì—°ì‚°ì Expression ë…¸ë“œ
    
    Returns:
        ì—°ì‚° ê²°ê³¼ DataArray
    """
    # 1ï¸âƒ£ ìˆœíšŒ(Traversal): ìì‹ ë…¸ë“œë“¤ í‰ê°€
    child_result_1 = node.child1.accept(self)  # ê¹Šì´ ìš°ì„ 
    child_result_2 = node.child2.accept(self)  # (ìˆëŠ” ê²½ìš°)
    
    # 2ï¸âƒ£ ê³„ì‚° ìœ„ì„(Delegation): ì—°ì‚°ìì—ê²Œ ë§¡ê¹€
    result = node.compute(child_result_1, child_result_2)
    
    # 3ï¸âƒ£ ìƒíƒœ ìˆ˜ì§‘(State Collection): ê²°ê³¼ ìºì‹±
    self._cache_result("OperatorName", result)
    
    return result
```

**Visitorì˜ ì—­í• :**

- âœ… **íŠ¸ë¦¬ ìˆœíšŒ:** ê¹Šì´ ìš°ì„ ìœ¼ë¡œ ìì‹ ë…¸ë“œ ë°©ë¬¸
- âœ… **ê³„ì‚° ìœ„ì„:** `node.compute()`ë¡œ ê³„ì‚° ë§¡ê¹€
- âœ… **ìƒíƒœ ìˆ˜ì§‘:** ì¤‘ê°„ ê²°ê³¼ë¥¼ ì •ìˆ˜ ìŠ¤í…ìœ¼ë¡œ ìºì‹±
- âŒ **ê³„ì‚° ë¡œì§ í¬í•¨ ê¸ˆì§€:** rolling, rank, quantile ë“±ì˜ ë¡œì§ì€ ì—°ì‚°ìì— ì†í•¨

### 3.3.4. í…ŒìŠ¤íŠ¸ ì „ëµ

**1. ì—°ì‚°ì ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (Operator Unit Tests):**

```python
def test_ts_mean_compute_directly():
    """TsMean.compute() ë©”ì„œë“œë¥¼ ì§ì ‘ í…ŒìŠ¤íŠ¸ (Visitor ì—†ì´)."""
    # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
    data = xr.DataArray(
        [[1, 2], [3, 4], [5, 6]],
        dims=['time', 'asset']
    )
    
    # ì—°ì‚°ì ìƒì„±
    operator = TsMean(child=Field('dummy'), window=2)
    
    # compute() ì§ì ‘ í˜¸ì¶œ (Visitor ìš°íšŒ)
    result = operator.compute(data)
    
    # ê²€ì¦
    assert np.isnan(result.values[0, 0])  # ì²« í–‰ NaN
    assert result.values[1, 0] == 2.0     # mean([1, 3])
```

**2. í†µí•© í…ŒìŠ¤íŠ¸ (Integration Tests):**

```python
def test_ts_mean_with_visitor():
    """TsMeanì´ Visitorì™€ í†µí•©ë˜ì–´ ì‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸."""
    ds = xr.Dataset({'returns': data})
    visitor = EvaluateVisitor(ds)
    
    expr = TsMean(child=Field('returns'), window=3)
    result = visitor.evaluate(expr)
    
    # ìºì‹± ê²€ì¦
    assert len(visitor._cache) == 2  # Field + TsMean
```

### 3.3.5. ì´ì  ìš”ì•½

| ì¸¡ë©´ | ì˜ëª»ëœ íŒ¨í„´ | ì˜¬ë°”ë¥¸ íŒ¨í„´ |
|------|-------------|-------------|
| **ì±…ì„** | Visitorê°€ ëª¨ë“  ê³„ì‚° ë‹´ë‹¹ | ì—°ì‚°ìê°€ ìì‹ ì˜ ê³„ì‚° ì†Œìœ  |
| **í…ŒìŠ¤íŠ¸** | Visitorë¥¼ í†µí•´ì„œë§Œ í…ŒìŠ¤íŠ¸ | `compute()` ì§ì ‘ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥ |
| **ìœ ì§€ë³´ìˆ˜** | Visitorê°€ ë¹„ëŒ€í•´ì§ | ê° ì—°ì‚°ì ë…ë¦½ì  |
| **í™•ì¥ì„±** | ìƒˆ ì—°ì‚°ìë§ˆë‹¤ Visitor ìˆ˜ì • | Visitor ìˆ˜ì • ìµœì†Œí™” |
| **ë‹¨ì¼ ì±…ì„** | Visitorê°€ ë‹¤ì¤‘ ì±…ì„ | ê° í´ë˜ìŠ¤ ë‹¨ì¼ ì±…ì„ |

---

## 3.4. Cross-Sectional Quantile ì—°ì‚°ì êµ¬í˜„ âœ… **IMPLEMENTED**

### 3.4.1. `CsQuantile` Expression í´ë˜ìŠ¤ (ì‹¤ì œ êµ¬í˜„)

```python
from dataclasses import dataclass
from typing import List, Optional
import numpy as np
import pandas as pd
import xarray as xr

@dataclass(eq=False)  # eq=False to preserve Expression comparison operators
class CsQuantile(Expression):
    """Cross-sectional quantile bucketing - returns categorical labels.
    
    Preserves input (T, N) shape. Each timestep is independently bucketed.
    Supports both independent sort (whole universe) and dependent sort
    (within groups via group_by parameter).
    """
    child: Expression  # ë²„í‚·í™”í•  ë°ì´í„° (e.g., Field('market_cap'))
    bins: int  # ë²„í‚· ê°œìˆ˜
    labels: List[str]  # ë ˆì´ë¸” ë¦¬ìŠ¤íŠ¸ (ê¸¸ì´ = bins)
    group_by: Optional[str] = None  # ì¢…ì† ì •ë ¬ìš©: field ì´ë¦„ (string)
    
    def __post_init__(self):
        """Validate parameters."""
        if len(self.labels) != self.bins:
            raise ValueError(
                f"labels length ({len(self.labels)}) must equal bins ({self.bins})"
            )
    
    def accept(self, visitor):
        """Visitor ì¸í„°í˜ì´ìŠ¤."""
        return visitor.visit_operator(self)
    
    def compute(
        self, 
        child_result: xr.DataArray, 
        group_labels: Optional[xr.DataArray] = None
    ) -> xr.DataArray:
        """Apply quantile bucketing - í•µì‹¬ ê³„ì‚° ë¡œì§."""
        if group_labels is None:
            return self._quantile_independent(child_result)
        else:
            return self._quantile_grouped(child_result, group_labels)
```

### 3.4.2. ë…ë¦½ ì •ë ¬ (Independent Sort) êµ¬í˜„

**í•µì‹¬ íŒ¨í„´:** `xarray.groupby('time').map()` + `pd.qcut` + **flatten-reshape**

```python
def _quantile_independent(self, data: xr.DataArray) -> xr.DataArray:
    """Independent sort - qcut at each timestep across all assets.
    
    í•µì‹¬: pd.qcutì€ 1D ì…ë ¥ì´ í•„ìš”í•˜ë¯€ë¡œ flatten â†’ qcut â†’ reshape íŒ¨í„´ ì‚¬ìš©
    """
    def qcut_at_timestep(data_slice):
        """Apply pd.qcut to a single timestep's cross-section."""
        try:
            # CRITICAL: Flatten to 1D for pd.qcut
            values_1d = data_slice.values.flatten()
            result = pd.qcut(
                values_1d, 
                q=self.bins, 
                labels=self.labels, 
                duplicates='drop'  # Handle edge cases gracefully
            )
            # CRITICAL: Reshape back to original shape
            result_array = np.array(result).reshape(data_slice.shape)
            return xr.DataArray(
                result_array, 
                dims=data_slice.dims, 
                coords=data_slice.coords
            )
        except Exception:
            # Edge case: all same values, all NaN, etc.
            return xr.DataArray(
                np.full_like(data_slice.values, np.nan, dtype=object),
                dims=data_slice.dims, 
                coords=data_slice.coords
            )
    
    # xarray.groupby('time').map() automatically concatenates back to (T, N)
    result = data.groupby('time').map(qcut_at_timestep)
    return result
```

### 3.4.3. ì¢…ì† ì •ë ¬ (Dependent Sort) êµ¬í˜„

**í•µì‹¬ íŒ¨í„´:** ì¤‘ì²©ëœ groupby (groups â†’ time â†’ qcut)

```python
def _quantile_grouped(
    self, 
    data: xr.DataArray, 
    groups: xr.DataArray
) -> xr.DataArray:
    """Dependent sort - qcut within each group at each timestep.
    
    Nested groupby pattern:
    1. Group by categorical labels (e.g., 'small', 'big')
    2. Within each group, apply independent sort (group by time â†’ qcut)
    3. xarray automatically concatenates results back to (T, N) shape
    """
    def apply_qcut_within_group(group_data: xr.DataArray) -> xr.DataArray:
        """Apply qcut at each timestep within this group."""
        return self._quantile_independent(group_data)
    
    # Nested groupby: groups â†’ time â†’ qcut
    # xarray automatically concatenates results back
    result = data.groupby(groups).map(apply_qcut_within_group)
    return result
```

### 3.4.4. Visitor í†µí•© (Special Case Handling)

**CsQuantileì€ `visit_operator()`ì—ì„œ íŠ¹ë³„ ì²˜ë¦¬ í•„ìš” (group_by ì¡°íšŒ)**:

```python
# In EvaluateVisitor.visit_operator()
from alpha_canvas.ops.classification import CsQuantile

# Special handling for CsQuantile (needs group_by lookup)
if isinstance(node, CsQuantile):
    # 1. Evaluate child
    child_result = node.child.accept(self)
    
    # 2. Look up group_by field if specified
    group_labels = None
    if node.group_by is not None:
        if node.group_by not in self._data:
            raise ValueError(
                f"group_by field '{node.group_by}' not found in dataset"
            )
        group_labels = self._data[node.group_by]
    
    # 3. Delegate to compute()
    result = node.compute(child_result, group_labels)
    
    # 4. Apply universe masking (automatic)
    if self._universe_mask is not None:
        result = result.where(self._universe_mask, np.nan)
    
    # 5. Cache
    self._cache_result("CsQuantile", result)
    return result
```

### 3.4.5. í•µì‹¬ êµ¬í˜„ êµí›ˆ (ì‹¤í—˜ì—ì„œ ë°œê²¬)

**1. Flatten-Reshape íŒ¨í„´ í•„ìˆ˜:**
- `pd.qcut`ì€ 1D ë°°ì—´ë§Œ ë°›ìŒ
- `data_slice.values.flatten()` â†’ qcut â†’ `reshape(data_slice.shape)`
- ì´ íŒ¨í„´ ì—†ì´ëŠ” shape ë³´ì¡´ ë¶ˆê°€ëŠ¥

**2. xarray.groupby().map() vs .apply():**
- `.map()`ì´ xarray â†’ xarray ë³€í™˜ì— ë” ê¹”ë”
- ìë™ concatenationìœ¼ë¡œ shape ë³´ì¡´
- `.apply()`ë„ ì‘ë™í•˜ì§€ë§Œ pandas ë°˜í™˜ ì‹œ ì‚¬ìš©

**3. duplicates='drop' í•„ìˆ˜:**
- ëª¨ë“  ê°’ì´ ë™ì¼í•œ edge case ì²˜ë¦¬
- ëª¨ë“  NaNì¸ ê²½ìš° graceful degradation
- ì—ëŸ¬ ë°œìƒ ëŒ€ì‹  NaN ë°˜í™˜

**4. ì¢…ì† ì •ë ¬ ì„±ëŠ¥:**
- ë…ë¦½ ì •ë ¬: ~27ms for (10, 6) data
- ì¢…ì† ì •ë ¬: ~117ms for (10, 6) data (4.26x overhead)
- **í—ˆìš© ê°€ëŠ¥:** íŒ©í„° ì—°êµ¬ëŠ” ë°°ì¹˜ ì²˜ë¦¬ (ì‹¤ì‹œê°„ ì•„ë‹˜)

**5. ê²€ì¦ ë°©ë²•:**
- ë…ë¦½ vs ì¢…ì† ì •ë ¬ì˜ cutoffê°€ **ë‹¬ë¼ì•¼ í•¨**
- ì‹¤í—˜ì—ì„œ 17%ì˜ positionsê°€ ë‹¤ë¥¸ label ë°›ìŒ
- Fama-French ë…¼ë¬¸ methodologyì™€ ì¼ì¹˜

### 3.4.6. ì‚¬ìš© ì˜ˆì‹œ (ì‹¤ì œ ì½”ë“œ)

```python
from alpha_canvas.ops.classification import CsQuantile
from alpha_canvas.core.expression import Field

# ë…ë¦½ ì •ë ¬: ì „ì²´ ìœ ë‹ˆë²„ìŠ¤ì—ì„œ quantile
size_expr = CsQuantile(
    child=Field('market_cap'),
    bins=2,
    labels=['small', 'big']
)

# ì¢…ì† ì •ë ¬: size ê·¸ë£¹ ë‚´ì—ì„œ value quantile (Fama-French)
value_expr = CsQuantile(
    child=Field('book_to_market'),
    bins=3,
    labels=['low', 'mid', 'high'],
    group_by='size'  # 'size' fieldë¥¼ ë¨¼ì € ì¡°íšŒ â†’ ê° ê·¸ë£¹ë³„ quantile
)

# ì‚¬ìš©
rc.add_data('size', size_expr)  # ë¨¼ì € size ìƒì„±
rc.add_data('value', value_expr)  # size ê·¸ë£¹ ë‚´ì—ì„œ value ê³„ì‚°

# Boolean Expression í†µí•©
small_value = (rc.data['size'] == 'small') & (rc.data['value'] == 'high')
```

## 3.4. Property Accessor êµ¬í˜„ âœ… **IMPLEMENTED**

```python
from alpha_canvas.core.expression import Field


class DataAccessor:
    """Returns Field Expressions for lazy evaluation."""
    
    def __getitem__(self, field_name: str) -> Field:
        """Return Field Expression (not raw data!)"""
        if not isinstance(field_name, str):
            raise TypeError(
                f"Field name must be string, got {type(field_name).__name__}"
            )
        return Field(field_name)
    
    def __getattr__(self, name: str):
        """Prevent attribute access - item access only."""
        raise AttributeError(
            f"DataAccessor does not support attribute access. "
            f"Use rc.data['{name}'] instead of rc.data.{name}"
        )


# AlphaCanvas í†µí•©
class AlphaCanvas:
    def __init__(self, ...):
        self._data_accessor = DataAccessor()
    
    @property
    def data(self) -> DataAccessor:
        """Access data fields as Field Expressions."""
        return self._data_accessor
```

**ì‚¬ìš© ì˜ˆì‹œ**:

```python
# Basic field access
field = rc.data['size']  # Returns Field('size')

# Comparison creates Expression
mask = rc.data['size'] == 'small'  # Returns Equals Expression

# Evaluate
result = rc.evaluate(mask)  # Boolean DataArray with universe masking
```

---

## 3.4.2. Signal Assignment (Lazy Evaluation) âœ… **IMPLEMENTED**

### ê°œìš”

**Signal Assignment**ëŠ” Expression ê°ì²´ì— ê°’ì„ í• ë‹¹í•˜ì—¬ ì‹œê·¸ë„ì„ êµ¬ì„±í•˜ëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤. Fama-French íŒ©í„°ì™€ ê°™ì€ ë³µì¡í•œ ì‹œê·¸ë„ì„ ì§ê´€ì ì¸ ë¬¸ë²•ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**í•µì‹¬ ì„¤ê³„ ì›ì¹™**:
- **Lazy Evaluation**: í• ë‹¹ì€ ì €ì¥ë§Œ í•˜ê³  ì¦‰ì‹œ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ
- **Implicit Canvas**: ë³„ë„ì˜ ìº”ë²„ìŠ¤ ìƒì„± ì—†ì´ Expression ê²°ê³¼ê°€ ìº”ë²„ìŠ¤ ì—­í• 
- **Traceability**: Base resultì™€ final resultë¥¼ ë³„ë„ë¡œ ìºì‹±í•˜ì—¬ ì¶”ì  ê°€ëŠ¥
- **DRY Principle**: Lazy initializationìœ¼ë¡œ ëª¨ë“  Expressionì—ì„œ ìë™ ì‘ë™

### Expression.__setitem__ êµ¬í˜„ (DRY Lazy Initialization)

```python
class Expression(ABC):
    """Base class for all Expressions.
    
    Supports lazy assignment via __setitem__:
        signal[mask] = value  # Stores assignment, does not evaluate
    """
    
    def __setitem__(self, mask, value):
        """Store assignment for lazy evaluation.
        
        Uses lazy initialization - _assignments list is created on first use.
        This follows the DRY principle: no __post_init__ needed in subclasses.
        
        Args:
            mask: Boolean Expression or DataArray indicating where to assign
            value: Scalar value to assign where mask is True
        
        Note:
            Assignments are stored as (mask, value) tuples and applied sequentially
            during evaluation. Later assignments overwrite earlier ones for overlapping
            positions.
        """
        # Lazy initialization - create _assignments if it doesn't exist
        if not hasattr(self, '_assignments'):
            self._assignments = []
        
        self._assignments.append((mask, value))
```

**Lazy Initializationì˜ ì¥ì **:
1. âœ… **No Boilerplate**: ëª¨ë“  Expression ì„œë¸Œí´ë˜ìŠ¤ì— `__post_init__` ë¶ˆí•„ìš”
2. âœ… **DRY Principle**: ì¤‘ë³µ ì½”ë“œ ì œê±°
3. âœ… **Automatic**: ëª¨ë“  Expressionì—ì„œ ìë™ìœ¼ë¡œ ì‘ë™
4. âœ… **Efficient**: í• ë‹¹ì´ ì—†ìœ¼ë©´ `_assignments` ì†ì„±ë„ ìƒì„±ë˜ì§€ ì•ŠìŒ

### Visitor Integration

```python
class EvaluateVisitor:
    def evaluate(self, expr: Expression) -> xr.DataArray:
        """Evaluate expression and apply assignments if present."""
        # Step 1: Evaluate base expression (tree traversal)
        base_result = expr.accept(self)
        
        # Step 2: Check if expression has assignments (lazy initialization)
        assignments = getattr(expr, '_assignments', None)
        if assignments:
            # Cache base result for traceability
            base_name = f"{expr.__class__.__name__}_base"
            self._cache[self._step_counter] = (base_name, base_result)
            self._step_counter += 1
            
            # Apply assignments sequentially
            final_result = self._apply_assignments(base_result, assignments)
            
            # Apply universe masking to final result
            if self._universe_mask is not None:
                final_result = final_result.where(self._universe_mask)
            
            # Cache final result
            final_name = f"{expr.__class__.__name__}_with_assignments"
            self._cache[self._step_counter] = (final_name, final_result)
            self._step_counter += 1
            
            return final_result
        
        # No assignments, return base result as-is
        return base_result
    
    def _apply_assignments(self, base_result: xr.DataArray, assignments: list) -> xr.DataArray:
        """Apply assignments sequentially to base result."""
        result = base_result.copy(deep=True)
        
        for mask_expr, value in assignments:
            # If mask is an Expression, evaluate it
            if hasattr(mask_expr, 'accept'):
                mask_data = mask_expr.accept(self)
            else:
                # Already a DataArray or numpy array
                mask_data = mask_expr
            
            # Ensure mask is boolean (required for ~ operator)
            mask_bool = mask_data.astype(bool)
            
            # Apply assignment: replace values where mask is True
            result = result.where(~mask_bool, value)
        
        return result
```

### Constant Expression (Blank Canvas)

```python
from dataclasses import dataclass
import numpy as np
import xarray as xr
from alpha_canvas.core.expression import Expression


@dataclass(eq=False)
class Constant(Expression):
    """Expression that produces a constant-valued DataArray.
    
    Creates a universe-shaped (T, N) DataArray filled with the specified
    constant value. Serves as a "blank canvas" for signal construction.
    
    Example:
        >>> signal = Constant(0.0)  # Blank canvas (all zeros)
        >>> signal[mask1] = 1.0     # Assign long positions
        >>> signal[mask2] = -1.0    # Assign short positions
    """
    value: float
    
    def accept(self, visitor):
        return visitor.visit_constant(self)
```

### ì‚¬ìš© ì˜ˆì‹œ: Fama-French 2Ã—3 Factor

```python
# Step 1: Create size and value classifications
rc.add_data('size', CsQuantile(Field('market_cap'), bins=2, labels=['small', 'big']))
rc.add_data('value', CsQuantile(Field('book_to_market'), bins=3, labels=['low', 'medium', 'high']))

# Step 2: Create selector masks
is_small = rc.data['size'] == 'small'
is_big = rc.data['size'] == 'big'
is_low = rc.data['value'] == 'low'
is_high = rc.data['value'] == 'high'

# Step 3: Construct signal with lazy assignments
signal = Constant(0.0)                # Implicit blank canvas
signal[is_small & is_high] = 1.0      # Small/High-Value (long)
signal[is_big & is_low] = -1.0        # Big/Low-Value (short)

# Step 4: Evaluate (assignments applied here)
result = rc.evaluate(signal)

# Result: universe-shaped (T, N) array
#  - 1.0 where size=='small' AND value=='high'
#  - -1.0 where size=='big' AND value=='low'
#  - 0.0 elsewhere (neutral)
#  - NaN outside universe
```

### Overlapping Masks (Sequential Application)

```python
signal = Constant(0.0)
signal[is_small] = 0.5                # All small caps = 0.5
signal[is_small & is_high] = 1.0      # Small/High overwrites to 1.0

# Result: Later assignment wins for overlapping positions
#  - Small/High: 1.0 (overwritten)
#  - Small/Other: 0.5 (from first assignment)
#  - Others: 0.0 (from Constant)
```

### Traceability

```python
# After evaluation, check cached steps
for step_idx in sorted(rc._evaluator._cache.keys()):
    name, data = rc._evaluator._cache[step_idx]
    print(f"Step {step_idx}: {name}")

# Output:
#   Step 0: Constant_0.0_base             # Base constant array
#   Step 1: Field_size                     # Size classification
#   Step 2: Field_value                    # Value classification
#   Step 3: Equals                         # is_small mask
#   Step 4: Equals                         # is_high mask
#   Step 5: And                            # is_small & is_high
#   Step 6: Constant_0.0_with_assignments  # Final signal with assignments
```

**í•µì‹¬ ì¥ì **:
- Base resultì™€ final resultê°€ ë³„ë„ ë‹¨ê³„ë¡œ ìºì‹±ë¨
- PnL trackingì— í•„ìˆ˜ì ì¸ ê¸°ëŠ¥
- ê° í• ë‹¹ì˜ ì˜í–¥ì„ ë‹¨ê³„ë³„ë¡œ ì¶”ì  ê°€ëŠ¥

### Implementation Checklist

- âœ… `Expression.__setitem__` with lazy initialization (DRY)
- âœ… `Visitor.evaluate()` handles assignments
- âœ… `Visitor._apply_assignments()` sequential application
- âœ… `Constant` Expression for blank canvas
- âœ… `visit_constant()` in Visitor
- âœ… Boolean mask conversion (`.astype(bool)`)
- âœ… Universe masking integration
- âœ… Traceability (separate base/final caching)
- âœ… Tests: storage, evaluation, overlapping masks, caching
- âœ… Showcase: Fama-French 2Ã—3 factor construction

---

## 3.4.3. Portfolio Weight Scaling âœ… **IMPLEMENTED**

### ê°œìš”

**Portfolio Weight Scaling**ì€ ì„ì˜ì˜ ì‹œê·¸ë„ ê°’ì„ ì œì•½ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¤‘ì¹˜ë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤. Strategy Patternì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ë§ ì „ëµì„ í”ŒëŸ¬ê·¸ì¸ ë°©ì‹ìœ¼ë¡œ ì§€ì›í•©ë‹ˆë‹¤.

**êµ¬í˜„ ìœ„ì¹˜**: `src/alpha_canvas/portfolio/`
- `base.py`: WeightScaler ì¶”ìƒ ë² ì´ìŠ¤ í´ë˜ìŠ¤
- `strategies.py`: GrossNetScaler, DollarNeutralScaler, LongOnlyScaler

**í•µì‹¬ ì„¤ê³„ ì›ì¹™**:
- **Stateless**: ìŠ¤ì¼€ì¼ëŸ¬ëŠ” ìƒíƒœë¥¼ ì €ì¥í•˜ì§€ ì•ŠìŒ (í•­ìƒ ëª…ì‹œì  íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬)
- **Strategy Pattern**: ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ë§ ì „ëµì„ ì‰½ê²Œ êµì²´ ê°€ëŠ¥
- **Cross-Sectional**: ê° ì‹œì  ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬ (`groupby('time').map()` íŒ¨í„´)
- **NaN-Aware**: ìœ ë‹ˆë²„ìŠ¤ ë§ˆìŠ¤í‚¹ ìë™ ë³´ì¡´

### WeightScaler ë² ì´ìŠ¤ í´ë˜ìŠ¤

```python
from abc import ABC, abstractmethod
import xarray as xr


class WeightScaler(ABC):
    """Abstract base class for weight scaling strategies.
    
    Converts arbitrary signal values to portfolio weights by applying
    constraints cross-sectionally (independently for each time period).
    
    Philosophy:
    - Operates on (T, N) signal DataArray
    - Returns (T, N) weight DataArray
    - Each time slice processed independently
    - NaN-aware (respects universe masking)
    - Strategy pattern: subclasses define scaling logic
    """
    
    @abstractmethod
    def scale(self, signal: xr.DataArray) -> xr.DataArray:
        """Scale signal to weights.
        
        Args:
            signal: (T, N) DataArray with arbitrary signal values
        
        Returns:
            (T, N) DataArray with portfolio weights
        
        Note:
            - Must handle NaN values (preserve them in output)
            - Must process each time slice independently
            - Should validate inputs (not all NaN, etc.)
        """
        pass
    
    def _validate_signal(self, signal: xr.DataArray):
        """Validate signal before scaling."""
        if signal.dims != ('time', 'asset'):
            raise ValueError(
                f"Signal must have dims ('time', 'asset'), got {signal.dims}"
            )
        
        # Check if any time slice has non-NaN values
        non_nan_counts = (~signal.isnull()).sum(dim='asset')
        if (non_nan_counts == 0).all():
            raise ValueError(
                "All signal values are NaN across all time periods"
            )
```

### GrossNetScaler (í†µí•© í”„ë ˆì„ì›Œí¬)

```python
class GrossNetScaler(WeightScaler):
    """Unified weight scaler based on gross and net exposure targets.
    
    Uses the unified framework:
        L_target = (G + N) / 2
        S_target = (G - N) / 2
    
    Where:
        G = target_gross_exposure = sum(abs(weights))
        N = target_net_exposure = sum(weights)
        L = sum of positive weights
        S = sum of negative weights (negative value)
    
    Args:
        target_gross: Target gross exposure (default: 2.0 for 200% gross)
        target_net: Target net exposure (default: 0.0 for dollar-neutral)
    
    Example:
        >>> # Dollar neutral: L=1.0, S=-1.0
        >>> scaler = GrossNetScaler(target_gross=2.0, target_net=0.0)
        >>> 
        >>> # Net long 10%: L=1.1, S=-0.9
        >>> scaler = GrossNetScaler(target_gross=2.0, target_net=0.2)
        >>> 
        >>> # Crypto futures: L=0.5, S=-0.5
        >>> scaler = GrossNetScaler(target_gross=1.0, target_net=0.0)
    """
    
    def __init__(self, target_gross: float = 2.0, target_net: float = 0.0):
        self.target_gross = target_gross
        self.target_net = target_net
        
        # Validate constraints
        if target_gross < 0:
            raise ValueError("target_gross must be non-negative")
        if abs(target_net) > target_gross:
            raise ValueError(
                "Absolute net exposure cannot exceed gross exposure"
            )
        
        # Calculate target long and short books
        self.L_target = (target_gross + target_net) / 2.0
        self.S_target = (target_net - target_gross) / 2.0  # Negative value
    
    def scale(self, signal: xr.DataArray) -> xr.DataArray:
        """Scale signal using fully vectorized gross/net exposure constraints.
        
        Key innovation: NO ITERATION - pure vectorized operations.
        Always meets gross target, even for one-sided signals.
        """
        self._validate_signal(signal)
        
        # Step 1: Separate positive/negative (vectorized)
        s_pos = signal.where(signal > 0, 0.0)
        s_neg = signal.where(signal < 0, 0.0)
        
        # Step 2: Sum along asset dimension (vectorized)
        sum_pos = s_pos.sum(dim='asset', skipna=True)  # Shape: (time,)
        sum_neg = s_neg.sum(dim='asset', skipna=True)  # Shape: (time,)
        
        # Step 3: Normalize (vectorized, handles 0/0 â†’ nan â†’ 0)
        norm_pos = (s_pos / sum_pos).fillna(0.0)
        norm_neg_abs = (np.abs(s_neg) / np.abs(sum_neg)).fillna(0.0)
        
        # Step 4: Apply L/S targets (vectorized)
        weights_long = norm_pos * self.L_target
        weights_short_mag = norm_neg_abs * np.abs(self.S_target)
        
        # Step 5: Combine (subtract to make short side negative)
        weights = weights_long - weights_short_mag
        
        # Step 6: Calculate actual gross per row (vectorized)
        actual_gross = np.abs(weights).sum(dim='asset', skipna=True)  # Shape: (time,)
        
        # Step 7: Scale to meet target gross (vectorized)
        # Use xr.where to avoid inf from 0/0
        scale_factor = xr.where(actual_gross > 0, self.target_gross / actual_gross, 1.0)
        final_weights = weights * scale_factor
        
        # Step 8: Convert computational NaN to 0 (BEFORE universe mask)
        final_weights = final_weights.fillna(0.0)
        
        # Step 9: Apply universe mask (preserves NaN where signal was NaN)
        final_weights = final_weights.where(~signal.isnull())
        
        return final_weights
```

### í¸ì˜ Scaler í´ë˜ìŠ¤ë“¤

```python
class DollarNeutralScaler(GrossNetScaler):
    """Dollar neutral: sum(long) = 1.0, sum(short) = -1.0.
    
    Convenience wrapper for GrossNetScaler(2.0, 0.0).
    
    This is the most common scaler for market-neutral strategies.
    """
    def __init__(self):
        super().__init__(target_gross=2.0, target_net=0.0)


class LongOnlyScaler(WeightScaler):
    """Long-only portfolio: sum(weights) = target_long.
    
    Ignores negative signals, normalizes positive signals to sum to target.
    Uses vectorized operations for efficiency.
    
    Args:
        target_long: Target sum of weights (default: 1.0)
    
    Example:
        >>> scaler = LongOnlyScaler(target_long=1.0)
        >>> # All negative signals become 0, positives sum to 1.0
    """
    
    def __init__(self, target_long: float = 1.0):
        self.target_long = target_long
    
    def scale(self, signal: xr.DataArray) -> xr.DataArray:
        """Scale using vectorized long-only normalization."""
        self._validate_signal(signal)
        
        # Only keep positive values (vectorized)
        s_pos = signal.where(signal > 0, 0.0)
        
        # Sum along asset dimension (vectorized)
        sum_pos = s_pos.sum(dim='asset', skipna=True)  # Shape: (time,)
        
        # Normalize and scale (vectorized, handles 0/0 â†’ nan â†’ 0)
        weights = (s_pos / sum_pos * self.target_long).fillna(0.0)
        
        # Preserve NaN where signal was NaN (universe masking)
        return weights.where(~signal.isnull())
```

### Facade í†µí•©

```python
# In AlphaCanvas class (src/alpha_canvas/core/facade.py)

def scale_weights(
    self, 
    signal: Union[Expression, xr.DataArray], 
    scaler: 'WeightScaler'
) -> xr.DataArray:
    """Scale signal to portfolio weights.
    
    Args:
        signal: Expression or DataArray with signal values
        scaler: WeightScaler strategy instance (REQUIRED)
    
    Returns:
        (T, N) DataArray with portfolio weights
    
    Note:
        Scaler is a required parameter - no default.
        This is intentional for explicit, research-friendly API.
    
    Example:
        >>> from alpha_canvas.portfolio import DollarNeutralScaler
        >>> 
        >>> signal = ts_mean(Field('returns'), 5)
        >>> scaler = DollarNeutralScaler()
        >>> weights = rc.scale_weights(signal, scaler)
        >>> 
        >>> # Compare multiple scalers
        >>> w1 = rc.scale_weights(signal, DollarNeutralScaler())
        >>> w2 = rc.scale_weights(signal, LongOnlyScaler(1.0))
    """
    # Evaluate if Expression
    if hasattr(signal, 'accept'):
        signal_data = self.evaluate(signal)
    else:
        signal_data = signal
    
    # Apply scaling strategy
    weights = scaler.scale(signal_data)
    
    return weights
```

### ì‚¬ìš© íŒ¨í„´ ë° ì˜ˆì‹œ

**íŒ¨í„´ 1: ì§ì ‘ ì‚¬ìš© (ê°€ì¥ ëª…ì‹œì )**

```python
from alpha_canvas.portfolio import DollarNeutralScaler

# 1. Signal ìƒì„±
signal_expr = ts_mean(Field('returns'), 5)
signal_data = rc.evaluate(signal_expr)

# 2. Scaler ìƒì„± ë° ì ìš©
scaler = DollarNeutralScaler()
weights = scaler.scale(signal_data)

# ê²€ì¦
assert abs(weights[weights > 0].sum() - 1.0) < 1e-6  # Long = 1.0
assert abs(weights[weights < 0].sum() + 1.0) < 1e-6  # Short = -1.0
```

**íŒ¨í„´ 2: Facade í¸ì˜ ë©”ì„œë“œ**

```python
from alpha_canvas.portfolio import GrossNetScaler

signal_expr = ts_mean(Field('returns'), 5)
scaler = GrossNetScaler(target_gross=2.0, target_net=0.2)

# evaluate + scale í•œ ë²ˆì—
weights = rc.scale_weights(signal_expr, scaler)
```

**íŒ¨í„´ 3: ì—¬ëŸ¬ ìŠ¤ì¼€ì¼ëŸ¬ ë¹„êµ (ì—°êµ¬ìš©)**

```python
from alpha_canvas.portfolio import (
    DollarNeutralScaler,
    GrossNetScaler,
    LongOnlyScaler
)

# ë™ì¼ ì‹œê·¸ë„ì— ì—¬ëŸ¬ ìŠ¤ì¼€ì¼ë§ ì „ëµ ì ìš©
signal = rc.evaluate(my_alpha_expr)

strategies = {
    'dollar_neutral': DollarNeutralScaler(),
    'net_long_10pct': GrossNetScaler(2.0, 0.2),
    'long_only': LongOnlyScaler(1.0),
    'crypto_futures': GrossNetScaler(1.0, 0.0)
}

weights_dict = {
    name: scaler.scale(signal)
    for name, scaler in strategies.items()
}

# ê° ì „ëµ ë¹„êµ
for name, weights in weights_dict.items():
    print(f"{name}:")
    print(f"  Gross: {abs(weights).sum()}")
    print(f"  Net: {weights.sum()}")
```

### Module Structure

```
src/alpha_canvas/portfolio/
â”œâ”€â”€ __init__.py              # Export all scalers
â”œâ”€â”€ base.py                  # WeightScaler abstract base class
â””â”€â”€ strategies.py            # GrossNetScaler, DollarNeutralScaler, LongOnlyScaler
```

### í…ŒìŠ¤íŠ¸ ì „ëµ

**1. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸** (`tests/test_portfolio/test_strategies.py`):
- `GrossNetScaler` ìˆ˜í•™ ê²€ì¦ (L_target, S_target ê³„ì‚°)
- ê° scalerì˜ constraint ì¶©ì¡± í™•ì¸
- NaN ë³´ì¡´ ê²€ì¦
- Edge cases: all positive, all negative, zeros

**2. í†µí•© í…ŒìŠ¤íŠ¸**:
- AlphaCanvas.scale_weights() í†µí•©
- Expression â†’ evaluation â†’ scaling ì „ì²´ íŒŒì´í”„ë¼ì¸
- Universe masking ë³´ì¡´ ê²€ì¦

**3. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸**:
- í¬ë¡œìŠ¤-ì„¹ì…˜ ë…ë¦½ì„± ê²€ì¦
- Large dataset (T=1000, N=3000) ë²¤ì¹˜ë§ˆí¬

### Implementation Checklist

- [x] `WeightScaler` abstract base class âœ… **DONE** (`portfolio/base.py`)
- [x] `GrossNetScaler` with unified framework (fully vectorized) âœ… **DONE** (`portfolio/strategies.py`)
- [x] `DollarNeutralScaler` convenience wrapper âœ… **DONE** (`portfolio/strategies.py`)
- [x] `LongOnlyScaler` implementation (fully vectorized) âœ… **DONE** (`portfolio/strategies.py`)
- [x] `AlphaCanvas.scale_weights()` facade method âœ… **DONE** (`core/facade.py`)
- [x] Unit tests for each scaler âœ… **DONE** (`tests/test_portfolio/`)
- [x] Integration tests with facade âœ… **DONE** (`tests/test_core/test_facade_weights.py`)
- [x] **Experiment: weight scaling validation** (exp_18_weight_scaling.py - ALL PASS âœ…)
- [x] **Showcase: Fama-French signal â†’ weights** âœ… **DONE** (`showcase/14_weight_scaling.py`, `showcase/16_backtest_attribution.py`)
- [x] **Documentation: FINDINGS.md updated** âœ…
- [x] **Documentation: architecture.md updated** âœ…
- [x] **Documentation: implementation.md updated** âœ…

**Performance Validated**:
- Small (10Ã—6): 7ms
- Medium (100Ã—50): 7ms
- 1Y Daily (252Ã—100): 8ms
- Large (1000Ã—500): 34ms
- **10x-220x speedup** vs iterative approach

---

## 3.5. ê°œë°œ ì›ì¹™

### 3.5.1. ì§€ì—° í‰ê°€ (Lazy Evaluation)

- `Expression` ê°ì²´ëŠ” "ë ˆì‹œí”¼"ì´ë©° ë°ì´í„°ë¥¼ ê°€ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.
- `EvaluateVisitor`ê°€ ì‹¤ì œ í‰ê°€ë¥¼ ë‹´ë‹¹í•˜ë©°, ì´ë•Œ ìºì‹±ì´ ë°œìƒí•©ë‹ˆë‹¤.
- ë¶ˆí•„ìš”í•œ ì¬ê³„ì‚°ì„ ë°©ì§€í•˜ì—¬ ì„±ëŠ¥ì„ ìµœì í™”í•©ë‹ˆë‹¤.

### 3.5.2. ë ˆì´ë¸” ìš°ì„  (Label-first)

- ëª¨ë“  ë²„í‚· ì—°ì‚°ì€ ì •ìˆ˜ ì¸ë±ìŠ¤ ëŒ€ì‹  **ì˜ë¯¸ ìˆëŠ” ë ˆì´ë¸”**ì„ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
- ì˜ˆ: `cs_quantile(..., labels=['small', 'mid', 'big'])`
- ì´ëŠ” PRDì˜ í•µì‹¬ ë¬¸ì œ 2ë¥¼ í•´ê²°í•˜ëŠ” ì„¤ê³„ ì›ì¹™ì…ë‹ˆë‹¤.

### 3.5.3. ì¶”ì ì„± ìš°ì„  (Traceability-first)

- ëª¨ë“  ì¤‘ê°„ ê³„ì‚° ê²°ê³¼ëŠ” **ì •ìˆ˜ step ì¸ë±ìŠ¤**ë¡œ ìºì‹œë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
- ì‚¬ìš©ìëŠ” ì¬ê³„ì‚° ì—†ì´ ëª¨ë“  ì¤‘ê°„ ë‹¨ê³„ë¥¼ ê²€ì‚¬í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
- ì´ëŠ” PRDì˜ í•µì‹¬ ë¬¸ì œ 3ì„ í•´ê²°í•˜ëŠ” ì„¤ê³„ ì›ì¹™ì…ë‹ˆë‹¤.

### 3.5.4. Pythonic ìš°ì„  (Pythonic-first)

- ë¬¸ìì—´ DSL ëŒ€ì‹  Pythonì˜ ë„¤ì´í‹°ë¸Œ ë¬¸ë²•ì„ í™œìš©í•©ë‹ˆë‹¤.
- ì˜ˆ: `&`, `|`, `[]`, `=` ì—°ì‚°ì ì˜¤ë²„ë¡œë”©
- IDE ìë™ì™„ì„± ë° íƒ€ì… íŒíŠ¸ë¥¼ ìµœëŒ€í•œ í™œìš©í•©ë‹ˆë‹¤.

### 3.5.5. ì¢…ì† ì •ë ¬ ì§€ì› (Dependent Sort Support)

- `cs_quantile`ì€ `group_by` íŒŒë¼ë¯¸í„°ë¡œ ì¢…ì† ì •ë ¬ì„ ì§€ì›í•´ì•¼ í•©ë‹ˆë‹¤.
- ì´ëŠ” Fama-French íŒ©í„° ì¬í˜„ì„ ìœ„í•œ í•µì‹¬ ìš”êµ¬ì‚¬í•­ì…ë‹ˆë‹¤.
- `mask` íŒŒë¼ë¯¸í„°ë¡œ ë¡œìš°ë ˆë²¨ ì»¤ìŠ¤í„°ë§ˆì´ì§•ë„ ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤.

## 3.6. í…ŒìŠ¤íŠ¸ ì „ëµ

### 3.6.1. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

- `Expression` ê° ë…¸ë“œ í´ë˜ìŠ¤
- `EvaluateVisitor` ë©”ì„œë“œë³„ í…ŒìŠ¤íŠ¸ (íŠ¹íˆ `visit_cs_quantile`ì˜ `group_by` ë¡œì§)
- `ConfigLoader` YAML íŒŒì‹± í…ŒìŠ¤íŠ¸
- ì •ìˆ˜ step ì¸ë±ì‹± ë¡œì§

### 3.6.2. í†µí•© í…ŒìŠ¤íŠ¸

- ì „ì²´ ì›Œí¬í”Œë¡œìš° (ì´ˆê¸°í™” â†’ ë°ì´í„° ë¡œë“œ â†’ Expression í‰ê°€ â†’ PnL ì¶”ì )
- ë“€ì–¼ ì¸í„°í˜ì´ìŠ¤ ì¡°í•© ì‹œë‚˜ë¦¬ì˜¤
- ë…ë¦½/ì¢…ì† ì´ì¤‘ ì •ë ¬ ê¸°ë°˜ íŒ©í„° ìˆ˜ìµë¥  ê³„ì‚° íŒ¨í„´
- Fama-French SMB, HML íŒ©í„° ì¬í˜„ ê²€ì¦

### 3.6.3. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

- ëŒ€ìš©ëŸ‰ ë°ì´í„° (T=1000, N=3000) ì²˜ë¦¬ ì‹œê°„
- ìºì‹± íš¨ê³¼ ê²€ì¦ (stepë³„ ì¡°íšŒ ì„±ëŠ¥)
- ì¢…ì† ì •ë ¬ì˜ ì˜¤ë²„í—¤ë“œ ì¸¡ì •
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§

## 3.7. ì½”ë”© ì»¨ë²¤ì…˜

- **íƒ€ì… íŒíŠ¸:** ëª¨ë“  public ë©”ì„œë“œì— íƒ€ì… íŒíŠ¸ í•„ìˆ˜
- **Docstring:** Google ìŠ¤íƒ€ì¼ docstring ì‚¬ìš©
- **Linting:** `ruff` ì‚¬ìš©
- **Formatting:** `black` ì‚¬ìš©
- **Import ìˆœì„œ:** í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ â†’ ì„œë“œíŒŒí‹° â†’ ë¡œì»¬

## 3.8. ì¸í„°í˜ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ

### 3.8.1. Step ì¸ë±ì‹± ë³€ê²½ì‚¬í•­

**ì´ì „ (ë¬¸ìì—´ ê¸°ë°˜):**

```python
# âŒ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
rc.trace_pnl('alpha1', step='ts_mean')
rc.get_intermediate('alpha1', step='ts_mean')
```

**í˜„ì¬ (ì •ìˆ˜ ê¸°ë°˜):**

```python
# âœ… ì˜¬ë°”ë¥¸ ì‚¬ìš©ë²•
rc.trace_pnl('alpha1', step=1)  # step 1ê¹Œì§€ ì¶”ì 
rc.get_intermediate('alpha1', step=1)  # step 1 ë°ì´í„° ì¡°íšŒ

# ëª¨ë“  ë‹¨ê³„ ì¶”ì 
rc.trace_pnl('alpha1')  # step=None (ê¸°ë³¸ê°’)
# ë°˜í™˜: {0: {...}, 1: {...}, 2: {...}}
```

### 3.8.2. ë…ë¦½/ì¢…ì† ì •ë ¬ íŒ¨í„´

**ë…ë¦½ ì •ë ¬ (ë³€ê²½ ì—†ìŒ):**

```python
# âœ… ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ì‘ë™
rc.add_axis('size', cs_quantile(rc.data.mcap, bins=2, labels=['small','big']))
```

**ì¢…ì† ì •ë ¬ (ì‹ ê·œ ê¸°ëŠ¥):**

```python
# âœ… ìƒˆë¡œìš´ ê¸°ëŠ¥
rc.add_axis('value', cs_quantile(rc.data.btm, bins=3, labels=['low','mid','high'],
                                   group_by='size'))
```

**ë§ˆìŠ¤í¬ ê¸°ë°˜ í•„í„°ë§ (ì‹ ê·œ ê¸°ëŠ¥):**

```python
# âœ… ìƒˆë¡œìš´ ê¸°ëŠ¥
mask = rc.data.volume > threshold
rc.add_axis('filtered', cs_quantile(rc.data.returns, bins=5, labels=[...],
                                      mask=mask))
```

## 3.9. êµ¬í˜„ ì„±ê³µ ê¸°ì¤€

### 3.9.1. Step ì¸ë±ì‹± ê²€ì¦

âœ… **í•„ìˆ˜ ë™ì‘:**

- `rc.trace_pnl('alpha', step=2)` â†’ step 2ê¹Œì§€ì˜ PnL ë°˜í™˜
- `rc.get_intermediate('alpha', step=2)` â†’ step 2ì˜ ìºì‹œëœ DataArray ë°˜í™˜
- ë³‘ë ¬ Expression (ë¸Œëœì¹˜ê°€ ìˆëŠ” íŠ¸ë¦¬)ì—ì„œ ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ ì¸ë±ì‹±
- ì˜ëª»ëœ step ì¸ë±ìŠ¤ ì…ë ¥ ì‹œ ëª…í™•í•œ ì—ëŸ¬ ë©”ì‹œì§€

### 3.9.2. ì¢…ì† ì •ë ¬ ê²€ì¦

âœ… **í•„ìˆ˜ ë™ì‘:**

- **ë…ë¦½ ì •ë ¬**: `cs_quantile(...)` â†’ ì „ì²´ ìœ ë‹ˆë²„ìŠ¤ ëŒ€ìƒ quantile
- **ì¢…ì† ì •ë ¬**: `cs_quantile(..., group_by='axis')` â†’ ê° ê·¸ë£¹ ë‚´ quantile
- **ë§ˆìŠ¤í¬ í•„í„°ë§**: `cs_quantile(..., mask=...)` â†’ í•„í„°ë§ëœ ë¶€ë¶„ì§‘í•© ëŒ€ìƒ quantile
- ë…ë¦½/ì¢…ì† ì •ë ¬ì˜ ê²°ê³¼ cutoffê°€ ëª…í™•íˆ ë‹¤ë¦„ (ê²€ì¦ í…ŒìŠ¤íŠ¸ í•„ìš”)

### 3.9.3. Fama-French ì¬í˜„ ê²€ì¦

âœ… **í•„ìˆ˜ ë™ì‘:**

- SMB (ë…ë¦½ 2Ã—3 ì •ë ¬) â†’ ì˜ˆìƒëœ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¤‘ì¹˜ ìƒì„±
- HML (ì¢…ì† 2Ã—3 ì •ë ¬) â†’ ì˜ˆìƒëœ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¤‘ì¹˜ ìƒì„±
- ë…ë¦½/ì¢…ì† ë°©ì‹ì˜ cutoff ì°¨ì´ ê²€ì¦ (academic paper ê¸°ì¤€ê³¼ ì¼ì¹˜)

## 3.10. êµ¬í˜„ í˜„í™© ë° ë‹¤ìŒ ë‹¨ê³„

### âœ… Phase 1: í•µì‹¬ ì»´í¬ë„ŒíŠ¸ êµ¬í˜„ **COMPLETE**

- [x] `Expression` ì¶”ìƒ í´ë˜ìŠ¤ ë° Leaf/Composite êµ¬í˜„ (`core/expression.py`)
- [x] `EvaluateVisitor` ê¸°ë³¸ êµ¬ì¡° ë° ìºì‹± ë©”ì»¤ë‹ˆì¦˜ (ì •ìˆ˜ step ì¹´ìš´í„° í¬í•¨) (`core/visitor.py`)
- [x] `ConfigLoader` ë° YAML íŒŒì‹± (`core/config.py`)
- [x] `AlphaCanvas` Facade ê¸°ë³¸ êµ¬ì¡° (`core/facade.py`)
- [x] `DataPanel` ëª¨ë¸ (`core/data_model.py`)
- [x] `DataLoader` Parquet í†µí•© (`core/data_loader.py`)

### âœ… Phase 2: ì—°ì‚°ì êµ¬í˜„ **COMPLETE**

- [x] Timeseries ì—°ì‚°ì (`ts_mean`, `ts_any` ë“±) (`ops/timeseries.py`)
- [x] Cross-sectional ì—°ì‚°ì (`cs_rank`, `cs_quantile` with `group_by` and `mask`) (`ops/crosssection.py`, `ops/classification.py`)
- [x] Boolean ì—°ì‚°ì (`==`, `!=`, `<`, `>`, `&`, `|`, `~`) (`ops/logical.py`)
- [x] Constant ì—°ì‚°ì (blank canvas) (`ops/constants.py`)

### âœ… Phase 3: ì¶”ì ì„± êµ¬í˜„ **COMPLETE**

- [x] Triple-cache ì•„í‚¤í…ì²˜ (signal, weight, port_return) (`core/visitor.py`)
- [x] ì •ìˆ˜ ì¸ë±ìŠ¤ ê¸°ë°˜ step ì¶”ì  (`_step_counter`)
- [x] ê³µê°œ API: `get_signal()`, `get_weights()`, `get_port_return()`, `get_daily_pnl()`, `get_cumulative_pnl()` (`core/facade.py`)
- [x] Backtesting: Shift-mask workflow, position-level returns (`core/visitor.py`)

### âœ… Phase 4: ì¸í„°í˜ì´ìŠ¤ ì™„ì„± **COMPLETE**

- [x] Property accessor `rc.data` (Expression ë°˜í™˜) (`utils/accessor.py`)
- [x] Signal assignment `expr[mask] = value` (lazy evaluation) (`core/expression.py`)
- [x] Boolean Expression ì§€ì› (comparison + logical operators) (`core/expression.py`, `ops/logical.py`)
- [x] Universe masking (double masking strategy) (`core/facade.py`, `core/visitor.py`)

### âœ… Phase 5: í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„± **COMPLETE**

- [x] `WeightScaler` abstract base class (`portfolio/base.py`)
- [x] `GrossNetScaler` (unified framework, fully vectorized) (`portfolio/strategies.py`)
- [x] `DollarNeutralScaler` convenience wrapper (`portfolio/strategies.py`)
- [x] `LongOnlyScaler` (`portfolio/strategies.py`)
- [x] Facade í†µí•©: `rc.scale_weights()`, `rc.evaluate(expr, scaler=...)` (`core/facade.py`)

### âœ… Phase 6: ê²€ì¦ ë° í…ŒìŠ¤íŠ¸ **COMPLETE**

- [x] 176ê°œ ë‹¨ìœ„ ë° í†µí•© í…ŒìŠ¤íŠ¸ (`tests/`)
- [x] 18ê°œ ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸ (`experiments/`)
- [x] 16ê°œ showcase ì˜ˆì œ (`showcase/`)
- [x] Fama-French íŒ¨í„´ ê²€ì¦ (`showcase/12_cs_quantile.py`, `showcase/13_signal_assignment.py`)
- [x] ì¢…ì† ì •ë ¬ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (exp_12)

---

### ğŸ“‹ **ë¯¸êµ¬í˜„ ê¸°ëŠ¥** (alpha-labìœ¼ë¡œ ì´ê´€ ì˜ˆì •)

- [ ] `PnLTracer` ì»´í¬ë„ŒíŠ¸ â†’ **alpha-lab**ìœ¼ë¡œ ì´ê´€
- [ ] ì„±ê³¼ ì§€í‘œ ê³„ì‚° (Sharpe, drawdown, etc.) â†’ **alpha-lab**ë¡œ ì´ê´€
- [ ] PnL ì‹œê°í™” (curves, heatmaps) â†’ **alpha-lab**ë¡œ ì´ê´€
- [ ] Step ë¹„êµ ë° ë¶„ì„ â†’ **alpha-lab**ë¡œ ì´ê´€

**ê·¼ê±°**: alpha-canvasëŠ” **compute engine** (signal ìƒì„±, weight ê³„ì‚°, portfolio return ê³„ì‚°)ì— ì§‘ì¤‘í•˜ë©°, ë¶„ì„ ë° ì‹œê°í™”ëŠ” **alpha-lab**ì—ì„œ ì œê³µí•©ë‹ˆë‹¤. ì´ëŠ” PRD Section 1.9 ë° 2.1ì— ëª…ì‹œëœ ì•„í‚¤í…ì²˜ ë¶„ë¦¬ ì›ì¹™ì„ ë”°ë¦…ë‹ˆë‹¤.

---

### ğŸ“¦ **ë‹¤ìŒ ì£¼ìš” ì‘ì—…: Monorepo ë¦¬íŒ©í† ë§ ë° alpha-database ë§ˆì´ê·¸ë ˆì´ì…˜**

**ëª©í‘œ**: ë°ì´í„° ë¡œë”© ì±…ì„ì„ alpha-database íŒ¨í‚¤ì§€ë¡œ ë¶„ë¦¬

#### ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš:

**Phase 1: alpha-database íŒ¨í‚¤ì§€ êµ¬í˜„**
1. Config-driven data loader í¬íŒ… (`core/config.py`, `core/data_loader.py` â†’ alpha-database)
2. Multi-backend ì§€ì› (Parquet, CSV, Excel, DuckDB)
3. Write capabilities (Dataset/Alpha/Factor catalogs)
4. Catalog explorer ë° metadata ê´€ë¦¬

**Phase 2: alpha-canvas í†µí•©**
1. Dependency injection: `AlphaCanvas(data_source=alpha_database_instance)`
2. `Field('adj_close')` â†’ alpha-database ìë™ í˜¸ì¶œ
3. Backward compatibility ìœ ì§€ (ê¸°ì¡´ data_loader ìœ ì§€)

**Phase 3: ê²€ì¦ ë° ì „í™˜**
1. 176ê°œ í…ŒìŠ¤íŠ¸ ëª¨ë‘ alpha-database ì‚¬ìš©ìœ¼ë¡œ ì „í™˜
2. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (ê¸°ì¡´ê³¼ ë™ì¼ ë˜ëŠ” ê°œì„ )
3. ê¸°ì¡´ data_loader ì œê±° (alpha-databaseë¡œ ì™„ì „ ëŒ€ì²´)

**Phase 4: ì˜ì¡´ì„± ì •ë¦¬ (`pyproject.toml`)**
1. alpha-canvas core: `pandas`, `xarray`ë§Œ ìœ ì§€
2. alpha-database extras: `duckdb`, `pyyaml`, `pyarrow`
3. alpha-lab extras: `scipy`, `matplotlib` (ë¯¸ë˜)

**ì°¸ê³ **: 
- alpha-canvasì˜ í˜„ì¬ êµ¬í˜„ì€ **ì™„ì „íˆ ì‘ë™í•˜ë©° í”„ë¡œë•ì…˜ ì¤€ë¹„ ìƒíƒœ**ì…ë‹ˆë‹¤.
- alpha-database ë§ˆì´ê·¸ë ˆì´ì…˜ì€ **ì•„í‚¤í…ì²˜ ê°œì„ **ì„ ìœ„í•œ ë¦¬íŒ©í† ë§ì´ë©°, ê¸°ëŠ¥ ë³€ê²½ì´ ì•„ë‹™ë‹ˆë‹¤.
- ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘ì—ë„ backward compatibilityë¥¼ ìœ ì§€í•˜ì—¬ ë¦¬ìŠ¤í¬ë¥¼ ìµœì†Œí™”í•©ë‹ˆë‹¤.

---

**ì°¸ê³ :** ì´ ë¬¸ì„œëŠ” ì‹¤ì œ êµ¬í˜„ ê³¼ì •ì—ì„œ ë°œê²¬ë˜ëŠ” ìƒˆë¡œìš´ íŒ¨í„´ê³¼ êµí›ˆì„ ì§€ì†ì ìœ¼ë¡œ ë°˜ì˜í•´ì•¼ í•©ë‹ˆë‹¤ (Living Document).
