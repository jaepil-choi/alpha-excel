# 3. Implementation Guide

ì´ ë¬¸ì„œëŠ” alpha-canvasì˜ êµ¬ì²´ì ì¸ êµ¬í˜„ ë°©ë²•ë¡ , ì¸í„°íŽ˜ì´ìŠ¤ ì„¤ê³„, ê·¸ë¦¬ê³  ê°œë°œ í‘œì¤€ì„ ì •ì˜í•©ë‹ˆë‹¤.

## 3.1. í”„ë¡œì íŠ¸ êµ¬ì¡°

```text
alpha-canvas/
â”œâ”€â”€ config/                      # íƒ€ìž…ë³„ ì„¤ì • íŒŒì¼
â”‚   â”œâ”€â”€ data.yaml               # ë°ì´í„° í•„ë“œ ì •ì˜
â”‚   â”œâ”€â”€ db.yaml                 # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì • (ì„ íƒì )
â”‚   â””â”€â”€ compute.yaml            # ê³„ì‚° ê´€ë ¨ ì„¤ì • (ì„ íƒì )
â”œâ”€â”€ src/
â”‚   â””â”€â”€ alpha_canvas/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ core/
â”‚       â”‚   â”œâ”€â”€ facade.py       # AlphaCanvas (rc) í¼ì‚¬ë“œ í´ëž˜ìŠ¤
â”‚       â”‚   â”œâ”€â”€ expression.py   # Expression ì»´í¬ì§“ íŠ¸ë¦¬
â”‚       â”‚   â”œâ”€â”€ visitor.py      # EvaluateVisitor íŒ¨í„´ (íƒ€ìž… ê²€ì‚¬ í¬í•¨)
â”‚       â”‚   â””â”€â”€ config.py       # ConfigLoader
â”‚       â”œâ”€â”€ ops/                # ì—°ì‚°ìž (ts_mean, rank, etc.)
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ timeseries.py   # ts_mean, ts_sum, etc. (ë‹¤í˜•ì„± ì—°ì‚°ìž)
â”‚       â”‚   â”œâ”€â”€ crosssection.py # cs_rank ë“± (Panel ì „ìš© ì—°ì‚°ìž)
â”‚       â”‚   â”œâ”€â”€ classification.py # cs_quantile, cs_cut (ë¶„ë¥˜ê¸°/ì¶• ìƒì„±)
â”‚       â”‚   â”œâ”€â”€ transform.py    # group_neutralize, etc.
â”‚       â”‚   â””â”€â”€ tensor.py       # ë¯¸ëž˜ í™•ìž¥ìš© (MVPì—ì„œëŠ” ë¹„ì–´ìžˆìŒ)
â”‚       â”œâ”€â”€ portfolio/          # í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„±
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ base.py         # WeightScaler ì¶”ìƒ ë² ì´ìŠ¤ í´ëž˜ìŠ¤
â”‚       â”‚   â””â”€â”€ strategies.py   # GrossNetScaler, DollarNeutralScaler, LongOnlyScaler
â”‚       â”œâ”€â”€ analysis/
â”‚       â”‚   â”œâ”€â”€ pnl.py          # PnLTracer
â”‚       â”‚   â””â”€â”€ metrics.py      # ì„±ê³¼ ì§€í‘œ ê³„ì‚°
â”‚       â””â”€â”€ utils/
â”‚           â”œâ”€â”€ accessor.py     # Property ì ‘ê·¼ìž (data, axis, rules)
â”‚           â””â”€â”€ mask.py         # ë§ˆìŠ¤í¬ í—¬í¼
â”œâ”€â”€ experiments/                # ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ tests/                      # í…ŒìŠ¤íŠ¸
â””â”€â”€ docs/
    â””â”€â”€ vibe_coding/
        â”œâ”€â”€ prd.md
        â”œâ”€â”€ architecture.md
        â””â”€â”€ implementation.md   # ì´ ë¬¸ì„œ
```

## 3.2. í•µì‹¬ ì¸í„°íŽ˜ì´ìŠ¤ ì„¤ê³„

### 3.2.1. ì´ˆê¸°í™” ë° ì„¤ì •

```python
from alpha_canvas import AlphaCanvas

# config/ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  YAML íŒŒì¼ì„ ìžë™ ë¡œë“œ
rc = AlphaCanvas()

# ë˜ëŠ” íŠ¹ì • config ë””ë ‰í† ë¦¬ ì§€ì •
rc = AlphaCanvas(config_dir='./custom_config')
```

**êµ¬í˜„ ìš”êµ¬ì‚¬í•­:**

- `AlphaCanvas.__init__()` ë‚´ë¶€ì—ì„œ `ConfigLoader`ë¥¼ ìƒì„±í•˜ê³  `config/` ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  `.yaml` íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
- `ConfigLoader`ëŠ” `data.yaml`, `db.yaml` ë“±ì„ ê°ê° íŒŒì‹±í•˜ì—¬ ë‚´ë¶€ dictì— ì €ìž¥í•©ë‹ˆë‹¤.

### 3.2.2. ì½”ì–´ ë°ì´í„° ëª¨ë¸ êµ¬í˜„ (Core Data Model Implementation)

#### A. `AlphaCanvas.add_data()` êµ¬í˜„ (`facade.py`)

```python
def add_data(self, name: str, data: Union[Expression, xr.DataArray]) -> None:
    """ë°ì´í„° ë³€ìˆ˜ë¥¼ Datasetì— ì¶”ê°€ (Expression ë˜ëŠ” DataArray ì§€ì›)"""
    
    # Case 1: Expression í‰ê°€ (ì¼ë°˜ì ì¸ ê²½ë¡œ)
    if isinstance(data, Expression):
        self.rules[name] = data  # Expression ì €ìž¥ (ìž¬í‰ê°€ ê°€ëŠ¥í•˜ë„ë¡)
        result_array = self._evaluator.evaluate(data)  # Visitorë¡œ í‰ê°€
        self.db = self.db.assign({name: result_array})  # data_varsì— ì¶”ê°€
    
    # Case 2: DataArray ì§ì ‘ ì£¼ìž… (Open Toolkit: Inject)
    elif isinstance(data, xr.DataArray):
        # ì™¸ë¶€ì—ì„œ ìƒì„±í•œ ë°ì´í„° ì£¼ìž… (Visitor ê±´ë„ˆë›°ê¸°)
        self.db = self.db.assign({name: data})
    
    else:
        raise TypeError(f"data must be Expression or DataArray, got {type(data)}")
```

**í•µì‹¬ ì‚¬í•­:**

- `xarray.Dataset.assign()`ì„ ì‚¬ìš©í•˜ì—¬ Data Variableë¡œ ì¶”ê°€
- `Expression`ê³¼ `DataArray` ëª¨ë‘ ì§€ì› (ì˜¤ë²„ë¡œë”©)
- Open Toolkit ì² í•™: ì™¸ë¶€ ê³„ì‚° ê²°ê³¼ë¥¼ seamlessly inject

#### B. `rc.db` í”„ë¡œí¼í‹° (Open Toolkit: Eject)

```python
@property
def db(self) -> xr.Dataset:
    """ìˆœìˆ˜ xarray.Dataset ë°˜í™˜ (Jupyter ejectìš©)"""
    return self._dataset  # ë‚´ë¶€ Datasetì„ ê·¸ëŒ€ë¡œ ë…¸ì¶œ
```

**í•µì‹¬ ì‚¬í•­:**

- ëž˜í•‘ ì—†ì´ ìˆœìˆ˜ `xarray.Dataset` ë°˜í™˜
- ì‚¬ìš©ìžëŠ” `pure_ds = rc.db`ë¡œ êº¼ë‚´ì„œ scipy/statsmodels ì‚¬ìš© ê°€ëŠ¥

#### C. ìœ ë‹ˆë²„ìŠ¤ ë§ˆìŠ¤í‚¹ (Universe Masking) âœ… **IMPLEMENTED**

**ìš”êµ¬ì‚¬í•­**: ì´ˆê¸°í™” ì‹œ ìœ ë‹ˆë²„ìŠ¤ë¥¼ ì„¤ì •í•˜ê³ , ëª¨ë“  ë°ì´í„°ì™€ ì—°ì‚°ì— ìžë™ ì ìš©

```python
# AlphaCanvas ì´ˆê¸°í™” with universe
rc = AlphaCanvas(
    start_date='2024-01-01',
    end_date='2024-12-31',
    universe=price > 5.0  # Boolean DataArray
)

# ë˜ëŠ” Expressionìœ¼ë¡œ ì„¤ì •
rc = AlphaCanvas(
    start_date='2024-01-01',
    end_date='2024-12-31',
    universe=Field('univ500')  # Field Expression (ë¯¸ëž˜ í™•ìž¥)
)

# ìœ ë‹ˆë²„ìŠ¤ í™•ì¸ (read-only)
print(f"Universe coverage: {rc.universe.sum().values} positions")
```

**êµ¬í˜„ ì„¸ë¶€ì‚¬í•­**:

**1. AlphaCanvasì— universe íŒŒë¼ë¯¸í„° ì¶”ê°€**:
```python
class AlphaCanvas:
    def __init__(
        self,
        config_dir='config',
        start_date=None,
        end_date=None,
        time_index=None,
        asset_index=None,
        universe: Optional[Union[Expression, xr.DataArray]] = None  # NEW
    ):
        # ... ê¸°ì¡´ ì´ˆê¸°í™” ...
        
        # Universe mask ì´ˆê¸°í™” (ë¶ˆë³€)
        self._universe_mask: Optional[xr.DataArray] = None
        if universe is not None:
            self._set_initial_universe(universe)
    
    def _set_initial_universe(self, universe: Union[Expression, xr.DataArray]) -> None:
        """ìœ ë‹ˆë²„ìŠ¤ ë§ˆìŠ¤í¬ë¥¼ ì´ˆê¸°í™” ì‹œ í•œ ë²ˆë§Œ ì„¤ì • (ë¶ˆë³€)."""
        # Expression í‰ê°€ (e.g., Field('univ500'))
        if isinstance(universe, Expression):
            universe_data = self._evaluator.evaluate(universe)
        else:
            universe_data = universe
        
        # Shape ê²€ì¦
        expected_shape = (
            len(self._panel.db.coords['time']), 
            len(self._panel.db.coords['asset'])
        )
        if universe_data.shape != expected_shape:
            raise ValueError(
                f"Universe mask shape {universe_data.shape} doesn't match "
                f"data shape {expected_shape}"
            )
        
        # Dtype ê²€ì¦
        if universe_data.dtype != bool:
            raise TypeError(f"Universe must be boolean, got {universe_data.dtype}")
        
        # ë¶ˆë³€ ì €ìž¥
        self._universe_mask = universe_data
        
        # Evaluatorì— ì „íŒŒ (ìžë™ ì ìš© ìœ„í•´)
        self._evaluator._universe_mask = self._universe_mask
    
    @property
    def universe(self) -> Optional[xr.DataArray]:
        """ìœ ë‹ˆë²„ìŠ¤ ë§ˆìŠ¤í¬ ì¡°íšŒ (read-only)."""
        return self._universe_mask
```

**2. EvaluateVisitorì— ì´ì¤‘ ë§ˆìŠ¤í‚¹ êµ¬í˜„**:
```python
class EvaluateVisitor:
    def __init__(self, data_source: xr.Dataset, data_loader=None):
        self._data = data_source
        self._data_loader = data_loader
        self._universe_mask: Optional[xr.DataArray] = None  # AlphaCanvasê°€ ì„¤ì •
        self._cache: Dict[int, Tuple[str, xr.DataArray]] = {}
        self._step_counter = 0
    
    def visit_field(self, node) -> xr.DataArray:
        """Field ë…¸ë“œ ë°©ë¬¸ with INPUT MASKING."""
        # í•„ë“œ ë¡œë“œ (ìºì‹œ ë˜ëŠ” DataLoader)
        if node.name in self._data:
            result = self._data[node.name]
        else:
            if self._data_loader is None:
                raise RuntimeError(f"Field '{node.name}' not found")
            result = self._data_loader.load_field(node.name)
            self._data = self._data.assign({node.name: result})
        
        # INPUT MASKING: í•„ë“œ ê²€ìƒ‰ ì‹œ ìœ ë‹ˆë²„ìŠ¤ ì ìš©
        if self._universe_mask is not None:
            result = result.where(self._universe_mask, np.nan)
        
        self._cache_result(f"Field_{node.name}", result)
        return result
    
    def visit_operator(self, node) -> xr.DataArray:
        """ì—°ì‚°ìž ë°©ë¬¸ with OUTPUT MASKING."""
        # 1. ìˆœíšŒ: ìžì‹ í‰ê°€ (ì´ë¯¸ ë§ˆìŠ¤í‚¹ë¨)
        child_result = node.child.accept(self)
        
        # 2. ìœ„ìž„: ì—°ì‚°ìžì˜ compute() í˜¸ì¶œ
        result = node.compute(child_result)
        
        # 3. OUTPUT MASKING: ì—°ì‚° ê²°ê³¼ì— ìœ ë‹ˆë²„ìŠ¤ ì ìš©
        if self._universe_mask is not None:
            result = result.where(self._universe_mask, np.nan)
        
        # 4. ìºì‹±
        operator_name = node.__class__.__name__
        self._cache_result(operator_name, result)
        
        return result
```

**3. add_data()ì—ì„œ ì£¼ìž… ë°ì´í„° ë§ˆìŠ¤í‚¹**:
```python
def add_data(self, name: str, data: Union[Expression, xr.DataArray]) -> None:
    """ë°ì´í„° ì¶”ê°€ with ìœ ë‹ˆë²„ìŠ¤ ë§ˆìŠ¤í‚¹."""
    if isinstance(data, Expression):
        # Expression ê²½ë¡œ - Evaluatorê°€ ìžë™ ë§ˆìŠ¤í‚¹
        self.rules[name] = data
        result = self._evaluator.evaluate(data)
        self._panel.add_data(name, result)
        
        # Evaluator ìž¬ë™ê¸°í™” ì‹œ ìœ ë‹ˆë²„ìŠ¤ ë³´ì¡´
        self._evaluator = EvaluateVisitor(self._panel.db, self._data_loader)
        if self._universe_mask is not None:
            self._evaluator._universe_mask = self._universe_mask
    
    elif isinstance(data, xr.DataArray):
        # DataArray ì§ì ‘ ì£¼ìž… - ì—¬ê¸°ì„œ ë§ˆìŠ¤í‚¹
        if self._universe_mask is not None:
            data = data.where(self._universe_mask, float('nan'))
        
        self._panel.add_data(name, data)
        self._evaluator = EvaluateVisitor(self._panel.db, self._data_loader)
        if self._universe_mask is not None:
            self._evaluator._universe_mask = self._universe_mask
```

**í•µì‹¬ ì‚¬í•­**:
- **ë¶ˆë³€ì„±**: ìœ ë‹ˆë²„ìŠ¤ëŠ” ì´ˆê¸°í™” ì‹œ í•œ ë²ˆë§Œ ì„¤ì •, ë³€ê²½ ë¶ˆê°€
- **ì´ì¤‘ ë§ˆìŠ¤í‚¹**: Field ìž…ë ¥ + Operator ì¶œë ¥ ëª¨ë‘ ë§ˆìŠ¤í‚¹ (ì‹ ë¢° ì²´ì¸)
- **Open Toolkit**: ì£¼ìž…ëœ DataArrayë„ ìžë™ ë§ˆìŠ¤í‚¹
- **ì„±ëŠ¥**: 13.6% ì˜¤ë²„í—¤ë“œ (xarray lazy evaluationìœ¼ë¡œ ë¬´ì‹œ ê°€ëŠ¥)

---

#### D. `rc.data` Accessor êµ¬í˜„ (Selector Interface) âœ… **IMPLEMENTED**

**ì„¤ê³„ ì² í•™**: Expression ê¸°ë°˜ í•„ë“œ ì ‘ê·¼ìœ¼ë¡œ ì§€ì—° í‰ê°€ ë° ìœ ë‹ˆë²„ìŠ¤ ì•ˆì „ì„± ë³´ìž¥

```python
from alpha_canvas.core.expression import Field


class DataAccessor:
    """rc.data accessor that returns Field Expressions.
    
    This enables Expression-based data access:
        rc.data['field_name'] â†’ Field('field_name')
        rc.data['size'] == 'small' â†’ Equals(Field('size'), 'small')
    
    Field Expressions remain lazy until explicitly evaluated,
    ensuring universe masking through the Visitor pattern.
    """
    
    def __getitem__(self, field_name: str) -> Field:
        """Return Field Expression for the given field name.
        
        Args:
            field_name: Name of the field to access
            
        Returns:
            Field Expression wrapping the field name
            
        Raises:
            TypeError: If field_name is not a string
        """
        if not isinstance(field_name, str):
            raise TypeError(
                f"Field name must be string, got {type(field_name).__name__}"
            )
        
        return Field(field_name)
    
    def __getattr__(self, name: str):
        """Prevent attribute access - only item access allowed.
        
        This ensures a single, consistent access pattern.
        """
        raise AttributeError(
            f"DataAccessor does not support attribute access. "
            f"Use rc.data['{name}'] instead of rc.data.{name}"
        )
```

**AlphaCanvas í†µí•©**:

```python
class AlphaCanvas:
    def __init__(self, ...):
        # ... existing init ...
        self._data_accessor = DataAccessor()  # Create once, reuse
    
    @property
    def data(self) -> DataAccessor:
        """Access data fields as Field Expressions."""
        return self._data_accessor
```

**í•µì‹¬ ì‚¬í•­:**

- âœ… **Expression ë°˜í™˜**: `rc.data['size']` â†’ `Field('size')` (lazy)
- âœ… **Lazy í‰ê°€**: ëª…ì‹œì  `rc.evaluate()` í˜¸ì¶œ ì „ê¹Œì§€ í‰ê°€ ì•ˆ ë¨
- âœ… **ìœ ë‹ˆë²„ìŠ¤ ì•ˆì „**: ëª¨ë“  Expressionì€ Visitorë¥¼ í†µí•´ í‰ê°€ë˜ì–´ ìœ ë‹ˆë²„ìŠ¤ ë§ˆìŠ¤í‚¹ ë³´ìž¥
- âœ… **Composable**: `ts_mean(rc.data['returns'], 10)` ê°™ì€ ì²´ì´ë‹ ê°€ëŠ¥
- âœ… **Item access only**: `rc.data['field']`ë§Œ ì§€ì›, `rc.data.field`ëŠ” ì—ëŸ¬
- âœ… **í†µí•©**: Phase 7A Boolean Expressionê³¼ ì™„ë²½ í†µí•©

**ì‚¬ìš© íŒ¨í„´**:

```python
# âœ… Correct pattern (Expression-based)
mask = rc.data['size'] == 'small'  # Returns Equals Expression
result = rc.evaluate(mask)         # Evaluates with universe masking

# âœ… Complex pattern
mask = (rc.data['size'] == 'small') & (rc.data['momentum'] == 'high')
result = rc.evaluate(mask)

# âŒ Wrong pattern (not supported)
mask = rc.data.size == 'small'  # AttributeError
```

### 3.2.3. Interface A: Formula-based (Excel-like)

```python
from alpha_canvas.ops import ts_mean, rank, group_neutralize, Field

# 1. ê°„ë‹¨í•œ í—¬í¼ í•¨ìˆ˜ ìŠ¤íƒ€ì¼ (ì¦‰ì‹œ í‰ê°€)
returns_10d = rc.ts_mean('returns', 10)  
# rc.db['returns_10d']ì— DataArray ì €ìž¥

# 2. ë³µìž¡í•œ Expression ì •ì˜ (ì§€ì—° í‰ê°€)
alpha_expr = group_neutralize(
    rank(ts_mean(Field('returns'), 10)),
    group_by='subindustry'
)

# 3. Expressionì„ ë³€ìˆ˜ë¡œ ë“±ë¡
rc.add_data_var('alpha1', alpha_expr)

# 4. ë°ì´í„° ì ‘ê·¼ (evaluated data)
alpha1_data = rc.db['alpha1']  # xarray.DataArray (T, N)
```

**êµ¬í˜„ ìš”êµ¬ì‚¬í•­:**

- `Field('returns')`: `ConfigLoader`ì—ì„œ `config/data.yaml`ì˜ `returns` ì •ì˜ë¥¼ ì°¸ì¡°í•˜ëŠ” Leaf Expression
- `ts_mean()`, `rank()` ë“±: Composite Expression ë…¸ë“œë¥¼ ìƒì„±
- `rc.add_data_var()`: Expressionì„ `rc.rules`ì— ë“±ë¡í•˜ê³ , `EvaluateVisitor`ë¡œ í‰ê°€í•˜ì—¬ `rc.db`ì— ì €ìž¥

### 3.2.3. Interface B: Selector-based (NumPy-like)

```python
# 1. ì‹œê·¸ë„ ìº”ë²„ìŠ¤ ì´ˆê¸°í™”
rc.init_signal_canvas('my_alpha')  
# rc.db['my_alpha']ì— (T, N) ì˜í–‰ë ¬ ìƒì„±

# 2. ë°ì´í„° ë“±ë¡
rc.add_data('mcap', Field('market_cap'))
rc.add_data('ret', Field('returns'))
rc.add_data('vol', Field('volume'))

# 3. ë¶„ë¥˜ ë°ì´í„° ì •ì˜ - ë ˆì´ë¸” ê¸°ë°˜ ë²„í‚·
rc.add_data('size', cs_quantile(rc.data['mcap'], bins=3, labels=['small', 'mid', 'big']))
rc.add_data('momentum', cs_quantile(rc.data['ret'], bins=2, labels=['low', 'high']))
rc.add_data('surge', ts_any(rc.data['ret'] > 0.3, window=252))  # Boolean

# 4. ë¹„êµ ì—°ì‚°ìœ¼ë¡œ Boolean ë§ˆìŠ¤í¬ ìƒì„±
mask_long = (rc.data['size'] == 'small') & (rc.data['momentum'] == 'high') & (rc.data['surge'] == True)
mask_short = (rc.data['size'] == 'big') & (rc.data['momentum'] == 'low')

# 5. NumPy-style í• ë‹¹
rc['my_alpha'][mask_long] = 1.0
rc['my_alpha'][mask_short] = -1.0

# ë˜ëŠ” í˜„ìž¬ í™œì„± ìº”ë²„ìŠ¤ì— ì§ì ‘ í• ë‹¹
rc[mask_long] = 1.0

# 6. ìµœì¢… ì‹œê·¸ë„ ì ‘ê·¼ (evaluated data)
my_alpha = rc.db['my_alpha']  # xarray.DataArray (T, N)
```

**êµ¬í˜„ ìš”êµ¬ì‚¬í•­:**

- `rc.add_data('size', expr)`: Expressionì„ í‰ê°€í•˜ê³  `rc.db.assign({'size': result})`ë¡œ data_varsì— ì¶”ê°€
- `rc.data['size'] == 'small'`:
  1. `rc.data['size']` â†’ `Field('size')` Expression ë°˜í™˜
  2. `Field('size') == 'small'` â†’ `Equals(Field('size'), 'small')` Expression ë°˜í™˜
  3. Expressionì€ lazyí•˜ê²Œ ìœ ì§€, `rc.evaluate(expr)`ë¡œ í‰ê°€
- `rc[mask] = value`: `xr.where(mask, value, rc.db[current_canvas])`ë¡œ í• ë‹¹ (ë¯¸êµ¬í˜„)

### 3.2.4. Interface C: Selective Traceability (Integer-Based Steps)

```python
# ë³µìž¡í•œ Expression ì •ì˜
complex_alpha = group_neutralize(
    rank(ts_mean(Field('returns'), 5)),
    group_by='subindustry'
)

rc.add_data_var('complex_alpha', complex_alpha)

# Expression íŠ¸ë¦¬ êµ¬ì¡° (depth-first ìˆœì„œ):
# step 0: Field('returns')
# step 1: ts_mean(Field('returns'), 5)
# step 2: rank(...)
# step 3: group_neutralize(...)

# 1. íŠ¹ì • ë‹¨ê³„ë§Œ ì¶”ì 
pnl_step1 = rc.trace_pnl('complex_alpha', step=1)
# {'sharpe': 0.7, 'total_pnl': 150, 'cumulative_returns': [...]}

# 2. ëª¨ë“  ë‹¨ê³„ ì¶”ì 
pnl_all = rc.trace_pnl('complex_alpha')  # step=None (default)
# {
#   0: {'step_name': 'Field_returns', 'sharpe': 0.5, ...},
#   1: {'step_name': 'ts_mean', 'sharpe': 0.7, ...},
#   2: {'step_name': 'rank', 'sharpe': 0.6, ...},
#   3: {'step_name': 'group_neutralize', 'sharpe': 0.8, ...}
# }

# 3. ì¤‘ê°„ ë°ì´í„° ì§ì ‘ ì ‘ê·¼
intermediate = rc.get_intermediate('complex_alpha', step=1)
# xarray.DataArray (T, N) - ts_mean ì ìš© í›„ ë°ì´í„°

# 4. ë³µìž¡í•œ Expression ì˜ˆì‹œ (ë³‘ë ¬ ì—°ì‚°)
combo_alpha = ts_mean(Field('returns'), 3) + rank(Field('market_cap'))
rc.add_data_var('combo', combo_alpha)

# step 0: Field('returns')
# step 1: ts_mean(Field('returns'), 3)
# step 2: Field('market_cap')
# step 3: rank(Field('market_cap'))
# step 4: add(step1, step3)

pnl_step4 = rc.trace_pnl('combo', step=4)  # ìµœì¢… ê²°ê³¼
```

**êµ¬í˜„ ìš”êµ¬ì‚¬í•­:**

- `EvaluateVisitor.cache` êµ¬ì¡°: `dict[str, dict[int, tuple[str, xr.DataArray]]]`
  - ì™¸ë¶€ í‚¤: ë³€ìˆ˜ëª… (e.g., `'complex_alpha'`)
  - ë‚´ë¶€ í‚¤: ì •ìˆ˜ step ì¸ë±ìŠ¤ (0ë¶€í„° ì‹œìž‘)
  - ê°’: `(ë…¸ë“œ_ì´ë¦„, DataArray)` íŠœí”Œ (ë””ë²„ê¹…ìš© ë©”íƒ€ë°ì´í„° í¬í•¨)
- `EvaluateVisitor._step_counter`: í˜„ìž¬ step ì¸ë±ìŠ¤ë¥¼ ì¶”ì í•˜ëŠ” ë‚´ë¶€ ì¹´ìš´í„°
- `EvaluateVisitor`ëŠ” Expression íŠ¸ë¦¬ë¥¼ **ê¹Šì´ ìš°ì„  íƒìƒ‰(depth-first)** ìœ¼ë¡œ ìˆœíšŒí•˜ë©° ê° ë…¸ë“œì˜ ë°˜í™˜ê°’ì„ ìºì‹œì— ì €ìž¥
- `rc.trace_pnl(var, step=None)`:
  - `step=None`: ëª¨ë“  ë‹¨ê³„ì˜ ìºì‹œ ë°ì´í„°ë¥¼ `PnLTracer`ì— ì „ë‹¬
  - `step=1`: í•´ë‹¹ ë‹¨ê³„ë§Œ ì „ë‹¬
- `rc.get_intermediate(var, step)`: `rc._evaluator.cache[var][step][1]` ë°˜í™˜ (DataArray ë¶€ë¶„)

### 3.2.5. í•µì‹¬ í™œìš© íŒ¨í„´: íŒ©í„° ìˆ˜ìµë¥  ê³„ì‚°

#### A. ë…ë¦½ ì´ì¤‘ ì •ë ¬ (Independent Double Sort) - Fama-French SMB

```python
from alpha_canvas.ops import cs_quantile, Field

# 1. ë°ì´í„° ë“±ë¡
rc.add_data('mcap', Field('market_cap'))
rc.add_data('btm', Field('book_to_market'))

# 2. ë…ë¦½ ì •ë ¬: ì „ì²´ ìœ ë‹ˆë²„ìŠ¤ì—ì„œ ê°ê° quantile ê³„ì‚°
rc.add_axis('size', cs_quantile(rc.data.mcap, bins=2, labels=['small', 'big']))
rc.add_axis('value', cs_quantile(rc.data.btm, bins=3, labels=['low', 'mid', 'high']))

# 3. SMB í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„±
rc.init_signal_canvas('smb')
rc[rc.axis.size['small']] = 1.0   # Long all small stocks
rc[rc.axis.size['big']] = -1.0    # Short all big stocks

# 4. íŒ©í„° ìˆ˜ìµë¥  ì¶”ì 
smb_returns = rc.trace_pnl('smb')
print(f"SMB Sharpe: {smb_returns['sharpe']:.2f}")
```

#### B. ì¢…ì† ì´ì¤‘ ì •ë ¬ (Dependent Double Sort) - Fama-French HML

```python
# 1. ì²« ë²ˆì§¸ ì •ë ¬: Size
rc.add_axis('size', cs_quantile(rc.data.mcap, bins=2, labels=['small', 'big']))

# 2. ì¢…ì† ì •ë ¬: ê° Size ê·¸ë£¹ ë‚´ì—ì„œ Value quantile ê³„ì‚°
rc.add_axis('value', cs_quantile(rc.data.btm, bins=3, labels=['low', 'mid', 'high'],
                                   group_by='size'))
# group_by='size'ëŠ” rc.rules['size']ë¥¼ ì°¸ì¡°í•˜ì—¬
# 'small' ê·¸ë£¹ê³¼ 'big' ê·¸ë£¹ ë‚´ì—ì„œ ê°ê° ë…ë¦½ì ìœ¼ë¡œ quantileì„ ê³„ì‚°

# 3. HML í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„± (ê° Size ê·¸ë£¹ ë‚´ì—ì„œ High-Low)
rc.init_signal_canvas('hml')
rc[rc.axis.value['high']] = 1.0   # Long high B/M (value) in both size groups
rc[rc.axis.value['low']] = -1.0   # Short low B/M (growth) in both size groups

# 4. íŒ©í„° ìˆ˜ìµë¥  ì¶”ì 
hml_returns = rc.trace_pnl('hml')
print(f"HML Sharpe: {hml_returns['sharpe']:.2f}")
```

#### C. ë¡œìš°ë ˆë²¨ ë§ˆìŠ¤í¬ í™œìš© (Advanced Custom Logic)

```python
# ìœ ë™ì„± í•„í„°ë§ëœ ìœ ë‹ˆë²„ìŠ¤ì—ì„œ ëª¨ë©˜í…€ íŒ©í„° êµ¬ì„±
rc.add_data('volume', Field('volume'))
rc.add_data('returns', Field('returns'))

# 1. ê³ ìœ ë™ì„± ë§ˆìŠ¤í¬ ìƒì„±
high_liquidity = rc.data.volume > rc.data.volume.quantile(0.5)

# 2. ë§ˆìŠ¤í¬ ì ìš©ëœ quantile ê³„ì‚°
rc.add_axis('momentum', cs_quantile(rc.data.returns, bins=5, labels=['q1','q2','q3','q4','q5'],
                                      mask=high_liquidity))
# mask=Falseì¸ ì¢…ëª©ì€ NaNìœ¼ë¡œ ì²˜ë¦¬ë¨

# 3. ë¡±-ìˆ í¬íŠ¸í´ë¦¬ì˜¤
rc.init_signal_canvas('momentum_factor')
rc[rc.axis.momentum['q5']] = 1.0
rc[rc.axis.momentum['q1']] = -1.0

mom_returns = rc.trace_pnl('momentum_factor')
```

#### D. ë‹¤ì°¨ì› íŒ©í„° ì¡°í•© (Multi-Factor Strategy)

```python
# ë…ë¦½ ì •ë ¬ë¡œ 3ê°œ íŒ©í„° ì¶• ìƒì„±
rc.add_axis('size', cs_quantile(rc.data.mcap, bins=3, labels=['small','mid','big']))
rc.add_axis('momentum', cs_quantile(rc.data.mom, bins=5, labels=['q1','q2','q3','q4','q5']))
rc.add_axis('quality', cs_quantile(rc.data.roe, bins=3, labels=['low','mid','high']))

# ë³µìž¡í•œ ë‹¤ì°¨ì› ì„ íƒ
rc.init_signal_canvas('multi_factor')

# Small & High Momentum & High Quality
long_mask = (rc.axis.size['small'] & 
             rc.axis.momentum['q5'] & 
             rc.axis.quality['high'])

# Big & Low Momentum & Low Quality
short_mask = (rc.axis.size['big'] & 
              rc.axis.momentum['q1'] & 
              rc.axis.quality['low'])

rc[long_mask] = 1.0
rc[short_mask] = -1.0

multi_returns = rc.trace_pnl('multi_factor')
```

**ì„¤ê³„ ì˜ë„:**

- ì´ íŒ¨í„´ë“¤ì´ alpha-canvasì˜ **í•µì‹¬ í™œìš© ì‚¬ë¡€**ìž…ë‹ˆë‹¤.
- **Fama-French ìž¬í˜„**: `group_by`ë¡œ ì¢…ì† ì •ë ¬ì„ ê°„ê²°í•˜ê²Œ í‘œí˜„
- **ìœ ì—°ì„±**: `mask`ë¡œ ì»¤ìŠ¤í…€ ìœ ë‹ˆë²„ìŠ¤ í•„í„°ë§ ê°€ëŠ¥
- ë“€ì–¼ ì¸í„°íŽ˜ì´ìŠ¤(Formula + Selector)ì˜ ì¡°í•©ìœ¼ë¡œ ë³µìž¡í•œ ë‹¤ì°¨ì› íŒ©í„° í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ ê°„ê²°í•˜ê²Œ í‘œí˜„í•©ë‹ˆë‹¤.
- ë ˆì´ë¸” ê¸°ë°˜ ì„ íƒ(`'small'`, `'q5'`, `'high'`)ìœ¼ë¡œ ê°€ë…ì„±ê³¼ ìœ ì§€ë³´ìˆ˜ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤.

## 3.3. ì—°ì‚°ìž êµ¬í˜„ íŒ¨í„´ (Operator Implementation Pattern)

### 3.3.1. ì±…ìž„ ë¶„ë¦¬ ì›ì¹™

**í•µì‹¬ ì›ì¹™:** ì—°ì‚°ìžëŠ” ìžì‹ ì˜ ê³„ì‚° ë¡œì§ì„ ì†Œìœ í•˜ê³ , VisitorëŠ” ìˆœíšŒ ë° ìºì‹±ë§Œ ë‹´ë‹¹í•©ë‹ˆë‹¤.

**ìž˜ëª»ëœ íŒ¨í„´ (Anti-Pattern):**

```python
# âŒ BAD: Visitorê°€ ê³„ì‚° ë¡œì§ì„ í¬í•¨
class EvaluateVisitor:
    def visit_ts_mean(self, node):
        child_result = node.child.accept(self)
        # Visitor ì•ˆì— rolling ê³„ì‚° ë¡œì§ì´ ë“¤ì–´ê° (ìž˜ëª»ë¨!)
        result = child_result.rolling(time=node.window, min_periods=node.window).mean()
        self._cache_result("TsMean", result)
        return result
```

**ì˜¬ë°”ë¥¸ íŒ¨í„´ (Correct Pattern):**

```python
# âœ… GOOD: ì—°ì‚°ìžê°€ ê³„ì‚° ë¡œì§ì„ ì†Œìœ 
@dataclass
class TsMean(Expression):
    child: Expression
    window: int
    
    def accept(self, visitor):
        """Visitor ì¸í„°íŽ˜ì´ìŠ¤: ìˆœíšŒë¥¼ ìœ„í•œ ì§„ìž…ì """
        return visitor.visit_ts_mean(self)
    
    def compute(self, child_result: xr.DataArray) -> xr.DataArray:
        """í•µì‹¬ ê³„ì‚° ë¡œì§: ì—°ì‚°ìžê°€ ì†Œìœ """
        return child_result.rolling(
            time=self.window,
            min_periods=self.window
        ).mean()

# âœ… GOOD: VisitorëŠ” ìˆœíšŒ ë° ìºì‹±ë§Œ ë‹´ë‹¹
class EvaluateVisitor:
    def visit_ts_mean(self, node: TsMean) -> xr.DataArray:
        """íŠ¸ë¦¬ ìˆœíšŒ ë° ìƒíƒœ ìˆ˜ì§‘"""
        # 1. ìˆœíšŒ: ìžì‹ ë…¸ë“œ í‰ê°€
        child_result = node.child.accept(self)
        
        # 2. ê³„ì‚° ìœ„ìž„: ì—°ì‚°ìžì—ê²Œ ë§¡ê¹€
        result = node.compute(child_result)
        
        # 3. ìƒíƒœ ìˆ˜ì§‘: ê²°ê³¼ ìºì‹±
        self._cache_result("TsMean", result)
        
        return result
```

### 3.3.2. ì—°ì‚°ìž êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

ëª¨ë“  ì—°ì‚°ìžëŠ” ë‹¤ìŒ êµ¬ì¡°ë¥¼ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤:

```python
@dataclass
class OperatorName(Expression):
    """ì—°ì‚°ìž ì„¤ëª….
    
    Args:
        child: ìžì‹ Expression (í•„ìš”ì‹œ)
        param1: ì—°ì‚°ìž íŒŒë¼ë¯¸í„° 1
        param2: ì—°ì‚°ìž íŒŒë¼ë¯¸í„° 2
    
    Returns:
        ì—°ì‚° ê²°ê³¼ DataArray
    """
    child: Expression  # ìžì‹ ë…¸ë“œ (ìžˆëŠ” ê²½ìš°)
    param1: type1      # ì—°ì‚°ìž íŒŒë¼ë¯¸í„°ë“¤
    param2: type2
    
    def accept(self, visitor) -> xr.DataArray:
        """Visitor ì¸í„°íŽ˜ì´ìŠ¤."""
        return visitor.visit_operator_name(self)
    
    def compute(self, *inputs: xr.DataArray) -> xr.DataArray:
        """í•µì‹¬ ê³„ì‚° ë¡œì§.
        
        Args:
            *inputs: ìžì‹ ë…¸ë“œë“¤ì˜ í‰ê°€ ê²°ê³¼
        
        Returns:
            ì´ ì—°ì‚°ì˜ ê²°ê³¼ DataArray
        
        Note:
            ì´ ë©”ì„œë“œëŠ” ìˆœìˆ˜ í•¨ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤ (ë¶€ìž‘ìš© ì—†ìŒ).
            Visitor ì°¸ì¡° ì—†ì´ ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤.
        """
        # ì‹¤ì œ ê³„ì‚° ë¡œì§
        result = ...  # xarray/numpy ì—°ì‚°
        return result
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**

- [ ] `accept()` ë©”ì„œë“œ: Visitor ì¸í„°íŽ˜ì´ìŠ¤ ì œê³µ
- [ ] `compute()` ë©”ì„œë“œ: í•µì‹¬ ê³„ì‚° ë¡œì§ ìº¡ìŠí™”
- [ ] `compute()`ëŠ” ìˆœìˆ˜ í•¨ìˆ˜ (ë¶€ìž‘ìš© ì—†ìŒ)
- [ ] `compute()`ëŠ” Visitor ë…ë¦½ì  (ì§ì ‘ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥)
- [ ] Docstringìœ¼ë¡œ Args/Returns ëª…í™•ížˆ ë¬¸ì„œí™”

### 3.3.3. Visitor êµ¬í˜„ íŒ¨í„´

ëª¨ë“  `visit_*()` ë©”ì„œë“œëŠ” ë™ì¼í•œ 3ë‹¨ê³„ íŒ¨í„´ì„ ë”°ë¦…ë‹ˆë‹¤:

```python
def visit_operator_name(self, node: OperatorName) -> xr.DataArray:
    """ì—°ì‚°ìž ë…¸ë“œ ë°©ë¬¸: ìˆœíšŒ ë° ìºì‹±.
    
    Args:
        node: ì—°ì‚°ìž Expression ë…¸ë“œ
    
    Returns:
        ì—°ì‚° ê²°ê³¼ DataArray
    """
    # 1ï¸âƒ£ ìˆœíšŒ(Traversal): ìžì‹ ë…¸ë“œë“¤ í‰ê°€
    child_result_1 = node.child1.accept(self)  # ê¹Šì´ ìš°ì„ 
    child_result_2 = node.child2.accept(self)  # (ìžˆëŠ” ê²½ìš°)
    
    # 2ï¸âƒ£ ê³„ì‚° ìœ„ìž„(Delegation): ì—°ì‚°ìžì—ê²Œ ë§¡ê¹€
    result = node.compute(child_result_1, child_result_2)
    
    # 3ï¸âƒ£ ìƒíƒœ ìˆ˜ì§‘(State Collection): ê²°ê³¼ ìºì‹±
    self._cache_result("OperatorName", result)
    
    return result
```

**Visitorì˜ ì—­í• :**

- âœ… **íŠ¸ë¦¬ ìˆœíšŒ:** ê¹Šì´ ìš°ì„ ìœ¼ë¡œ ìžì‹ ë…¸ë“œ ë°©ë¬¸
- âœ… **ê³„ì‚° ìœ„ìž„:** `node.compute()`ë¡œ ê³„ì‚° ë§¡ê¹€
- âœ… **ìƒíƒœ ìˆ˜ì§‘:** ì¤‘ê°„ ê²°ê³¼ë¥¼ ì •ìˆ˜ ìŠ¤í…ìœ¼ë¡œ ìºì‹±
- âŒ **ê³„ì‚° ë¡œì§ í¬í•¨ ê¸ˆì§€:** rolling, rank, quantile ë“±ì˜ ë¡œì§ì€ ì—°ì‚°ìžì— ì†í•¨

### 3.3.4. í…ŒìŠ¤íŠ¸ ì „ëžµ

**1. ì—°ì‚°ìž ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (Operator Unit Tests):**

```python
def test_ts_mean_compute_directly():
    """TsMean.compute() ë©”ì„œë“œë¥¼ ì§ì ‘ í…ŒìŠ¤íŠ¸ (Visitor ì—†ì´)."""
    # ìž…ë ¥ ë°ì´í„° ì¤€ë¹„
    data = xr.DataArray(
        [[1, 2], [3, 4], [5, 6]],
        dims=['time', 'asset']
    )
    
    # ì—°ì‚°ìž ìƒì„±
    operator = TsMean(child=Field('dummy'), window=2)
    
    # compute() ì§ì ‘ í˜¸ì¶œ (Visitor ìš°íšŒ)
    result = operator.compute(data)
    
    # ê²€ì¦
    assert np.isnan(result.values[0, 0])  # ì²« í–‰ NaN
    assert result.values[1, 0] == 2.0     # mean([1, 3])
```

**2. í†µí•© í…ŒìŠ¤íŠ¸ (Integration Tests):**

```python
def test_ts_mean_with_visitor():
    """TsMeanì´ Visitorì™€ í†µí•©ë˜ì–´ ìž‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸."""
    ds = xr.Dataset({'returns': data})
    visitor = EvaluateVisitor(ds)
    
    expr = TsMean(child=Field('returns'), window=3)
    result = visitor.evaluate(expr)
    
    # ìºì‹± ê²€ì¦
    assert len(visitor._cache) == 2  # Field + TsMean
```

### 3.3.5. ì´ì  ìš”ì•½

| ì¸¡ë©´ | ìž˜ëª»ëœ íŒ¨í„´ | ì˜¬ë°”ë¥¸ íŒ¨í„´ |
|------|-------------|-------------|
| **ì±…ìž„** | Visitorê°€ ëª¨ë“  ê³„ì‚° ë‹´ë‹¹ | ì—°ì‚°ìžê°€ ìžì‹ ì˜ ê³„ì‚° ì†Œìœ  |
| **í…ŒìŠ¤íŠ¸** | Visitorë¥¼ í†µí•´ì„œë§Œ í…ŒìŠ¤íŠ¸ | `compute()` ì§ì ‘ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥ |
| **ìœ ì§€ë³´ìˆ˜** | Visitorê°€ ë¹„ëŒ€í•´ì§ | ê° ì—°ì‚°ìž ë…ë¦½ì  |
| **í™•ìž¥ì„±** | ìƒˆ ì—°ì‚°ìžë§ˆë‹¤ Visitor ìˆ˜ì • | Visitor ìˆ˜ì • ìµœì†Œí™” |
| **ë‹¨ì¼ ì±…ìž„** | Visitorê°€ ë‹¤ì¤‘ ì±…ìž„ | ê° í´ëž˜ìŠ¤ ë‹¨ì¼ ì±…ìž„ |

---

## 3.4. Cross-Sectional Quantile ì—°ì‚°ìž êµ¬í˜„ âœ… **IMPLEMENTED**

### 3.4.1. `CsQuantile` Expression í´ëž˜ìŠ¤ (ì‹¤ì œ êµ¬í˜„)

```python
from dataclasses import dataclass
from typing import List, Optional
import numpy as np
import pandas as pd
import xarray as xr

@dataclass(eq=False)  # eq=False to preserve Expression comparison operators
class CsQuantile(Expression):
    """Cross-sectional quantile bucketing - returns categorical labels.
    
    Preserves input (T, N) shape. Each timestep is independently bucketed.
    Supports both independent sort (whole universe) and dependent sort
    (within groups via group_by parameter).
    """
    child: Expression  # ë²„í‚·í™”í•  ë°ì´í„° (e.g., Field('market_cap'))
    bins: int  # ë²„í‚· ê°œìˆ˜
    labels: List[str]  # ë ˆì´ë¸” ë¦¬ìŠ¤íŠ¸ (ê¸¸ì´ = bins)
    group_by: Optional[str] = None  # ì¢…ì† ì •ë ¬ìš©: field ì´ë¦„ (string)
    
    def __post_init__(self):
        """Validate parameters."""
        if len(self.labels) != self.bins:
            raise ValueError(
                f"labels length ({len(self.labels)}) must equal bins ({self.bins})"
            )
    
    def accept(self, visitor):
        """Visitor ì¸í„°íŽ˜ì´ìŠ¤."""
        return visitor.visit_operator(self)
    
    def compute(
        self, 
        child_result: xr.DataArray, 
        group_labels: Optional[xr.DataArray] = None
    ) -> xr.DataArray:
        """Apply quantile bucketing - í•µì‹¬ ê³„ì‚° ë¡œì§."""
        if group_labels is None:
            return self._quantile_independent(child_result)
        else:
            return self._quantile_grouped(child_result, group_labels)
```

### 3.4.2. ë…ë¦½ ì •ë ¬ (Independent Sort) êµ¬í˜„

**í•µì‹¬ íŒ¨í„´:** `xarray.groupby('time').map()` + `pd.qcut` + **flatten-reshape**

```python
def _quantile_independent(self, data: xr.DataArray) -> xr.DataArray:
    """Independent sort - qcut at each timestep across all assets.
    
    í•µì‹¬: pd.qcutì€ 1D ìž…ë ¥ì´ í•„ìš”í•˜ë¯€ë¡œ flatten â†’ qcut â†’ reshape íŒ¨í„´ ì‚¬ìš©
    """
    def qcut_at_timestep(data_slice):
        """Apply pd.qcut to a single timestep's cross-section."""
        try:
            # CRITICAL: Flatten to 1D for pd.qcut
            values_1d = data_slice.values.flatten()
            result = pd.qcut(
                values_1d, 
                q=self.bins, 
                labels=self.labels, 
                duplicates='drop'  # Handle edge cases gracefully
            )
            # CRITICAL: Reshape back to original shape
            result_array = np.array(result).reshape(data_slice.shape)
            return xr.DataArray(
                result_array, 
                dims=data_slice.dims, 
                coords=data_slice.coords
            )
        except Exception:
            # Edge case: all same values, all NaN, etc.
            return xr.DataArray(
                np.full_like(data_slice.values, np.nan, dtype=object),
                dims=data_slice.dims, 
                coords=data_slice.coords
            )
    
    # xarray.groupby('time').map() automatically concatenates back to (T, N)
    result = data.groupby('time').map(qcut_at_timestep)
    return result
```

### 3.4.3. ì¢…ì† ì •ë ¬ (Dependent Sort) êµ¬í˜„

**í•µì‹¬ íŒ¨í„´:** ì¤‘ì²©ëœ groupby (groups â†’ time â†’ qcut)

```python
def _quantile_grouped(
    self, 
    data: xr.DataArray, 
    groups: xr.DataArray
) -> xr.DataArray:
    """Dependent sort - qcut within each group at each timestep.
    
    Nested groupby pattern:
    1. Group by categorical labels (e.g., 'small', 'big')
    2. Within each group, apply independent sort (group by time â†’ qcut)
    3. xarray automatically concatenates results back to (T, N) shape
    """
    def apply_qcut_within_group(group_data: xr.DataArray) -> xr.DataArray:
        """Apply qcut at each timestep within this group."""
        return self._quantile_independent(group_data)
    
    # Nested groupby: groups â†’ time â†’ qcut
    # xarray automatically concatenates results back
    result = data.groupby(groups).map(apply_qcut_within_group)
    return result
```

### 3.4.4. Visitor í†µí•© (Special Case Handling)

**CsQuantileì€ `visit_operator()`ì—ì„œ íŠ¹ë³„ ì²˜ë¦¬ í•„ìš” (group_by ì¡°íšŒ)**:

```python
# In EvaluateVisitor.visit_operator()
from alpha_canvas.ops.classification import CsQuantile

# Special handling for CsQuantile (needs group_by lookup)
if isinstance(node, CsQuantile):
    # 1. Evaluate child
    child_result = node.child.accept(self)
    
    # 2. Look up group_by field if specified
    group_labels = None
    if node.group_by is not None:
        if node.group_by not in self._data:
            raise ValueError(
                f"group_by field '{node.group_by}' not found in dataset"
            )
        group_labels = self._data[node.group_by]
    
    # 3. Delegate to compute()
    result = node.compute(child_result, group_labels)
    
    # 4. Apply universe masking (automatic)
    if self._universe_mask is not None:
        result = result.where(self._universe_mask, np.nan)
    
    # 5. Cache
    self._cache_result("CsQuantile", result)
    return result
```

### 3.4.5. í•µì‹¬ êµ¬í˜„ êµí›ˆ (ì‹¤í—˜ì—ì„œ ë°œê²¬)

**1. Flatten-Reshape íŒ¨í„´ í•„ìˆ˜:**
- `pd.qcut`ì€ 1D ë°°ì—´ë§Œ ë°›ìŒ
- `data_slice.values.flatten()` â†’ qcut â†’ `reshape(data_slice.shape)`
- ì´ íŒ¨í„´ ì—†ì´ëŠ” shape ë³´ì¡´ ë¶ˆê°€ëŠ¥

**2. xarray.groupby().map() vs .apply():**
- `.map()`ì´ xarray â†’ xarray ë³€í™˜ì— ë” ê¹”ë”
- ìžë™ concatenationìœ¼ë¡œ shape ë³´ì¡´
- `.apply()`ë„ ìž‘ë™í•˜ì§€ë§Œ pandas ë°˜í™˜ ì‹œ ì‚¬ìš©

**3. duplicates='drop' í•„ìˆ˜:**
- ëª¨ë“  ê°’ì´ ë™ì¼í•œ edge case ì²˜ë¦¬
- ëª¨ë“  NaNì¸ ê²½ìš° graceful degradation
- ì—ëŸ¬ ë°œìƒ ëŒ€ì‹  NaN ë°˜í™˜

**4. ì¢…ì† ì •ë ¬ ì„±ëŠ¥:**
- ë…ë¦½ ì •ë ¬: ~27ms for (10, 6) data
- ì¢…ì† ì •ë ¬: ~117ms for (10, 6) data (4.26x overhead)
- **í—ˆìš© ê°€ëŠ¥:** íŒ©í„° ì—°êµ¬ëŠ” ë°°ì¹˜ ì²˜ë¦¬ (ì‹¤ì‹œê°„ ì•„ë‹˜)

**5. ê²€ì¦ ë°©ë²•:**
- ë…ë¦½ vs ì¢…ì† ì •ë ¬ì˜ cutoffê°€ **ë‹¬ë¼ì•¼ í•¨**
- ì‹¤í—˜ì—ì„œ 17%ì˜ positionsê°€ ë‹¤ë¥¸ label ë°›ìŒ
- Fama-French ë…¼ë¬¸ methodologyì™€ ì¼ì¹˜

### 3.4.6. ì‚¬ìš© ì˜ˆì‹œ (ì‹¤ì œ ì½”ë“œ)

```python
from alpha_canvas.ops.classification import CsQuantile
from alpha_canvas.core.expression import Field

# ë…ë¦½ ì •ë ¬: ì „ì²´ ìœ ë‹ˆë²„ìŠ¤ì—ì„œ quantile
size_expr = CsQuantile(
    child=Field('market_cap'),
    bins=2,
    labels=['small', 'big']
)

# ì¢…ì† ì •ë ¬: size ê·¸ë£¹ ë‚´ì—ì„œ value quantile (Fama-French)
value_expr = CsQuantile(
    child=Field('book_to_market'),
    bins=3,
    labels=['low', 'mid', 'high'],
    group_by='size'  # 'size' fieldë¥¼ ë¨¼ì € ì¡°íšŒ â†’ ê° ê·¸ë£¹ë³„ quantile
)

# ì‚¬ìš©
rc.add_data('size', size_expr)  # ë¨¼ì € size ìƒì„±
rc.add_data('value', value_expr)  # size ê·¸ë£¹ ë‚´ì—ì„œ value ê³„ì‚°

# Boolean Expression í†µí•©
small_value = (rc.data['size'] == 'small') & (rc.data['value'] == 'high')
```

## 3.4. Property Accessor êµ¬í˜„ âœ… **IMPLEMENTED**

```python
from alpha_canvas.core.expression import Field


class DataAccessor:
    """Returns Field Expressions for lazy evaluation."""
    
    def __getitem__(self, field_name: str) -> Field:
        """Return Field Expression (not raw data!)"""
        if not isinstance(field_name, str):
            raise TypeError(
                f"Field name must be string, got {type(field_name).__name__}"
            )
        return Field(field_name)
    
    def __getattr__(self, name: str):
        """Prevent attribute access - item access only."""
        raise AttributeError(
            f"DataAccessor does not support attribute access. "
            f"Use rc.data['{name}'] instead of rc.data.{name}"
        )


# AlphaCanvas í†µí•©
class AlphaCanvas:
    def __init__(self, ...):
        self._data_accessor = DataAccessor()
    
    @property
    def data(self) -> DataAccessor:
        """Access data fields as Field Expressions."""
        return self._data_accessor
```

**ì‚¬ìš© ì˜ˆì‹œ**:

```python
# Basic field access
field = rc.data['size']  # Returns Field('size')

# Comparison creates Expression
mask = rc.data['size'] == 'small'  # Returns Equals Expression

# Evaluate
result = rc.evaluate(mask)  # Boolean DataArray with universe masking
```

---

## 3.4.2. Signal Assignment (Lazy Evaluation) âœ… **IMPLEMENTED**

### ê°œìš”

**Signal Assignment**ëŠ” Expression ê°ì²´ì— ê°’ì„ í• ë‹¹í•˜ì—¬ ì‹œê·¸ë„ì„ êµ¬ì„±í•˜ëŠ” ê¸°ëŠ¥ìž…ë‹ˆë‹¤. Fama-French íŒ©í„°ì™€ ê°™ì€ ë³µìž¡í•œ ì‹œê·¸ë„ì„ ì§ê´€ì ì¸ ë¬¸ë²•ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.

**í•µì‹¬ ì„¤ê³„ ì›ì¹™**:
- **Lazy Evaluation**: í• ë‹¹ì€ ì €ìž¥ë§Œ í•˜ê³  ì¦‰ì‹œ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ
- **Implicit Canvas**: ë³„ë„ì˜ ìº”ë²„ìŠ¤ ìƒì„± ì—†ì´ Expression ê²°ê³¼ê°€ ìº”ë²„ìŠ¤ ì—­í• 
- **Traceability**: Base resultì™€ final resultë¥¼ ë³„ë„ë¡œ ìºì‹±í•˜ì—¬ ì¶”ì  ê°€ëŠ¥
- **DRY Principle**: Lazy initializationìœ¼ë¡œ ëª¨ë“  Expressionì—ì„œ ìžë™ ìž‘ë™

### Expression.__setitem__ êµ¬í˜„ (DRY Lazy Initialization)

```python
class Expression(ABC):
    """Base class for all Expressions.
    
    Supports lazy assignment via __setitem__:
        signal[mask] = value  # Stores assignment, does not evaluate
    """
    
    def __setitem__(self, mask, value):
        """Store assignment for lazy evaluation.
        
        Uses lazy initialization - _assignments list is created on first use.
        This follows the DRY principle: no __post_init__ needed in subclasses.
        
        Args:
            mask: Boolean Expression or DataArray indicating where to assign
            value: Scalar value to assign where mask is True
        
        Note:
            Assignments are stored as (mask, value) tuples and applied sequentially
            during evaluation. Later assignments overwrite earlier ones for overlapping
            positions.
        """
        # Lazy initialization - create _assignments if it doesn't exist
        if not hasattr(self, '_assignments'):
            self._assignments = []
        
        self._assignments.append((mask, value))
```

**Lazy Initializationì˜ ìž¥ì **:
1. âœ… **No Boilerplate**: ëª¨ë“  Expression ì„œë¸Œí´ëž˜ìŠ¤ì— `__post_init__` ë¶ˆí•„ìš”
2. âœ… **DRY Principle**: ì¤‘ë³µ ì½”ë“œ ì œê±°
3. âœ… **Automatic**: ëª¨ë“  Expressionì—ì„œ ìžë™ìœ¼ë¡œ ìž‘ë™
4. âœ… **Efficient**: í• ë‹¹ì´ ì—†ìœ¼ë©´ `_assignments` ì†ì„±ë„ ìƒì„±ë˜ì§€ ì•ŠìŒ

### Visitor Integration

```python
class EvaluateVisitor:
    def evaluate(self, expr: Expression) -> xr.DataArray:
        """Evaluate expression and apply assignments if present."""
        # Step 1: Evaluate base expression (tree traversal)
        base_result = expr.accept(self)
        
        # Step 2: Check if expression has assignments (lazy initialization)
        assignments = getattr(expr, '_assignments', None)
        if assignments:
            # Cache base result for traceability
            base_name = f"{expr.__class__.__name__}_base"
            self._cache[self._step_counter] = (base_name, base_result)
            self._step_counter += 1
            
            # Apply assignments sequentially
            final_result = self._apply_assignments(base_result, assignments)
            
            # Apply universe masking to final result
            if self._universe_mask is not None:
                final_result = final_result.where(self._universe_mask)
            
            # Cache final result
            final_name = f"{expr.__class__.__name__}_with_assignments"
            self._cache[self._step_counter] = (final_name, final_result)
            self._step_counter += 1
            
            return final_result
        
        # No assignments, return base result as-is
        return base_result
    
    def _apply_assignments(self, base_result: xr.DataArray, assignments: list) -> xr.DataArray:
        """Apply assignments sequentially to base result."""
        result = base_result.copy(deep=True)
        
        for mask_expr, value in assignments:
            # If mask is an Expression, evaluate it
            if hasattr(mask_expr, 'accept'):
                mask_data = mask_expr.accept(self)
            else:
                # Already a DataArray or numpy array
                mask_data = mask_expr
            
            # Ensure mask is boolean (required for ~ operator)
            mask_bool = mask_data.astype(bool)
            
            # Apply assignment: replace values where mask is True
            result = result.where(~mask_bool, value)
        
        return result
```

### Constant Expression (Blank Canvas)

```python
from dataclasses import dataclass
import numpy as np
import xarray as xr
from alpha_canvas.core.expression import Expression


@dataclass(eq=False)
class Constant(Expression):
    """Expression that produces a constant-valued DataArray.
    
    Creates a universe-shaped (T, N) DataArray filled with the specified
    constant value. Serves as a "blank canvas" for signal construction.
    
    Example:
        >>> signal = Constant(0.0)  # Blank canvas (all zeros)
        >>> signal[mask1] = 1.0     # Assign long positions
        >>> signal[mask2] = -1.0    # Assign short positions
    """
    value: float
    
    def accept(self, visitor):
        return visitor.visit_constant(self)
```

### ì‚¬ìš© ì˜ˆì‹œ: Fama-French 2Ã—3 Factor

```python
# Step 1: Create size and value classifications
rc.add_data('size', CsQuantile(Field('market_cap'), bins=2, labels=['small', 'big']))
rc.add_data('value', CsQuantile(Field('book_to_market'), bins=3, labels=['low', 'medium', 'high']))

# Step 2: Create selector masks
is_small = rc.data['size'] == 'small'
is_big = rc.data['size'] == 'big'
is_low = rc.data['value'] == 'low'
is_high = rc.data['value'] == 'high'

# Step 3: Construct signal with lazy assignments
signal = Constant(0.0)                # Implicit blank canvas
signal[is_small & is_high] = 1.0      # Small/High-Value (long)
signal[is_big & is_low] = -1.0        # Big/Low-Value (short)

# Step 4: Evaluate (assignments applied here)
result = rc.evaluate(signal)

# Result: universe-shaped (T, N) array
#  - 1.0 where size=='small' AND value=='high'
#  - -1.0 where size=='big' AND value=='low'
#  - 0.0 elsewhere (neutral)
#  - NaN outside universe
```

### Overlapping Masks (Sequential Application)

```python
signal = Constant(0.0)
signal[is_small] = 0.5                # All small caps = 0.5
signal[is_small & is_high] = 1.0      # Small/High overwrites to 1.0

# Result: Later assignment wins for overlapping positions
#  - Small/High: 1.0 (overwritten)
#  - Small/Other: 0.5 (from first assignment)
#  - Others: 0.0 (from Constant)
```

### Traceability

```python
# After evaluation, check cached steps
for step_idx in sorted(rc._evaluator._cache.keys()):
    name, data = rc._evaluator._cache[step_idx]
    print(f"Step {step_idx}: {name}")

# Output:
#   Step 0: Constant_0.0_base             # Base constant array
#   Step 1: Field_size                     # Size classification
#   Step 2: Field_value                    # Value classification
#   Step 3: Equals                         # is_small mask
#   Step 4: Equals                         # is_high mask
#   Step 5: And                            # is_small & is_high
#   Step 6: Constant_0.0_with_assignments  # Final signal with assignments
```

**í•µì‹¬ ìž¥ì **:
- Base resultì™€ final resultê°€ ë³„ë„ ë‹¨ê³„ë¡œ ìºì‹±ë¨
- PnL trackingì— í•„ìˆ˜ì ì¸ ê¸°ëŠ¥
- ê° í• ë‹¹ì˜ ì˜í–¥ì„ ë‹¨ê³„ë³„ë¡œ ì¶”ì  ê°€ëŠ¥

### Implementation Checklist

- âœ… `Expression.__setitem__` with lazy initialization (DRY)
- âœ… `Visitor.evaluate()` handles assignments
- âœ… `Visitor._apply_assignments()` sequential application
- âœ… `Constant` Expression for blank canvas
- âœ… `visit_constant()` in Visitor
- âœ… Boolean mask conversion (`.astype(bool)`)
- âœ… Universe masking integration
- âœ… Traceability (separate base/final caching)
- âœ… Tests: storage, evaluation, overlapping masks, caching
- âœ… Showcase: Fama-French 2Ã—3 factor construction

---

## 3.4.3. Portfolio Weight Scaling ðŸ“‹ **PLANNED**

### ê°œìš”

**Portfolio Weight Scaling**ì€ ìž„ì˜ì˜ ì‹œê·¸ë„ ê°’ì„ ì œì•½ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¤‘ì¹˜ë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë“ˆìž…ë‹ˆë‹¤. Strategy Patternì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ë§ ì „ëžµì„ í”ŒëŸ¬ê·¸ì¸ ë°©ì‹ìœ¼ë¡œ ì§€ì›í•©ë‹ˆë‹¤.

**í•µì‹¬ ì„¤ê³„ ì›ì¹™**:
- **Stateless**: ìŠ¤ì¼€ì¼ëŸ¬ëŠ” ìƒíƒœë¥¼ ì €ìž¥í•˜ì§€ ì•ŠìŒ (í•­ìƒ ëª…ì‹œì  íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬)
- **Strategy Pattern**: ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ë§ ì „ëžµì„ ì‰½ê²Œ êµì²´ ê°€ëŠ¥
- **Cross-Sectional**: ê° ì‹œì  ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬ (`groupby('time').map()` íŒ¨í„´)
- **NaN-Aware**: ìœ ë‹ˆë²„ìŠ¤ ë§ˆìŠ¤í‚¹ ìžë™ ë³´ì¡´

### WeightScaler ë² ì´ìŠ¤ í´ëž˜ìŠ¤

```python
from abc import ABC, abstractmethod
import xarray as xr


class WeightScaler(ABC):
    """Abstract base class for weight scaling strategies.
    
    Converts arbitrary signal values to portfolio weights by applying
    constraints cross-sectionally (independently for each time period).
    
    Philosophy:
    - Operates on (T, N) signal DataArray
    - Returns (T, N) weight DataArray
    - Each time slice processed independently
    - NaN-aware (respects universe masking)
    - Strategy pattern: subclasses define scaling logic
    """
    
    @abstractmethod
    def scale(self, signal: xr.DataArray) -> xr.DataArray:
        """Scale signal to weights.
        
        Args:
            signal: (T, N) DataArray with arbitrary signal values
        
        Returns:
            (T, N) DataArray with portfolio weights
        
        Note:
            - Must handle NaN values (preserve them in output)
            - Must process each time slice independently
            - Should validate inputs (not all NaN, etc.)
        """
        pass
    
    def _validate_signal(self, signal: xr.DataArray):
        """Validate signal before scaling."""
        if signal.dims != ('time', 'asset'):
            raise ValueError(
                f"Signal must have dims ('time', 'asset'), got {signal.dims}"
            )
        
        # Check if any time slice has non-NaN values
        non_nan_counts = (~signal.isnull()).sum(dim='asset')
        if (non_nan_counts == 0).all():
            raise ValueError(
                "All signal values are NaN across all time periods"
            )
```

### GrossNetScaler (í†µí•© í”„ë ˆìž„ì›Œí¬)

```python
class GrossNetScaler(WeightScaler):
    """Unified weight scaler based on gross and net exposure targets.
    
    Uses the unified framework:
        L_target = (G + N) / 2
        S_target = (G - N) / 2
    
    Where:
        G = target_gross_exposure = sum(abs(weights))
        N = target_net_exposure = sum(weights)
        L = sum of positive weights
        S = sum of negative weights (negative value)
    
    Args:
        target_gross: Target gross exposure (default: 2.0 for 200% gross)
        target_net: Target net exposure (default: 0.0 for dollar-neutral)
    
    Example:
        >>> # Dollar neutral: L=1.0, S=-1.0
        >>> scaler = GrossNetScaler(target_gross=2.0, target_net=0.0)
        >>> 
        >>> # Net long 10%: L=1.1, S=-0.9
        >>> scaler = GrossNetScaler(target_gross=2.0, target_net=0.2)
        >>> 
        >>> # Crypto futures: L=0.5, S=-0.5
        >>> scaler = GrossNetScaler(target_gross=1.0, target_net=0.0)
    """
    
    def __init__(self, target_gross: float = 2.0, target_net: float = 0.0):
        self.target_gross = target_gross
        self.target_net = target_net
        
        # Validate constraints
        if target_gross < 0:
            raise ValueError("target_gross must be non-negative")
        if abs(target_net) > target_gross:
            raise ValueError(
                "Absolute net exposure cannot exceed gross exposure"
            )
        
        # Calculate target long and short books
        self.L_target = (target_gross + target_net) / 2.0
        self.S_target = (target_net - target_gross) / 2.0  # Negative value
    
    def scale(self, signal: xr.DataArray) -> xr.DataArray:
        """Scale signal using gross/net exposure constraints."""
        self._validate_signal(signal)
        
        # Apply scaling cross-sectionally using groupby
        return signal.groupby('time').map(self._scale_single_period)
    
    def _scale_single_period(self, signal_slice: xr.DataArray) -> xr.DataArray:
        """Scale a single time period (cross-section).
        
        Note:
            Uses the same pattern as cs_quantile for cross-sectional
            independence and shape preservation.
        """
        # Separate positive and negative signals
        s_pos = signal_slice.where(signal_slice > 0, 0.0)
        s_neg = signal_slice.where(signal_slice < 0, 0.0)
        
        # Calculate sums (NaN-safe)
        sum_pos = s_pos.sum(skipna=True)
        sum_neg_abs = np.abs(s_neg.sum(skipna=True))
        
        # Initialize weights with zeros
        weights = xr.zeros_like(signal_slice)
        
        # Scale positive side
        if sum_pos > 0:
            weights = weights + (s_pos / sum_pos) * self.L_target
        
        # Scale negative side
        if sum_neg_abs > 0:
            weights = weights + (s_neg / sum_neg_abs) * self.S_target
        
        # Preserve NaN where signal was NaN (universe masking)
        weights = weights.where(~signal_slice.isnull())
        
        return weights
```

### íŽ¸ì˜ Scaler í´ëž˜ìŠ¤ë“¤

```python
class DollarNeutralScaler(GrossNetScaler):
    """Dollar neutral: sum(long) = 1.0, sum(short) = -1.0.
    
    Convenience wrapper for GrossNetScaler(2.0, 0.0).
    
    This is the most common scaler for market-neutral strategies.
    """
    def __init__(self):
        super().__init__(target_gross=2.0, target_net=0.0)


class LongOnlyScaler(WeightScaler):
    """Long-only portfolio: sum(weights) = target_long.
    
    Ignores negative signals, normalizes positive signals to sum to target.
    
    Args:
        target_long: Target sum of weights (default: 1.0)
    
    Example:
        >>> scaler = LongOnlyScaler(target_long=1.0)
        >>> # All negative signals become 0, positives sum to 1.0
    """
    
    def __init__(self, target_long: float = 1.0):
        self.target_long = target_long
    
    def scale(self, signal: xr.DataArray) -> xr.DataArray:
        self._validate_signal(signal)
        return signal.groupby('time').map(self._scale_single_period)
    
    def _scale_single_period(self, signal_slice: xr.DataArray) -> xr.DataArray:
        # Only keep positive values
        s_pos = signal_slice.where(signal_slice > 0, 0.0)
        sum_pos = s_pos.sum(skipna=True)
        
        if sum_pos > 0:
            weights = (s_pos / sum_pos) * self.target_long
        else:
            weights = xr.zeros_like(signal_slice)
        
        # Preserve NaN
        return weights.where(~signal_slice.isnull())
```

### Facade í†µí•©

```python
# In AlphaCanvas class (src/alpha_canvas/core/facade.py)

def scale_weights(
    self, 
    signal: Union[Expression, xr.DataArray], 
    scaler: 'WeightScaler'
) -> xr.DataArray:
    """Scale signal to portfolio weights.
    
    Args:
        signal: Expression or DataArray with signal values
        scaler: WeightScaler strategy instance (REQUIRED)
    
    Returns:
        (T, N) DataArray with portfolio weights
    
    Note:
        Scaler is a required parameter - no default.
        This is intentional for explicit, research-friendly API.
    
    Example:
        >>> from alpha_canvas.portfolio import DollarNeutralScaler
        >>> 
        >>> signal = ts_mean(Field('returns'), 5)
        >>> scaler = DollarNeutralScaler()
        >>> weights = rc.scale_weights(signal, scaler)
        >>> 
        >>> # Compare multiple scalers
        >>> w1 = rc.scale_weights(signal, DollarNeutralScaler())
        >>> w2 = rc.scale_weights(signal, LongOnlyScaler(1.0))
    """
    # Evaluate if Expression
    if hasattr(signal, 'accept'):
        signal_data = self.evaluate(signal)
    else:
        signal_data = signal
    
    # Apply scaling strategy
    weights = scaler.scale(signal_data)
    
    return weights
```

### ì‚¬ìš© íŒ¨í„´ ë° ì˜ˆì‹œ

**íŒ¨í„´ 1: ì§ì ‘ ì‚¬ìš© (ê°€ìž¥ ëª…ì‹œì )**

```python
from alpha_canvas.portfolio import DollarNeutralScaler

# 1. Signal ìƒì„±
signal_expr = ts_mean(Field('returns'), 5)
signal_data = rc.evaluate(signal_expr)

# 2. Scaler ìƒì„± ë° ì ìš©
scaler = DollarNeutralScaler()
weights = scaler.scale(signal_data)

# ê²€ì¦
assert abs(weights[weights > 0].sum() - 1.0) < 1e-6  # Long = 1.0
assert abs(weights[weights < 0].sum() + 1.0) < 1e-6  # Short = -1.0
```

**íŒ¨í„´ 2: Facade íŽ¸ì˜ ë©”ì„œë“œ**

```python
from alpha_canvas.portfolio import GrossNetScaler

signal_expr = ts_mean(Field('returns'), 5)
scaler = GrossNetScaler(target_gross=2.0, target_net=0.2)

# evaluate + scale í•œ ë²ˆì—
weights = rc.scale_weights(signal_expr, scaler)
```

**íŒ¨í„´ 3: ì—¬ëŸ¬ ìŠ¤ì¼€ì¼ëŸ¬ ë¹„êµ (ì—°êµ¬ìš©)**

```python
from alpha_canvas.portfolio import (
    DollarNeutralScaler,
    GrossNetScaler,
    LongOnlyScaler
)

# ë™ì¼ ì‹œê·¸ë„ì— ì—¬ëŸ¬ ìŠ¤ì¼€ì¼ë§ ì „ëžµ ì ìš©
signal = rc.evaluate(my_alpha_expr)

strategies = {
    'dollar_neutral': DollarNeutralScaler(),
    'net_long_10pct': GrossNetScaler(2.0, 0.2),
    'long_only': LongOnlyScaler(1.0),
    'crypto_futures': GrossNetScaler(1.0, 0.0)
}

weights_dict = {
    name: scaler.scale(signal)
    for name, scaler in strategies.items()
}

# ê° ì „ëžµ ë¹„êµ
for name, weights in weights_dict.items():
    print(f"{name}:")
    print(f"  Gross: {abs(weights).sum()}")
    print(f"  Net: {weights.sum()}")
```

### Module Structure

```
src/alpha_canvas/portfolio/
â”œâ”€â”€ __init__.py              # Export all scalers
â”œâ”€â”€ base.py                  # WeightScaler abstract base class
â””â”€â”€ strategies.py            # GrossNetScaler, DollarNeutralScaler, LongOnlyScaler
```

### í…ŒìŠ¤íŠ¸ ì „ëžµ

**1. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸** (`tests/test_portfolio/test_strategies.py`):
- `GrossNetScaler` ìˆ˜í•™ ê²€ì¦ (L_target, S_target ê³„ì‚°)
- ê° scalerì˜ constraint ì¶©ì¡± í™•ì¸
- NaN ë³´ì¡´ ê²€ì¦
- Edge cases: all positive, all negative, zeros

**2. í†µí•© í…ŒìŠ¤íŠ¸**:
- AlphaCanvas.scale_weights() í†µí•©
- Expression â†’ evaluation â†’ scaling ì „ì²´ íŒŒì´í”„ë¼ì¸
- Universe masking ë³´ì¡´ ê²€ì¦

**3. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸**:
- í¬ë¡œìŠ¤-ì„¹ì…˜ ë…ë¦½ì„± ê²€ì¦
- Large dataset (T=1000, N=3000) ë²¤ì¹˜ë§ˆí¬

### Implementation Checklist

- [ ] `WeightScaler` abstract base class
- [ ] `GrossNetScaler` with unified framework
- [ ] `DollarNeutralScaler` convenience wrapper
- [ ] `LongOnlyScaler` implementation
- [ ] `AlphaCanvas.scale_weights()` facade method
- [ ] Unit tests for each scaler
- [ ] Integration tests with facade
- [ ] Experiment: weight scaling validation
- [ ] Showcase: Fama-French signal â†’ weights
- [ ] Documentation in all three docs

---

## 3.5. ê°œë°œ ì›ì¹™

### 3.5.1. ì§€ì—° í‰ê°€ (Lazy Evaluation)

- `Expression` ê°ì²´ëŠ” "ë ˆì‹œí”¼"ì´ë©° ë°ì´í„°ë¥¼ ê°€ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.
- `EvaluateVisitor`ê°€ ì‹¤ì œ í‰ê°€ë¥¼ ë‹´ë‹¹í•˜ë©°, ì´ë•Œ ìºì‹±ì´ ë°œìƒí•©ë‹ˆë‹¤.
- ë¶ˆí•„ìš”í•œ ìž¬ê³„ì‚°ì„ ë°©ì§€í•˜ì—¬ ì„±ëŠ¥ì„ ìµœì í™”í•©ë‹ˆë‹¤.

### 3.5.2. ë ˆì´ë¸” ìš°ì„  (Label-first)

- ëª¨ë“  ë²„í‚· ì—°ì‚°ì€ ì •ìˆ˜ ì¸ë±ìŠ¤ ëŒ€ì‹  **ì˜ë¯¸ ìžˆëŠ” ë ˆì´ë¸”**ì„ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.
- ì˜ˆ: `cs_quantile(..., labels=['small', 'mid', 'big'])`
- ì´ëŠ” PRDì˜ í•µì‹¬ ë¬¸ì œ 2ë¥¼ í•´ê²°í•˜ëŠ” ì„¤ê³„ ì›ì¹™ìž…ë‹ˆë‹¤.

### 3.5.3. ì¶”ì ì„± ìš°ì„  (Traceability-first)

- ëª¨ë“  ì¤‘ê°„ ê³„ì‚° ê²°ê³¼ëŠ” **ì •ìˆ˜ step ì¸ë±ìŠ¤**ë¡œ ìºì‹œë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
- ì‚¬ìš©ìžëŠ” ìž¬ê³„ì‚° ì—†ì´ ëª¨ë“  ì¤‘ê°„ ë‹¨ê³„ë¥¼ ê²€ì‚¬í•  ìˆ˜ ìžˆì–´ì•¼ í•©ë‹ˆë‹¤.
- ì´ëŠ” PRDì˜ í•µì‹¬ ë¬¸ì œ 3ì„ í•´ê²°í•˜ëŠ” ì„¤ê³„ ì›ì¹™ìž…ë‹ˆë‹¤.

### 3.5.4. Pythonic ìš°ì„  (Pythonic-first)

- ë¬¸ìžì—´ DSL ëŒ€ì‹  Pythonì˜ ë„¤ì´í‹°ë¸Œ ë¬¸ë²•ì„ í™œìš©í•©ë‹ˆë‹¤.
- ì˜ˆ: `&`, `|`, `[]`, `=` ì—°ì‚°ìž ì˜¤ë²„ë¡œë”©
- IDE ìžë™ì™„ì„± ë° íƒ€ìž… ížŒíŠ¸ë¥¼ ìµœëŒ€í•œ í™œìš©í•©ë‹ˆë‹¤.

### 3.5.5. ì¢…ì† ì •ë ¬ ì§€ì› (Dependent Sort Support)

- `cs_quantile`ì€ `group_by` íŒŒë¼ë¯¸í„°ë¡œ ì¢…ì† ì •ë ¬ì„ ì§€ì›í•´ì•¼ í•©ë‹ˆë‹¤.
- ì´ëŠ” Fama-French íŒ©í„° ìž¬í˜„ì„ ìœ„í•œ í•µì‹¬ ìš”êµ¬ì‚¬í•­ìž…ë‹ˆë‹¤.
- `mask` íŒŒë¼ë¯¸í„°ë¡œ ë¡œìš°ë ˆë²¨ ì»¤ìŠ¤í„°ë§ˆì´ì§•ë„ ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤.

## 3.6. í…ŒìŠ¤íŠ¸ ì „ëžµ

### 3.6.1. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

- `Expression` ê° ë…¸ë“œ í´ëž˜ìŠ¤
- `EvaluateVisitor` ë©”ì„œë“œë³„ í…ŒìŠ¤íŠ¸ (íŠ¹ížˆ `visit_cs_quantile`ì˜ `group_by` ë¡œì§)
- `ConfigLoader` YAML íŒŒì‹± í…ŒìŠ¤íŠ¸
- ì •ìˆ˜ step ì¸ë±ì‹± ë¡œì§

### 3.6.2. í†µí•© í…ŒìŠ¤íŠ¸

- ì „ì²´ ì›Œí¬í”Œë¡œìš° (ì´ˆê¸°í™” â†’ ë°ì´í„° ë¡œë“œ â†’ Expression í‰ê°€ â†’ PnL ì¶”ì )
- ë“€ì–¼ ì¸í„°íŽ˜ì´ìŠ¤ ì¡°í•© ì‹œë‚˜ë¦¬ì˜¤
- ë…ë¦½/ì¢…ì† ì´ì¤‘ ì •ë ¬ ê¸°ë°˜ íŒ©í„° ìˆ˜ìµë¥  ê³„ì‚° íŒ¨í„´
- Fama-French SMB, HML íŒ©í„° ìž¬í˜„ ê²€ì¦

### 3.6.3. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

- ëŒ€ìš©ëŸ‰ ë°ì´í„° (T=1000, N=3000) ì²˜ë¦¬ ì‹œê°„
- ìºì‹± íš¨ê³¼ ê²€ì¦ (stepë³„ ì¡°íšŒ ì„±ëŠ¥)
- ì¢…ì† ì •ë ¬ì˜ ì˜¤ë²„í—¤ë“œ ì¸¡ì •
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§

## 3.7. ì½”ë”© ì»¨ë²¤ì…˜

- **íƒ€ìž… ížŒíŠ¸:** ëª¨ë“  public ë©”ì„œë“œì— íƒ€ìž… ížŒíŠ¸ í•„ìˆ˜
- **Docstring:** Google ìŠ¤íƒ€ì¼ docstring ì‚¬ìš©
- **Linting:** `ruff` ì‚¬ìš©
- **Formatting:** `black` ì‚¬ìš©
- **Import ìˆœì„œ:** í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ â†’ ì„œë“œíŒŒí‹° â†’ ë¡œì»¬

## 3.8. ì¸í„°íŽ˜ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ

### 3.8.1. Step ì¸ë±ì‹± ë³€ê²½ì‚¬í•­

**ì´ì „ (ë¬¸ìžì—´ ê¸°ë°˜):**

```python
# âŒ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
rc.trace_pnl('alpha1', step='ts_mean')
rc.get_intermediate('alpha1', step='ts_mean')
```

**í˜„ìž¬ (ì •ìˆ˜ ê¸°ë°˜):**

```python
# âœ… ì˜¬ë°”ë¥¸ ì‚¬ìš©ë²•
rc.trace_pnl('alpha1', step=1)  # step 1ê¹Œì§€ ì¶”ì 
rc.get_intermediate('alpha1', step=1)  # step 1 ë°ì´í„° ì¡°íšŒ

# ëª¨ë“  ë‹¨ê³„ ì¶”ì 
rc.trace_pnl('alpha1')  # step=None (ê¸°ë³¸ê°’)
# ë°˜í™˜: {0: {...}, 1: {...}, 2: {...}}
```

### 3.8.2. ë…ë¦½/ì¢…ì† ì •ë ¬ íŒ¨í„´

**ë…ë¦½ ì •ë ¬ (ë³€ê²½ ì—†ìŒ):**

```python
# âœ… ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ìž‘ë™
rc.add_axis('size', cs_quantile(rc.data.mcap, bins=2, labels=['small','big']))
```

**ì¢…ì† ì •ë ¬ (ì‹ ê·œ ê¸°ëŠ¥):**

```python
# âœ… ìƒˆë¡œìš´ ê¸°ëŠ¥
rc.add_axis('value', cs_quantile(rc.data.btm, bins=3, labels=['low','mid','high'],
                                   group_by='size'))
```

**ë§ˆìŠ¤í¬ ê¸°ë°˜ í•„í„°ë§ (ì‹ ê·œ ê¸°ëŠ¥):**

```python
# âœ… ìƒˆë¡œìš´ ê¸°ëŠ¥
mask = rc.data.volume > threshold
rc.add_axis('filtered', cs_quantile(rc.data.returns, bins=5, labels=[...],
                                      mask=mask))
```

## 3.9. êµ¬í˜„ ì„±ê³µ ê¸°ì¤€

### 3.9.1. Step ì¸ë±ì‹± ê²€ì¦

âœ… **í•„ìˆ˜ ë™ìž‘:**

- `rc.trace_pnl('alpha', step=2)` â†’ step 2ê¹Œì§€ì˜ PnL ë°˜í™˜
- `rc.get_intermediate('alpha', step=2)` â†’ step 2ì˜ ìºì‹œëœ DataArray ë°˜í™˜
- ë³‘ë ¬ Expression (ë¸Œëžœì¹˜ê°€ ìžˆëŠ” íŠ¸ë¦¬)ì—ì„œ ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ ì¸ë±ì‹±
- ìž˜ëª»ëœ step ì¸ë±ìŠ¤ ìž…ë ¥ ì‹œ ëª…í™•í•œ ì—ëŸ¬ ë©”ì‹œì§€

### 3.9.2. ì¢…ì† ì •ë ¬ ê²€ì¦

âœ… **í•„ìˆ˜ ë™ìž‘:**

- **ë…ë¦½ ì •ë ¬**: `cs_quantile(...)` â†’ ì „ì²´ ìœ ë‹ˆë²„ìŠ¤ ëŒ€ìƒ quantile
- **ì¢…ì† ì •ë ¬**: `cs_quantile(..., group_by='axis')` â†’ ê° ê·¸ë£¹ ë‚´ quantile
- **ë§ˆìŠ¤í¬ í•„í„°ë§**: `cs_quantile(..., mask=...)` â†’ í•„í„°ë§ëœ ë¶€ë¶„ì§‘í•© ëŒ€ìƒ quantile
- ë…ë¦½/ì¢…ì† ì •ë ¬ì˜ ê²°ê³¼ cutoffê°€ ëª…í™•ížˆ ë‹¤ë¦„ (ê²€ì¦ í…ŒìŠ¤íŠ¸ í•„ìš”)

### 3.9.3. Fama-French ìž¬í˜„ ê²€ì¦

âœ… **í•„ìˆ˜ ë™ìž‘:**

- SMB (ë…ë¦½ 2Ã—3 ì •ë ¬) â†’ ì˜ˆìƒëœ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¤‘ì¹˜ ìƒì„±
- HML (ì¢…ì† 2Ã—3 ì •ë ¬) â†’ ì˜ˆìƒëœ í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¤‘ì¹˜ ìƒì„±
- ë…ë¦½/ì¢…ì† ë°©ì‹ì˜ cutoff ì°¨ì´ ê²€ì¦ (academic paper ê¸°ì¤€ê³¼ ì¼ì¹˜)

## 3.10. ë‹¤ìŒ ë‹¨ê³„

### Phase 1: í•µì‹¬ ì»´í¬ë„ŒíŠ¸ êµ¬í˜„

- [ ] `Expression` ì¶”ìƒ í´ëž˜ìŠ¤ ë° Leaf/Composite êµ¬í˜„
- [ ] `EvaluateVisitor` ê¸°ë³¸ êµ¬ì¡° ë° ìºì‹± ë©”ì»¤ë‹ˆì¦˜ (ì •ìˆ˜ step ì¹´ìš´í„° í¬í•¨)
- [ ] `ConfigLoader` ë° YAML íŒŒì‹±
- [ ] `AlphaCanvas` Facade ê¸°ë³¸ êµ¬ì¡°

### Phase 2: ì—°ì‚°ìž êµ¬í˜„

- [ ] Timeseries ì—°ì‚°ìž (`ts_mean`, `ts_sum`, etc.)
- [ ] Cross-sectional ì—°ì‚°ìž (`cs_rank`, `cs_quantile` with `group_by` and `mask`)
- [ ] Transform ì—°ì‚°ìž (`group_neutralize`, etc.)

### Phase 3: ì¶”ì ì„± ë° ë¶„ì„

- [ ] `PnLTracer` êµ¬í˜„
- [ ] ì„ íƒì  ë‹¨ê³„ ì¶”ì  ë¡œì§ (ì •ìˆ˜ ì¸ë±ìŠ¤ ê¸°ë°˜)
- [ ] ì„±ê³¼ ì§€í‘œ ê³„ì‚°
- [ ] PnL ë¦¬í¬íŠ¸ì— step ë©”íƒ€ë°ì´í„° í‘œì‹œ

### Phase 4: ì¸í„°íŽ˜ì´ìŠ¤ ì™„ì„±

- [ ] Property accessor (`rc.data`, `rc.axis`)
- [ ] NumPy-style í• ë‹¹ (`rc[mask] = value`)
- [ ] í—¬í¼ ë©”ì„œë“œ (`rc.ts_mean()` ë“±)

### Phase 5: ê²€ì¦ ë° í…ŒìŠ¤íŠ¸

- [ ] ì •ìˆ˜ step ì¸ë±ì‹± ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
- [ ] `_quantile_grouped` ë¡œì§ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
- [ ] Fama-French SMB/HML í†µí•© í…ŒìŠ¤íŠ¸
- [ ] ì¢…ì† ì •ë ¬ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

---

**ì°¸ê³ :** ì´ ë¬¸ì„œëŠ” ì‹¤ì œ êµ¬í˜„ ê³¼ì •ì—ì„œ ë°œê²¬ë˜ëŠ” ìƒˆë¡œìš´ íŒ¨í„´ê³¼ êµí›ˆì„ ì§€ì†ì ìœ¼ë¡œ ë°˜ì˜í•´ì•¼ í•©ë‹ˆë‹¤ (Living Document).
