# 3. Implementation Guide

Ïù¥ Î¨∏ÏÑúÎäî alpha-canvasÏùò Íµ¨Ï≤¥Ï†ÅÏù∏ Íµ¨ÌòÑ Î∞©Î≤ïÎ°†, Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÑ§Í≥Ñ, Í∑∏Î¶¨Í≥† Í∞úÎ∞ú ÌëúÏ§ÄÏùÑ Ï†ïÏùòÌï©ÎãàÎã§.

## 3.1. ÌîÑÎ°úÏ†ùÌä∏ Íµ¨Ï°∞

```text
alpha-canvas/
‚îú‚îÄ‚îÄ config/                      # ÌÉÄÏûÖÎ≥Ñ ÏÑ§Ï†ï ÌååÏùº
‚îÇ   ‚îú‚îÄ‚îÄ data.yaml               # Îç∞Ïù¥ÌÑ∞ ÌïÑÎìú Ï†ïÏùò
‚îÇ   ‚îú‚îÄ‚îÄ db.yaml                 # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ ÏÑ§Ï†ï (ÏÑ†ÌÉùÏ†Å)
‚îÇ   ‚îî‚îÄ‚îÄ compute.yaml            # Í≥ÑÏÇ∞ Í¥ÄÎ†® ÏÑ§Ï†ï (ÏÑ†ÌÉùÏ†Å)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ alpha_canvas/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ core/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ facade.py       # AlphaCanvas (rc) ÌçºÏÇ¨Îìú ÌÅ¥ÎûòÏä§
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ expression.py   # Expression Ïª¥Ìè¨Ïßì Ìä∏Î¶¨
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ visitor.py      # EvaluateVisitor Ìå®ÌÑ¥ (ÌÉÄÏûÖ Í≤ÄÏÇ¨ Ìè¨Ìï®)
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ config.py       # ConfigLoader
‚îÇ       ‚îú‚îÄ‚îÄ ops/                # Ïó∞ÏÇ∞Ïûê (ts_mean, rank, etc.)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ timeseries.py   # ts_mean, ts_sum, etc. (Îã§ÌòïÏÑ± Ïó∞ÏÇ∞Ïûê)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ crosssection.py # cs_rank Îì± (Panel Ï†ÑÏö© Ïó∞ÏÇ∞Ïûê)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ classification.py # cs_quantile, cs_cut (Î∂ÑÎ•òÍ∏∞/Ï∂ï ÏÉùÏÑ±)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ transform.py    # group_neutralize, etc.
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ tensor.py       # ÎØ∏Îûò ÌôïÏû•Ïö© (MVPÏóêÏÑúÎäî ÎπÑÏñ¥ÏûàÏùå)
‚îÇ       ‚îú‚îÄ‚îÄ portfolio/          # Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ Íµ¨ÏÑ±
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ base.py         # WeightScaler Ï∂îÏÉÅ Î≤†Ïù¥Ïä§ ÌÅ¥ÎûòÏä§
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ strategies.py   # GrossNetScaler, DollarNeutralScaler, LongOnlyScaler
‚îÇ       ‚îú‚îÄ‚îÄ analysis/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ pnl.py          # PnLTracer
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ metrics.py      # ÏÑ±Í≥º ÏßÄÌëú Í≥ÑÏÇ∞
‚îÇ       ‚îî‚îÄ‚îÄ utils/
‚îÇ           ‚îú‚îÄ‚îÄ accessor.py     # Property Ï†ëÍ∑ºÏûê (data, axis, rules)
‚îÇ           ‚îî‚îÄ‚îÄ mask.py         # ÎßàÏä§ÌÅ¨ Ìó¨Ìçº
‚îú‚îÄ‚îÄ experiments/                # Ïã§Ìóò Ïä§ÌÅ¨Î¶ΩÌä∏
‚îú‚îÄ‚îÄ tests/                      # ÌÖåÏä§Ìä∏
‚îî‚îÄ‚îÄ docs/
    ‚îî‚îÄ‚îÄ vibe_coding/
        ‚îú‚îÄ‚îÄ prd.md
        ‚îú‚îÄ‚îÄ architecture.md
        ‚îî‚îÄ‚îÄ implementation.md   # Ïù¥ Î¨∏ÏÑú
```

## 3.2. ÌïµÏã¨ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏÑ§Í≥Ñ

### 3.2.1. Ï¥àÍ∏∞Ìôî Î∞è ÏÑ§Ï†ï

```python
from alpha_canvas import AlphaCanvas

# config/ ÎîîÎ†âÌÜ†Î¶¨Ïùò Î™®Îì† YAML ÌååÏùºÏùÑ ÏûêÎèô Î°úÎìú
rc = AlphaCanvas()

# ÎòêÎäî ÌäπÏ†ï config ÎîîÎ†âÌÜ†Î¶¨ ÏßÄÏ†ï
rc = AlphaCanvas(config_dir='./custom_config')
```

**Íµ¨ÌòÑ ÏöîÍµ¨ÏÇ¨Ìï≠:**

- `AlphaCanvas.__init__()` ÎÇ¥Î∂ÄÏóêÏÑú `ConfigLoader`Î•º ÏÉùÏÑ±ÌïòÍ≥† `config/` ÎîîÎ†âÌÜ†Î¶¨Ïùò Î™®Îì† `.yaml` ÌååÏùºÏùÑ Î°úÎìúÌï©ÎãàÎã§.
- `ConfigLoader`Îäî `data.yaml`, `db.yaml` Îì±ÏùÑ Í∞ÅÍ∞Å ÌååÏã±ÌïòÏó¨ ÎÇ¥Î∂Ä dictÏóê Ï†ÄÏû•Ìï©ÎãàÎã§.

### 3.2.2. ÏΩîÏñ¥ Îç∞Ïù¥ÌÑ∞ Î™®Îç∏ Íµ¨ÌòÑ (Core Data Model Implementation)

#### A. `AlphaCanvas.add_data()` Íµ¨ÌòÑ (`facade.py`)

```python
def add_data(self, name: str, data: Union[Expression, xr.DataArray]) -> None:
    """Îç∞Ïù¥ÌÑ∞ Î≥ÄÏàòÎ•º DatasetÏóê Ï∂îÍ∞Ä (Expression ÎòêÎäî DataArray ÏßÄÏõê)"""
    
    # Case 1: Expression ÌèâÍ∞Ä (ÏùºÎ∞òÏ†ÅÏù∏ Í≤ΩÎ°ú)
    if isinstance(data, Expression):
        self.rules[name] = data  # Expression Ï†ÄÏû• (Ïû¨ÌèâÍ∞Ä Í∞ÄÎä•ÌïòÎèÑÎ°ù)
        result_array = self._evaluator.evaluate(data)  # VisitorÎ°ú ÌèâÍ∞Ä
        self.db = self.db.assign({name: result_array})  # data_varsÏóê Ï∂îÍ∞Ä
    
    # Case 2: DataArray ÏßÅÏ†ë Ï£ºÏûÖ (Open Toolkit: Inject)
    elif isinstance(data, xr.DataArray):
        # Ïô∏Î∂ÄÏóêÏÑú ÏÉùÏÑ±Ìïú Îç∞Ïù¥ÌÑ∞ Ï£ºÏûÖ (Visitor Í±¥ÎÑàÎõ∞Í∏∞)
        self.db = self.db.assign({name: data})
    
    else:
        raise TypeError(f"data must be Expression or DataArray, got {type(data)}")
```

**ÌïµÏã¨ ÏÇ¨Ìï≠:**

- `xarray.Dataset.assign()`ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Data VariableÎ°ú Ï∂îÍ∞Ä
- `Expression`Í≥º `DataArray` Î™®Îëê ÏßÄÏõê (Ïò§Î≤ÑÎ°úÎî©)
- Open Toolkit Ï≤†Ìïô: Ïô∏Î∂Ä Í≥ÑÏÇ∞ Í≤∞Í≥ºÎ•º seamlessly inject

#### B. `rc.db` ÌîÑÎ°úÌçºÌã∞ (Open Toolkit: Eject)

```python
@property
def db(self) -> xr.Dataset:
    """ÏàúÏàò xarray.Dataset Î∞òÌôò (Jupyter ejectÏö©)"""
    return self._dataset  # ÎÇ¥Î∂Ä DatasetÏùÑ Í∑∏ÎåÄÎ°ú ÎÖ∏Ï∂ú
```

**ÌïµÏã¨ ÏÇ¨Ìï≠:**

- ÎûòÌïë ÏóÜÏù¥ ÏàúÏàò `xarray.Dataset` Î∞òÌôò
- ÏÇ¨Ïö©ÏûêÎäî `pure_ds = rc.db`Î°ú Í∫ºÎÇ¥ÏÑú scipy/statsmodels ÏÇ¨Ïö© Í∞ÄÎä•

#### C. Ïú†ÎãàÎ≤ÑÏä§ ÎßàÏä§ÌÇπ (Universe Masking) ‚úÖ **IMPLEMENTED**

**ÏöîÍµ¨ÏÇ¨Ìï≠**: Ï¥àÍ∏∞Ìôî Ïãú Ïú†ÎãàÎ≤ÑÏä§Î•º ÏÑ§Ï†ïÌïòÍ≥†, Î™®Îì† Îç∞Ïù¥ÌÑ∞ÏôÄ Ïó∞ÏÇ∞Ïóê ÏûêÎèô Ï†ÅÏö©

```python
# AlphaCanvas Ï¥àÍ∏∞Ìôî with universe
rc = AlphaCanvas(
    start_date='2024-01-01',
    end_date='2024-12-31',
    universe=price > 5.0  # Boolean DataArray
)

# ÎòêÎäî ExpressionÏúºÎ°ú ÏÑ§Ï†ï
rc = AlphaCanvas(
    start_date='2024-01-01',
    end_date='2024-12-31',
    universe=Field('univ500')  # Field Expression (ÎØ∏Îûò ÌôïÏû•)
)

# Ïú†ÎãàÎ≤ÑÏä§ ÌôïÏù∏ (read-only)
print(f"Universe coverage: {rc.universe.sum().values} positions")
```

**Íµ¨ÌòÑ ÏÑ∏Î∂ÄÏÇ¨Ìï≠**:

**1. AlphaCanvasÏóê universe ÌååÎùºÎØ∏ÌÑ∞ Ï∂îÍ∞Ä**:
```python
class AlphaCanvas:
    def __init__(
        self,
        config_dir='config',
        start_date=None,
        end_date=None,
        time_index=None,
        asset_index=None,
        universe: Optional[Union[Expression, xr.DataArray]] = None  # NEW
    ):
        # ... Í∏∞Ï°¥ Ï¥àÍ∏∞Ìôî ...
        
        # Universe mask Ï¥àÍ∏∞Ìôî (Î∂àÎ≥Ä)
        self._universe_mask: Optional[xr.DataArray] = None
        if universe is not None:
            self._set_initial_universe(universe)
    
    def _set_initial_universe(self, universe: Union[Expression, xr.DataArray]) -> None:
        """Ïú†ÎãàÎ≤ÑÏä§ ÎßàÏä§ÌÅ¨Î•º Ï¥àÍ∏∞Ìôî Ïãú Ìïú Î≤àÎßå ÏÑ§Ï†ï (Î∂àÎ≥Ä)."""
        # Expression ÌèâÍ∞Ä (e.g., Field('univ500'))
        if isinstance(universe, Expression):
            universe_data = self._evaluator.evaluate(universe)
        else:
            universe_data = universe
        
        # Shape Í≤ÄÏ¶ù
        expected_shape = (
            len(self._panel.db.coords['time']), 
            len(self._panel.db.coords['asset'])
        )
        if universe_data.shape != expected_shape:
            raise ValueError(
                f"Universe mask shape {universe_data.shape} doesn't match "
                f"data shape {expected_shape}"
            )
        
        # Dtype Í≤ÄÏ¶ù
        if universe_data.dtype != bool:
            raise TypeError(f"Universe must be boolean, got {universe_data.dtype}")
        
        # Î∂àÎ≥Ä Ï†ÄÏû•
        self._universe_mask = universe_data
        
        # EvaluatorÏóê Ï†ÑÌåå (ÏûêÎèô Ï†ÅÏö© ÏúÑÌï¥)
        self._evaluator._universe_mask = self._universe_mask
    
    @property
    def universe(self) -> Optional[xr.DataArray]:
        """Ïú†ÎãàÎ≤ÑÏä§ ÎßàÏä§ÌÅ¨ Ï°∞Ìöå (read-only)."""
        return self._universe_mask
```

**2. EvaluateVisitorÏóê Ïù¥Ï§ë ÎßàÏä§ÌÇπ Íµ¨ÌòÑ**:
```python
class EvaluateVisitor:
    def __init__(self, data_source: xr.Dataset, data_loader=None):
        self._data = data_source
        self._data_loader = data_loader
        self._universe_mask: Optional[xr.DataArray] = None  # AlphaCanvasÍ∞Ä ÏÑ§Ï†ï
        self._cache: Dict[int, Tuple[str, xr.DataArray]] = {}
        self._step_counter = 0
    
    def visit_field(self, node) -> xr.DataArray:
        """Field ÎÖ∏Îìú Î∞©Î¨∏ with INPUT MASKING."""
        # ÌïÑÎìú Î°úÎìú (Ï∫êÏãú ÎòêÎäî DataLoader)
        if node.name in self._data:
            result = self._data[node.name]
        else:
            if self._data_loader is None:
                raise RuntimeError(f"Field '{node.name}' not found")
            result = self._data_loader.load_field(node.name)
            self._data = self._data.assign({node.name: result})
        
        # INPUT MASKING: ÌïÑÎìú Í≤ÄÏÉâ Ïãú Ïú†ÎãàÎ≤ÑÏä§ Ï†ÅÏö©
        if self._universe_mask is not None:
            result = result.where(self._universe_mask, np.nan)
        
        self._cache_result(f"Field_{node.name}", result)
        return result
    
    def visit_operator(self, node) -> xr.DataArray:
        """Ïó∞ÏÇ∞Ïûê Î∞©Î¨∏ with OUTPUT MASKING."""
        # 1. ÏàúÌöå: ÏûêÏãù ÌèâÍ∞Ä (Ïù¥ÎØ∏ ÎßàÏä§ÌÇπÎê®)
        child_result = node.child.accept(self)
        
        # 2. ÏúÑÏûÑ: Ïó∞ÏÇ∞ÏûêÏùò compute() Ìò∏Ï∂ú
        result = node.compute(child_result)
        
        # 3. OUTPUT MASKING: Ïó∞ÏÇ∞ Í≤∞Í≥ºÏóê Ïú†ÎãàÎ≤ÑÏä§ Ï†ÅÏö©
        if self._universe_mask is not None:
            result = result.where(self._universe_mask, np.nan)
        
        # 4. Ï∫êÏã±
        operator_name = node.__class__.__name__
        self._cache_result(operator_name, result)
        
        return result
```

**3. add_data()ÏóêÏÑú Ï£ºÏûÖ Îç∞Ïù¥ÌÑ∞ ÎßàÏä§ÌÇπ**:
```python
def add_data(self, name: str, data: Union[Expression, xr.DataArray]) -> None:
    """Îç∞Ïù¥ÌÑ∞ Ï∂îÍ∞Ä with Ïú†ÎãàÎ≤ÑÏä§ ÎßàÏä§ÌÇπ."""
    if isinstance(data, Expression):
        # Expression Í≤ΩÎ°ú - EvaluatorÍ∞Ä ÏûêÎèô ÎßàÏä§ÌÇπ
        self.rules[name] = data
        result = self._evaluator.evaluate(data)
        self._panel.add_data(name, result)
        
        # Evaluator Ïû¨ÎèôÍ∏∞Ìôî Ïãú Ïú†ÎãàÎ≤ÑÏä§ Î≥¥Ï°¥
        self._evaluator = EvaluateVisitor(self._panel.db, self._data_loader)
        if self._universe_mask is not None:
            self._evaluator._universe_mask = self._universe_mask
    
    elif isinstance(data, xr.DataArray):
        # DataArray ÏßÅÏ†ë Ï£ºÏûÖ - Ïó¨Í∏∞ÏÑú ÎßàÏä§ÌÇπ
        if self._universe_mask is not None:
            data = data.where(self._universe_mask, float('nan'))
        
        self._panel.add_data(name, data)
        self._evaluator = EvaluateVisitor(self._panel.db, self._data_loader)
        if self._universe_mask is not None:
            self._evaluator._universe_mask = self._universe_mask
```

**ÌïµÏã¨ ÏÇ¨Ìï≠**:
- **Î∂àÎ≥ÄÏÑ±**: Ïú†ÎãàÎ≤ÑÏä§Îäî Ï¥àÍ∏∞Ìôî Ïãú Ìïú Î≤àÎßå ÏÑ§Ï†ï, Î≥ÄÍ≤Ω Î∂àÍ∞Ä
- **Ïù¥Ï§ë ÎßàÏä§ÌÇπ**: Field ÏûÖÎ†• + Operator Ï∂úÎ†• Î™®Îëê ÎßàÏä§ÌÇπ (Ïã†Î¢∞ Ï≤¥Ïù∏)
- **Open Toolkit**: Ï£ºÏûÖÎêú DataArrayÎèÑ ÏûêÎèô ÎßàÏä§ÌÇπ
- **ÏÑ±Îä•**: 13.6% Ïò§Î≤ÑÌó§Îìú (xarray lazy evaluationÏúºÎ°ú Î¨¥Ïãú Í∞ÄÎä•)

---

#### D. `rc.data` Accessor Íµ¨ÌòÑ (Selector Interface) ‚úÖ **IMPLEMENTED**

**ÏÑ§Í≥Ñ Ï≤†Ìïô**: Expression Í∏∞Î∞ò ÌïÑÎìú Ï†ëÍ∑ºÏúºÎ°ú ÏßÄÏó∞ ÌèâÍ∞Ä Î∞è Ïú†ÎãàÎ≤ÑÏä§ ÏïàÏ†ÑÏÑ± Î≥¥Ïû•

```python
from alpha_canvas.core.expression import Field


class DataAccessor:
    """rc.data accessor that returns Field Expressions.
    
    This enables Expression-based data access:
        rc.data['field_name'] ‚Üí Field('field_name')
        rc.data['size'] == 'small' ‚Üí Equals(Field('size'), 'small')
    
    Field Expressions remain lazy until explicitly evaluated,
    ensuring universe masking through the Visitor pattern.
    """
    
    def __getitem__(self, field_name: str) -> Field:
        """Return Field Expression for the given field name.
        
        Args:
            field_name: Name of the field to access
            
        Returns:
            Field Expression wrapping the field name
            
        Raises:
            TypeError: If field_name is not a string
        """
        if not isinstance(field_name, str):
            raise TypeError(
                f"Field name must be string, got {type(field_name).__name__}"
            )
        
        return Field(field_name)
    
    def __getattr__(self, name: str):
        """Prevent attribute access - only item access allowed.
        
        This ensures a single, consistent access pattern.
        """
        raise AttributeError(
            f"DataAccessor does not support attribute access. "
            f"Use rc.data['{name}'] instead of rc.data.{name}"
        )
```

**AlphaCanvas ÌÜµÌï©**:

```python
class AlphaCanvas:
    def __init__(self, ...):
        # ... existing init ...
        self._data_accessor = DataAccessor()  # Create once, reuse
    
    @property
    def data(self) -> DataAccessor:
        """Access data fields as Field Expressions."""
        return self._data_accessor
```

**ÌïµÏã¨ ÏÇ¨Ìï≠:**

- ‚úÖ **Expression Î∞òÌôò**: `rc.data['size']` ‚Üí `Field('size')` (lazy)
- ‚úÖ **Lazy ÌèâÍ∞Ä**: Î™ÖÏãúÏ†Å `rc.evaluate()` Ìò∏Ï∂ú Ï†ÑÍπåÏßÄ ÌèâÍ∞Ä Ïïà Îê®
- ‚úÖ **Ïú†ÎãàÎ≤ÑÏä§ ÏïàÏ†Ñ**: Î™®Îì† ExpressionÏùÄ VisitorÎ•º ÌÜµÌï¥ ÌèâÍ∞ÄÎêòÏñ¥ Ïú†ÎãàÎ≤ÑÏä§ ÎßàÏä§ÌÇπ Î≥¥Ïû•
- ‚úÖ **Composable**: `ts_mean(rc.data['returns'], 10)` Í∞ôÏùÄ Ï≤¥Ïù¥Îãù Í∞ÄÎä•
- ‚úÖ **Item access only**: `rc.data['field']`Îßå ÏßÄÏõê, `rc.data.field`Îäî ÏóêÎü¨
- ‚úÖ **ÌÜµÌï©**: Phase 7A Boolean ExpressionÍ≥º ÏôÑÎ≤Ω ÌÜµÌï©

**ÏÇ¨Ïö© Ìå®ÌÑ¥**:

```python
# ‚úÖ Correct pattern (Expression-based)
mask = rc.data['size'] == 'small'  # Returns Equals Expression
result = rc.evaluate(mask)         # Evaluates with universe masking

# ‚úÖ Complex pattern
mask = (rc.data['size'] == 'small') & (rc.data['momentum'] == 'high')
result = rc.evaluate(mask)

# ‚ùå Wrong pattern (not supported)
mask = rc.data.size == 'small'  # AttributeError
```

### 3.2.3. Interface A: Formula-based (Excel-like)

```python
from alpha_canvas.ops import ts_mean, rank, group_neutralize, Field

# 1. Í∞ÑÎã®Ìïú Ìó¨Ìçº Ìï®Ïàò Ïä§ÌÉÄÏùº (Ï¶âÏãú ÌèâÍ∞Ä)
returns_10d = rc.ts_mean('returns', 10)  
# rc.db['returns_10d']Ïóê DataArray Ï†ÄÏû•

# 2. Î≥µÏû°Ìïú Expression Ï†ïÏùò (ÏßÄÏó∞ ÌèâÍ∞Ä)
alpha_expr = group_neutralize(
    rank(ts_mean(Field('returns'), 10)),
    group_by='subindustry'
)

# 3. ExpressionÏùÑ Î≥ÄÏàòÎ°ú Îì±Î°ù
rc.add_data_var('alpha1', alpha_expr)

# 4. Îç∞Ïù¥ÌÑ∞ Ï†ëÍ∑º (evaluated data)
alpha1_data = rc.db['alpha1']  # xarray.DataArray (T, N)
```

**Íµ¨ÌòÑ ÏöîÍµ¨ÏÇ¨Ìï≠:**

- `Field('returns')`: `ConfigLoader`ÏóêÏÑú `config/data.yaml`Ïùò `returns` Ï†ïÏùòÎ•º Ï∞∏Ï°∞ÌïòÎäî Leaf Expression
- `ts_mean()`, `rank()` Îì±: Composite Expression ÎÖ∏ÎìúÎ•º ÏÉùÏÑ±
- `rc.add_data_var()`: ExpressionÏùÑ `rc.rules`Ïóê Îì±Î°ùÌïòÍ≥†, `EvaluateVisitor`Î°ú ÌèâÍ∞ÄÌïòÏó¨ `rc.db`Ïóê Ï†ÄÏû•

### 3.2.3. Interface B: Selector-based (NumPy-like)

```python
# 1. ÏãúÍ∑∏ÎÑê Ï∫îÎ≤ÑÏä§ Ï¥àÍ∏∞Ìôî
rc.init_signal_canvas('my_alpha')  
# rc.db['my_alpha']Ïóê (T, N) ÏòÅÌñâÎ†¨ ÏÉùÏÑ±

# 2. Îç∞Ïù¥ÌÑ∞ Îì±Î°ù
rc.add_data('mcap', Field('market_cap'))
rc.add_data('ret', Field('returns'))
rc.add_data('vol', Field('volume'))

# 3. Î∂ÑÎ•ò Îç∞Ïù¥ÌÑ∞ Ï†ïÏùò - Î†àÏù¥Î∏î Í∏∞Î∞ò Î≤ÑÌÇ∑
rc.add_data('size', cs_quantile(rc.data['mcap'], bins=3, labels=['small', 'mid', 'big']))
rc.add_data('momentum', cs_quantile(rc.data['ret'], bins=2, labels=['low', 'high']))
rc.add_data('surge', ts_any(rc.data['ret'] > 0.3, window=252))  # Boolean

# 4. ÎπÑÍµê Ïó∞ÏÇ∞ÏúºÎ°ú Boolean ÎßàÏä§ÌÅ¨ ÏÉùÏÑ±
mask_long = (rc.data['size'] == 'small') & (rc.data['momentum'] == 'high') & (rc.data['surge'] == True)
mask_short = (rc.data['size'] == 'big') & (rc.data['momentum'] == 'low')

# 5. NumPy-style Ìï†Îãπ
rc['my_alpha'][mask_long] = 1.0
rc['my_alpha'][mask_short] = -1.0

# ÎòêÎäî ÌòÑÏû¨ ÌôúÏÑ± Ï∫îÎ≤ÑÏä§Ïóê ÏßÅÏ†ë Ìï†Îãπ
rc[mask_long] = 1.0

# 6. ÏµúÏ¢Ö ÏãúÍ∑∏ÎÑê Ï†ëÍ∑º (evaluated data)
my_alpha = rc.db['my_alpha']  # xarray.DataArray (T, N)
```

**Íµ¨ÌòÑ ÏöîÍµ¨ÏÇ¨Ìï≠:**

- `rc.add_data('size', expr)`: ExpressionÏùÑ ÌèâÍ∞ÄÌïòÍ≥† `rc.db.assign({'size': result})`Î°ú data_varsÏóê Ï∂îÍ∞Ä
- `rc.data['size'] == 'small'`:
  1. `rc.data['size']` ‚Üí `Field('size')` Expression Î∞òÌôò
  2. `Field('size') == 'small'` ‚Üí `Equals(Field('size'), 'small')` Expression Î∞òÌôò
  3. ExpressionÏùÄ lazyÌïòÍ≤å Ïú†ÏßÄ, `rc.evaluate(expr)`Î°ú ÌèâÍ∞Ä
- `rc[mask] = value`: `xr.where(mask, value, rc.db[current_canvas])`Î°ú Ìï†Îãπ (ÎØ∏Íµ¨ÌòÑ)

### 3.2.4. Interface C: Selective Traceability (Integer-Based Steps)

```python
# Î≥µÏû°Ìïú Expression Ï†ïÏùò
complex_alpha = group_neutralize(
    rank(ts_mean(Field('returns'), 5)),
    group_by='subindustry'
)

rc.add_data_var('complex_alpha', complex_alpha)

# Expression Ìä∏Î¶¨ Íµ¨Ï°∞ (depth-first ÏàúÏÑú):
# step 0: Field('returns')
# step 1: ts_mean(Field('returns'), 5)
# step 2: rank(...)
# step 3: group_neutralize(...)

# 1. ÌäπÏ†ï Îã®Í≥ÑÎßå Ï∂îÏ†Å
pnl_step1 = rc.trace_pnl('complex_alpha', step=1)
# {'sharpe': 0.7, 'total_pnl': 150, 'cumulative_returns': [...]}

# 2. Î™®Îì† Îã®Í≥Ñ Ï∂îÏ†Å
pnl_all = rc.trace_pnl('complex_alpha')  # step=None (default)
# {
#   0: {'step_name': 'Field_returns', 'sharpe': 0.5, ...},
#   1: {'step_name': 'ts_mean', 'sharpe': 0.7, ...},
#   2: {'step_name': 'rank', 'sharpe': 0.6, ...},
#   3: {'step_name': 'group_neutralize', 'sharpe': 0.8, ...}
# }

# 3. Ï§ëÍ∞Ñ Îç∞Ïù¥ÌÑ∞ ÏßÅÏ†ë Ï†ëÍ∑º
intermediate = rc.get_intermediate('complex_alpha', step=1)
# xarray.DataArray (T, N) - ts_mean Ï†ÅÏö© ÌõÑ Îç∞Ïù¥ÌÑ∞

# 4. Î≥µÏû°Ìïú Expression ÏòàÏãú (Î≥ëÎ†¨ Ïó∞ÏÇ∞)
combo_alpha = ts_mean(Field('returns'), 3) + rank(Field('market_cap'))
rc.add_data_var('combo', combo_alpha)

# step 0: Field('returns')
# step 1: ts_mean(Field('returns'), 3)
# step 2: Field('market_cap')
# step 3: rank(Field('market_cap'))
# step 4: add(step1, step3)

pnl_step4 = rc.trace_pnl('combo', step=4)  # ÏµúÏ¢Ö Í≤∞Í≥º
```

**Íµ¨ÌòÑ ÏöîÍµ¨ÏÇ¨Ìï≠:**

- `EvaluateVisitor.cache` Íµ¨Ï°∞: `dict[str, dict[int, tuple[str, xr.DataArray]]]`
  - Ïô∏Î∂Ä ÌÇ§: Î≥ÄÏàòÎ™Ö (e.g., `'complex_alpha'`)
  - ÎÇ¥Î∂Ä ÌÇ§: Ï†ïÏàò step Ïù∏Îç±Ïä§ (0Î∂ÄÌÑ∞ ÏãúÏûë)
  - Í∞í: `(ÎÖ∏Îìú_Ïù¥Î¶Ñ, DataArray)` ÌäúÌîå (ÎîîÎ≤ÑÍπÖÏö© Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ìè¨Ìï®)
- `EvaluateVisitor._step_counter`: ÌòÑÏû¨ step Ïù∏Îç±Ïä§Î•º Ï∂îÏ†ÅÌïòÎäî ÎÇ¥Î∂Ä Ïπ¥Ïö¥ÌÑ∞
- `EvaluateVisitor`Îäî Expression Ìä∏Î¶¨Î•º **ÍπäÏù¥ Ïö∞ÏÑ† ÌÉêÏÉâ(depth-first)** ÏúºÎ°ú ÏàúÌöåÌïòÎ©∞ Í∞Å ÎÖ∏ÎìúÏùò Î∞òÌôòÍ∞íÏùÑ Ï∫êÏãúÏóê Ï†ÄÏû•
- `rc.trace_pnl(var, step=None)`:
  - `step=None`: Î™®Îì† Îã®Í≥ÑÏùò Ï∫êÏãú Îç∞Ïù¥ÌÑ∞Î•º `PnLTracer`Ïóê Ï†ÑÎã¨
  - `step=1`: Ìï¥Îãπ Îã®Í≥ÑÎßå Ï†ÑÎã¨
- `rc.get_intermediate(var, step)`: `rc._evaluator.cache[var][step][1]` Î∞òÌôò (DataArray Î∂ÄÎ∂Ñ)

### 3.2.5. ÌïµÏã¨ ÌôúÏö© Ìå®ÌÑ¥: Ìå©ÌÑ∞ ÏàòÏùµÎ•† Í≥ÑÏÇ∞

#### A. ÎèÖÎ¶Ω Ïù¥Ï§ë Ï†ïÎ†¨ (Independent Double Sort) - Fama-French SMB

```python
from alpha_canvas.ops import cs_quantile, Field

# 1. Îç∞Ïù¥ÌÑ∞ Îì±Î°ù
rc.add_data('mcap', Field('market_cap'))
rc.add_data('btm', Field('book_to_market'))

# 2. ÎèÖÎ¶Ω Ï†ïÎ†¨: Ï†ÑÏ≤¥ Ïú†ÎãàÎ≤ÑÏä§ÏóêÏÑú Í∞ÅÍ∞Å quantile Í≥ÑÏÇ∞
rc.add_axis('size', cs_quantile(rc.data.mcap, bins=2, labels=['small', 'big']))
rc.add_axis('value', cs_quantile(rc.data.btm, bins=3, labels=['low', 'mid', 'high']))

# 3. SMB Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ Íµ¨ÏÑ±
rc.init_signal_canvas('smb')
rc[rc.axis.size['small']] = 1.0   # Long all small stocks
rc[rc.axis.size['big']] = -1.0    # Short all big stocks

# 4. Ìå©ÌÑ∞ ÏàòÏùµÎ•† Ï∂îÏ†Å
smb_returns = rc.trace_pnl('smb')
print(f"SMB Sharpe: {smb_returns['sharpe']:.2f}")
```

#### B. Ï¢ÖÏÜç Ïù¥Ï§ë Ï†ïÎ†¨ (Dependent Double Sort) - Fama-French HML

```python
# 1. Ï≤´ Î≤àÏß∏ Ï†ïÎ†¨: Size
rc.add_axis('size', cs_quantile(rc.data.mcap, bins=2, labels=['small', 'big']))

# 2. Ï¢ÖÏÜç Ï†ïÎ†¨: Í∞Å Size Í∑∏Î£π ÎÇ¥ÏóêÏÑú Value quantile Í≥ÑÏÇ∞
rc.add_axis('value', cs_quantile(rc.data.btm, bins=3, labels=['low', 'mid', 'high'],
                                   group_by='size'))
# group_by='size'Îäî rc.rules['size']Î•º Ï∞∏Ï°∞ÌïòÏó¨
# 'small' Í∑∏Î£πÍ≥º 'big' Í∑∏Î£π ÎÇ¥ÏóêÏÑú Í∞ÅÍ∞Å ÎèÖÎ¶ΩÏ†ÅÏúºÎ°ú quantileÏùÑ Í≥ÑÏÇ∞

# 3. HML Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ Íµ¨ÏÑ± (Í∞Å Size Í∑∏Î£π ÎÇ¥ÏóêÏÑú High-Low)
rc.init_signal_canvas('hml')
rc[rc.axis.value['high']] = 1.0   # Long high B/M (value) in both size groups
rc[rc.axis.value['low']] = -1.0   # Short low B/M (growth) in both size groups

# 4. Ìå©ÌÑ∞ ÏàòÏùµÎ•† Ï∂îÏ†Å
hml_returns = rc.trace_pnl('hml')
print(f"HML Sharpe: {hml_returns['sharpe']:.2f}")
```

#### C. Î°úÏö∞Î†àÎ≤® ÎßàÏä§ÌÅ¨ ÌôúÏö© (Advanced Custom Logic)

```python
# Ïú†ÎèôÏÑ± ÌïÑÌÑ∞ÎßÅÎêú Ïú†ÎãàÎ≤ÑÏä§ÏóêÏÑú Î™®Î©òÌÖÄ Ìå©ÌÑ∞ Íµ¨ÏÑ±
rc.add_data('volume', Field('volume'))
rc.add_data('returns', Field('returns'))

# 1. Í≥†Ïú†ÎèôÏÑ± ÎßàÏä§ÌÅ¨ ÏÉùÏÑ±
high_liquidity = rc.data.volume > rc.data.volume.quantile(0.5)

# 2. ÎßàÏä§ÌÅ¨ Ï†ÅÏö©Îêú quantile Í≥ÑÏÇ∞
rc.add_axis('momentum', cs_quantile(rc.data.returns, bins=5, labels=['q1','q2','q3','q4','q5'],
                                      mask=high_liquidity))
# mask=FalseÏù∏ Ï¢ÖÎ™©ÏùÄ NaNÏúºÎ°ú Ï≤òÎ¶¨Îê®

# 3. Î°±-Ïàè Ìè¨Ìä∏Ìè¥Î¶¨Ïò§
rc.init_signal_canvas('momentum_factor')
rc[rc.axis.momentum['q5']] = 1.0
rc[rc.axis.momentum['q1']] = -1.0

mom_returns = rc.trace_pnl('momentum_factor')
```

#### D. Îã§Ï∞®Ïõê Ìå©ÌÑ∞ Ï°∞Ìï© (Multi-Factor Strategy)

```python
# ÎèÖÎ¶Ω Ï†ïÎ†¨Î°ú 3Í∞ú Ìå©ÌÑ∞ Ï∂ï ÏÉùÏÑ±
rc.add_axis('size', cs_quantile(rc.data.mcap, bins=3, labels=['small','mid','big']))
rc.add_axis('momentum', cs_quantile(rc.data.mom, bins=5, labels=['q1','q2','q3','q4','q5']))
rc.add_axis('quality', cs_quantile(rc.data.roe, bins=3, labels=['low','mid','high']))

# Î≥µÏû°Ìïú Îã§Ï∞®Ïõê ÏÑ†ÌÉù
rc.init_signal_canvas('multi_factor')

# Small & High Momentum & High Quality
long_mask = (rc.axis.size['small'] & 
             rc.axis.momentum['q5'] & 
             rc.axis.quality['high'])

# Big & Low Momentum & Low Quality
short_mask = (rc.axis.size['big'] & 
              rc.axis.momentum['q1'] & 
              rc.axis.quality['low'])

rc[long_mask] = 1.0
rc[short_mask] = -1.0

multi_returns = rc.trace_pnl('multi_factor')
```

**ÏÑ§Í≥Ñ ÏùòÎèÑ:**

- Ïù¥ Ìå®ÌÑ¥Îì§Ïù¥ alpha-canvasÏùò **ÌïµÏã¨ ÌôúÏö© ÏÇ¨Î°Ä**ÏûÖÎãàÎã§.
- **Fama-French Ïû¨ÌòÑ**: `group_by`Î°ú Ï¢ÖÏÜç Ï†ïÎ†¨ÏùÑ Í∞ÑÍ≤∞ÌïòÍ≤å ÌëúÌòÑ
- **Ïú†Ïó∞ÏÑ±**: `mask`Î°ú Ïª§Ïä§ÌÖÄ Ïú†ÎãàÎ≤ÑÏä§ ÌïÑÌÑ∞ÎßÅ Í∞ÄÎä•
- ÎìÄÏñº Ïù∏ÌÑ∞ÌéòÏù¥Ïä§(Formula + Selector)Ïùò Ï°∞Ìï©ÏúºÎ°ú Î≥µÏû°Ìïú Îã§Ï∞®Ïõê Ìå©ÌÑ∞ Ìè¨Ìä∏Ìè¥Î¶¨Ïò§Î•º Í∞ÑÍ≤∞ÌïòÍ≤å ÌëúÌòÑÌï©ÎãàÎã§.
- Î†àÏù¥Î∏î Í∏∞Î∞ò ÏÑ†ÌÉù(`'small'`, `'q5'`, `'high'`)ÏúºÎ°ú Í∞ÄÎèÖÏÑ±Í≥º Ïú†ÏßÄÎ≥¥ÏàòÏÑ±ÏùÑ ÌôïÎ≥¥Ìï©ÎãàÎã§.

## 3.3. Ïó∞ÏÇ∞Ïûê Íµ¨ÌòÑ Ìå®ÌÑ¥ (Operator Implementation Pattern)

### 3.3.1. Ï±ÖÏûÑ Î∂ÑÎ¶¨ ÏõêÏπô

**ÌïµÏã¨ ÏõêÏπô:** Ïó∞ÏÇ∞ÏûêÎäî ÏûêÏã†Ïùò Í≥ÑÏÇ∞ Î°úÏßÅÏùÑ ÏÜåÏú†ÌïòÍ≥†, VisitorÎäî ÏàúÌöå Î∞è Ï∫êÏã±Îßå Îã¥ÎãπÌï©ÎãàÎã§.

**ÏûòÎ™ªÎêú Ìå®ÌÑ¥ (Anti-Pattern):**

```python
# ‚ùå BAD: VisitorÍ∞Ä Í≥ÑÏÇ∞ Î°úÏßÅÏùÑ Ìè¨Ìï®
class EvaluateVisitor:
    def visit_ts_mean(self, node):
        child_result = node.child.accept(self)
        # Visitor ÏïàÏóê rolling Í≥ÑÏÇ∞ Î°úÏßÅÏù¥ Îì§Ïñ¥Í∞ê (ÏûòÎ™ªÎê®!)
        result = child_result.rolling(time=node.window, min_periods=node.window).mean()
        self._cache_result("TsMean", result)
        return result
```

**Ïò¨Î∞îÎ•∏ Ìå®ÌÑ¥ (Correct Pattern):**

```python
# ‚úÖ GOOD: Ïó∞ÏÇ∞ÏûêÍ∞Ä Í≥ÑÏÇ∞ Î°úÏßÅÏùÑ ÏÜåÏú†
@dataclass
class TsMean(Expression):
    child: Expression
    window: int
    
    def accept(self, visitor):
        """Visitor Ïù∏ÌÑ∞ÌéòÏù¥Ïä§: ÏàúÌöåÎ•º ÏúÑÌïú ÏßÑÏûÖÏ†ê"""
        return visitor.visit_ts_mean(self)
    
    def compute(self, child_result: xr.DataArray) -> xr.DataArray:
        """ÌïµÏã¨ Í≥ÑÏÇ∞ Î°úÏßÅ: Ïó∞ÏÇ∞ÏûêÍ∞Ä ÏÜåÏú†"""
        return child_result.rolling(
            time=self.window,
            min_periods=self.window
        ).mean()

# ‚úÖ GOOD: VisitorÎäî ÏàúÌöå Î∞è Ï∫êÏã±Îßå Îã¥Îãπ
class EvaluateVisitor:
    def visit_ts_mean(self, node: TsMean) -> xr.DataArray:
        """Ìä∏Î¶¨ ÏàúÌöå Î∞è ÏÉÅÌÉú ÏàòÏßë"""
        # 1. ÏàúÌöå: ÏûêÏãù ÎÖ∏Îìú ÌèâÍ∞Ä
        child_result = node.child.accept(self)
        
        # 2. Í≥ÑÏÇ∞ ÏúÑÏûÑ: Ïó∞ÏÇ∞ÏûêÏóêÍ≤å Îß°ÍπÄ
        result = node.compute(child_result)
        
        # 3. ÏÉÅÌÉú ÏàòÏßë: Í≤∞Í≥º Ï∫êÏã±
        self._cache_result("TsMean", result)
        
        return result
```

### 3.3.2. Ïó∞ÏÇ∞Ïûê Íµ¨ÌòÑ Ï≤¥ÌÅ¨Î¶¨Ïä§Ìä∏

Î™®Îì† Ïó∞ÏÇ∞ÏûêÎäî Îã§Ïùå Íµ¨Ï°∞Î•º Îî∞ÎùºÏïº Ìï©ÎãàÎã§:

```python
@dataclass
class OperatorName(Expression):
    """Ïó∞ÏÇ∞Ïûê ÏÑ§Î™Ö.
    
    Args:
        child: ÏûêÏãù Expression (ÌïÑÏöîÏãú)
        param1: Ïó∞ÏÇ∞Ïûê ÌååÎùºÎØ∏ÌÑ∞ 1
        param2: Ïó∞ÏÇ∞Ïûê ÌååÎùºÎØ∏ÌÑ∞ 2
    
    Returns:
        Ïó∞ÏÇ∞ Í≤∞Í≥º DataArray
    """
    child: Expression  # ÏûêÏãù ÎÖ∏Îìú (ÏûàÎäî Í≤ΩÏö∞)
    param1: type1      # Ïó∞ÏÇ∞Ïûê ÌååÎùºÎØ∏ÌÑ∞Îì§
    param2: type2
    
    def accept(self, visitor) -> xr.DataArray:
        """Visitor Ïù∏ÌÑ∞ÌéòÏù¥Ïä§."""
        return visitor.visit_operator_name(self)
    
    def compute(self, *inputs: xr.DataArray) -> xr.DataArray:
        """ÌïµÏã¨ Í≥ÑÏÇ∞ Î°úÏßÅ.
        
        Args:
            *inputs: ÏûêÏãù ÎÖ∏ÎìúÎì§Ïùò ÌèâÍ∞Ä Í≤∞Í≥º
        
        Returns:
            Ïù¥ Ïó∞ÏÇ∞Ïùò Í≤∞Í≥º DataArray
        
        Note:
            Ïù¥ Î©îÏÑúÎìúÎäî ÏàúÏàò Ìï®ÏàòÏó¨Ïïº Ìï©ÎãàÎã§ (Î∂ÄÏûëÏö© ÏóÜÏùå).
            Visitor Ï∞∏Ï°∞ ÏóÜÏù¥ ÎèÖÎ¶ΩÏ†ÅÏúºÎ°ú ÌÖåÏä§Ìä∏ Í∞ÄÎä•Ìï¥Ïïº Ìï©ÎãàÎã§.
        """
        # Ïã§Ï†ú Í≥ÑÏÇ∞ Î°úÏßÅ
        result = ...  # xarray/numpy Ïó∞ÏÇ∞
        return result
```

**Ï≤¥ÌÅ¨Î¶¨Ïä§Ìä∏:**

- [ ] `accept()` Î©îÏÑúÎìú: Visitor Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ï†úÍ≥µ
- [ ] `compute()` Î©îÏÑúÎìú: ÌïµÏã¨ Í≥ÑÏÇ∞ Î°úÏßÅ Ï∫°ÏäêÌôî
- [ ] `compute()`Îäî ÏàúÏàò Ìï®Ïàò (Î∂ÄÏûëÏö© ÏóÜÏùå)
- [ ] `compute()`Îäî Visitor ÎèÖÎ¶ΩÏ†Å (ÏßÅÏ†ë ÌÖåÏä§Ìä∏ Í∞ÄÎä•)
- [ ] DocstringÏúºÎ°ú Args/Returns Î™ÖÌôïÌûà Î¨∏ÏÑúÌôî

### 3.3.3. Visitor Íµ¨ÌòÑ Ìå®ÌÑ¥

Î™®Îì† `visit_*()` Î©îÏÑúÎìúÎäî ÎèôÏùºÌïú 3Îã®Í≥Ñ Ìå®ÌÑ¥ÏùÑ Îî∞Î¶ÖÎãàÎã§:

```python
def visit_operator_name(self, node: OperatorName) -> xr.DataArray:
    """Ïó∞ÏÇ∞Ïûê ÎÖ∏Îìú Î∞©Î¨∏: ÏàúÌöå Î∞è Ï∫êÏã±.
    
    Args:
        node: Ïó∞ÏÇ∞Ïûê Expression ÎÖ∏Îìú
    
    Returns:
        Ïó∞ÏÇ∞ Í≤∞Í≥º DataArray
    """
    # 1Ô∏è‚É£ ÏàúÌöå(Traversal): ÏûêÏãù ÎÖ∏ÎìúÎì§ ÌèâÍ∞Ä
    child_result_1 = node.child1.accept(self)  # ÍπäÏù¥ Ïö∞ÏÑ†
    child_result_2 = node.child2.accept(self)  # (ÏûàÎäî Í≤ΩÏö∞)
    
    # 2Ô∏è‚É£ Í≥ÑÏÇ∞ ÏúÑÏûÑ(Delegation): Ïó∞ÏÇ∞ÏûêÏóêÍ≤å Îß°ÍπÄ
    result = node.compute(child_result_1, child_result_2)
    
    # 3Ô∏è‚É£ ÏÉÅÌÉú ÏàòÏßë(State Collection): Í≤∞Í≥º Ï∫êÏã±
    self._cache_result("OperatorName", result)
    
    return result
```

**VisitorÏùò Ïó≠Ìï†:**

- ‚úÖ **Ìä∏Î¶¨ ÏàúÌöå:** ÍπäÏù¥ Ïö∞ÏÑ†ÏúºÎ°ú ÏûêÏãù ÎÖ∏Îìú Î∞©Î¨∏
- ‚úÖ **Í≥ÑÏÇ∞ ÏúÑÏûÑ:** `node.compute()`Î°ú Í≥ÑÏÇ∞ Îß°ÍπÄ
- ‚úÖ **ÏÉÅÌÉú ÏàòÏßë:** Ï§ëÍ∞Ñ Í≤∞Í≥ºÎ•º Ï†ïÏàò Ïä§ÌÖùÏúºÎ°ú Ï∫êÏã±
- ‚ùå **Í≥ÑÏÇ∞ Î°úÏßÅ Ìè¨Ìï® Í∏àÏßÄ:** rolling, rank, quantile Îì±Ïùò Î°úÏßÅÏùÄ Ïó∞ÏÇ∞ÏûêÏóê ÏÜçÌï®

### 3.3.4. ÌÖåÏä§Ìä∏ Ï†ÑÎûµ

**1. Ïó∞ÏÇ∞Ïûê Îã®ÏúÑ ÌÖåÏä§Ìä∏ (Operator Unit Tests):**

```python
def test_ts_mean_compute_directly():
    """TsMean.compute() Î©îÏÑúÎìúÎ•º ÏßÅÏ†ë ÌÖåÏä§Ìä∏ (Visitor ÏóÜÏù¥)."""
    # ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ
    data = xr.DataArray(
        [[1, 2], [3, 4], [5, 6]],
        dims=['time', 'asset']
    )
    
    # Ïó∞ÏÇ∞Ïûê ÏÉùÏÑ±
    operator = TsMean(child=Field('dummy'), window=2)
    
    # compute() ÏßÅÏ†ë Ìò∏Ï∂ú (Visitor Ïö∞Ìöå)
    result = operator.compute(data)
    
    # Í≤ÄÏ¶ù
    assert np.isnan(result.values[0, 0])  # Ï≤´ Ìñâ NaN
    assert result.values[1, 0] == 2.0     # mean([1, 3])
```

**2. ÌÜµÌï© ÌÖåÏä§Ìä∏ (Integration Tests):**

```python
def test_ts_mean_with_visitor():
    """TsMeanÏù¥ VisitorÏôÄ ÌÜµÌï©ÎêòÏñ¥ ÏûëÎèôÌïòÎäîÏßÄ ÌÖåÏä§Ìä∏."""
    ds = xr.Dataset({'returns': data})
    visitor = EvaluateVisitor(ds)
    
    expr = TsMean(child=Field('returns'), window=3)
    result = visitor.evaluate(expr)
    
    # Ï∫êÏã± Í≤ÄÏ¶ù
    assert len(visitor._cache) == 2  # Field + TsMean
```

### 3.3.5. Ïù¥Ï†ê ÏöîÏïΩ

| Ï∏°Î©¥ | ÏûòÎ™ªÎêú Ìå®ÌÑ¥ | Ïò¨Î∞îÎ•∏ Ìå®ÌÑ¥ |
|------|-------------|-------------|
| **Ï±ÖÏûÑ** | VisitorÍ∞Ä Î™®Îì† Í≥ÑÏÇ∞ Îã¥Îãπ | Ïó∞ÏÇ∞ÏûêÍ∞Ä ÏûêÏã†Ïùò Í≥ÑÏÇ∞ ÏÜåÏú† |
| **ÌÖåÏä§Ìä∏** | VisitorÎ•º ÌÜµÌï¥ÏÑúÎßå ÌÖåÏä§Ìä∏ | `compute()` ÏßÅÏ†ë ÌÖåÏä§Ìä∏ Í∞ÄÎä• |
| **Ïú†ÏßÄÎ≥¥Ïàò** | VisitorÍ∞Ä ÎπÑÎåÄÌï¥Ïßê | Í∞Å Ïó∞ÏÇ∞Ïûê ÎèÖÎ¶ΩÏ†Å |
| **ÌôïÏû•ÏÑ±** | ÏÉà Ïó∞ÏÇ∞ÏûêÎßàÎã§ Visitor ÏàòÏ†ï | Visitor ÏàòÏ†ï ÏµúÏÜåÌôî |
| **Îã®Ïùº Ï±ÖÏûÑ** | VisitorÍ∞Ä Îã§Ï§ë Ï±ÖÏûÑ | Í∞Å ÌÅ¥ÎûòÏä§ Îã®Ïùº Ï±ÖÏûÑ |

---

## 3.4. Cross-Sectional Quantile Ïó∞ÏÇ∞Ïûê Íµ¨ÌòÑ ‚úÖ **IMPLEMENTED**

### 3.4.1. `CsQuantile` Expression ÌÅ¥ÎûòÏä§ (Ïã§Ï†ú Íµ¨ÌòÑ)

```python
from dataclasses import dataclass
from typing import List, Optional
import numpy as np
import pandas as pd
import xarray as xr

@dataclass(eq=False)  # eq=False to preserve Expression comparison operators
class CsQuantile(Expression):
    """Cross-sectional quantile bucketing - returns categorical labels.
    
    Preserves input (T, N) shape. Each timestep is independently bucketed.
    Supports both independent sort (whole universe) and dependent sort
    (within groups via group_by parameter).
    """
    child: Expression  # Î≤ÑÌÇ∑ÌôîÌï† Îç∞Ïù¥ÌÑ∞ (e.g., Field('market_cap'))
    bins: int  # Î≤ÑÌÇ∑ Í∞úÏàò
    labels: List[str]  # Î†àÏù¥Î∏î Î¶¨Ïä§Ìä∏ (Í∏∏Ïù¥ = bins)
    group_by: Optional[str] = None  # Ï¢ÖÏÜç Ï†ïÎ†¨Ïö©: field Ïù¥Î¶Ñ (string)
    
    def __post_init__(self):
        """Validate parameters."""
        if len(self.labels) != self.bins:
            raise ValueError(
                f"labels length ({len(self.labels)}) must equal bins ({self.bins})"
            )
    
    def accept(self, visitor):
        """Visitor Ïù∏ÌÑ∞ÌéòÏù¥Ïä§."""
        return visitor.visit_operator(self)
    
    def compute(
        self, 
        child_result: xr.DataArray, 
        group_labels: Optional[xr.DataArray] = None
    ) -> xr.DataArray:
        """Apply quantile bucketing - ÌïµÏã¨ Í≥ÑÏÇ∞ Î°úÏßÅ."""
        if group_labels is None:
            return self._quantile_independent(child_result)
        else:
            return self._quantile_grouped(child_result, group_labels)
```

### 3.4.2. ÎèÖÎ¶Ω Ï†ïÎ†¨ (Independent Sort) Íµ¨ÌòÑ

**ÌïµÏã¨ Ìå®ÌÑ¥:** `xarray.groupby('time').map()` + `pd.qcut` + **flatten-reshape**

```python
def _quantile_independent(self, data: xr.DataArray) -> xr.DataArray:
    """Independent sort - qcut at each timestep across all assets.
    
    ÌïµÏã¨: pd.qcutÏùÄ 1D ÏûÖÎ†•Ïù¥ ÌïÑÏöîÌïòÎØÄÎ°ú flatten ‚Üí qcut ‚Üí reshape Ìå®ÌÑ¥ ÏÇ¨Ïö©
    """
    def qcut_at_timestep(data_slice):
        """Apply pd.qcut to a single timestep's cross-section."""
        try:
            # CRITICAL: Flatten to 1D for pd.qcut
            values_1d = data_slice.values.flatten()
            result = pd.qcut(
                values_1d, 
                q=self.bins, 
                labels=self.labels, 
                duplicates='drop'  # Handle edge cases gracefully
            )
            # CRITICAL: Reshape back to original shape
            result_array = np.array(result).reshape(data_slice.shape)
            return xr.DataArray(
                result_array, 
                dims=data_slice.dims, 
                coords=data_slice.coords
            )
        except Exception:
            # Edge case: all same values, all NaN, etc.
            return xr.DataArray(
                np.full_like(data_slice.values, np.nan, dtype=object),
                dims=data_slice.dims, 
                coords=data_slice.coords
            )
    
    # xarray.groupby('time').map() automatically concatenates back to (T, N)
    result = data.groupby('time').map(qcut_at_timestep)
    return result
```

### 3.4.3. Ï¢ÖÏÜç Ï†ïÎ†¨ (Dependent Sort) Íµ¨ÌòÑ

**ÌïµÏã¨ Ìå®ÌÑ¥:** Ï§ëÏ≤©Îêú groupby (groups ‚Üí time ‚Üí qcut)

```python
def _quantile_grouped(
    self, 
    data: xr.DataArray, 
    groups: xr.DataArray
) -> xr.DataArray:
    """Dependent sort - qcut within each group at each timestep.
    
    Nested groupby pattern:
    1. Group by categorical labels (e.g., 'small', 'big')
    2. Within each group, apply independent sort (group by time ‚Üí qcut)
    3. xarray automatically concatenates results back to (T, N) shape
    """
    def apply_qcut_within_group(group_data: xr.DataArray) -> xr.DataArray:
        """Apply qcut at each timestep within this group."""
        return self._quantile_independent(group_data)
    
    # Nested groupby: groups ‚Üí time ‚Üí qcut
    # xarray automatically concatenates results back
    result = data.groupby(groups).map(apply_qcut_within_group)
    return result
```

### 3.4.4. Visitor ÌÜµÌï© (Special Case Handling)

**CsQuantileÏùÄ `visit_operator()`ÏóêÏÑú ÌäπÎ≥Ñ Ï≤òÎ¶¨ ÌïÑÏöî (group_by Ï°∞Ìöå)**:

```python
# In EvaluateVisitor.visit_operator()
from alpha_canvas.ops.classification import CsQuantile

# Special handling for CsQuantile (needs group_by lookup)
if isinstance(node, CsQuantile):
    # 1. Evaluate child
    child_result = node.child.accept(self)
    
    # 2. Look up group_by field if specified
    group_labels = None
    if node.group_by is not None:
        if node.group_by not in self._data:
            raise ValueError(
                f"group_by field '{node.group_by}' not found in dataset"
            )
        group_labels = self._data[node.group_by]
    
    # 3. Delegate to compute()
    result = node.compute(child_result, group_labels)
    
    # 4. Apply universe masking (automatic)
    if self._universe_mask is not None:
        result = result.where(self._universe_mask, np.nan)
    
    # 5. Cache
    self._cache_result("CsQuantile", result)
    return result
```

### 3.4.5. ÌïµÏã¨ Íµ¨ÌòÑ ÍµêÌõà (Ïã§ÌóòÏóêÏÑú Î∞úÍ≤¨)

**1. Flatten-Reshape Ìå®ÌÑ¥ ÌïÑÏàò:**
- `pd.qcut`ÏùÄ 1D Î∞∞Ïó¥Îßå Î∞õÏùå
- `data_slice.values.flatten()` ‚Üí qcut ‚Üí `reshape(data_slice.shape)`
- Ïù¥ Ìå®ÌÑ¥ ÏóÜÏù¥Îäî shape Î≥¥Ï°¥ Î∂àÍ∞ÄÎä•

**2. xarray.groupby().map() vs .apply():**
- `.map()`Ïù¥ xarray ‚Üí xarray Î≥ÄÌôòÏóê Îçî ÍπîÎÅî
- ÏûêÎèô concatenationÏúºÎ°ú shape Î≥¥Ï°¥
- `.apply()`ÎèÑ ÏûëÎèôÌïòÏßÄÎßå pandas Î∞òÌôò Ïãú ÏÇ¨Ïö©

**3. duplicates='drop' ÌïÑÏàò:**
- Î™®Îì† Í∞íÏù¥ ÎèôÏùºÌïú edge case Ï≤òÎ¶¨
- Î™®Îì† NaNÏù∏ Í≤ΩÏö∞ graceful degradation
- ÏóêÎü¨ Î∞úÏÉù ÎåÄÏã† NaN Î∞òÌôò

**4. Ï¢ÖÏÜç Ï†ïÎ†¨ ÏÑ±Îä•:**
- ÎèÖÎ¶Ω Ï†ïÎ†¨: ~27ms for (10, 6) data
- Ï¢ÖÏÜç Ï†ïÎ†¨: ~117ms for (10, 6) data (4.26x overhead)
- **ÌóàÏö© Í∞ÄÎä•:** Ìå©ÌÑ∞ Ïó∞Íµ¨Îäî Î∞∞Ïπò Ï≤òÎ¶¨ (Ïã§ÏãúÍ∞Ñ ÏïÑÎãò)

**5. Í≤ÄÏ¶ù Î∞©Î≤ï:**
- ÎèÖÎ¶Ω vs Ï¢ÖÏÜç Ï†ïÎ†¨Ïùò cutoffÍ∞Ä **Îã¨ÎùºÏïº Ìï®**
- Ïã§ÌóòÏóêÏÑú 17%Ïùò positionsÍ∞Ä Îã§Î•∏ label Î∞õÏùå
- Fama-French ÎÖºÎ¨∏ methodologyÏôÄ ÏùºÏπò

### 3.4.6. ÏÇ¨Ïö© ÏòàÏãú (Ïã§Ï†ú ÏΩîÎìú)

```python
from alpha_canvas.ops.classification import CsQuantile
from alpha_canvas.core.expression import Field

# ÎèÖÎ¶Ω Ï†ïÎ†¨: Ï†ÑÏ≤¥ Ïú†ÎãàÎ≤ÑÏä§ÏóêÏÑú quantile
size_expr = CsQuantile(
    child=Field('market_cap'),
    bins=2,
    labels=['small', 'big']
)

# Ï¢ÖÏÜç Ï†ïÎ†¨: size Í∑∏Î£π ÎÇ¥ÏóêÏÑú value quantile (Fama-French)
value_expr = CsQuantile(
    child=Field('book_to_market'),
    bins=3,
    labels=['low', 'mid', 'high'],
    group_by='size'  # 'size' fieldÎ•º Î®ºÏ†Ä Ï°∞Ìöå ‚Üí Í∞Å Í∑∏Î£πÎ≥Ñ quantile
)

# ÏÇ¨Ïö©
rc.add_data('size', size_expr)  # Î®ºÏ†Ä size ÏÉùÏÑ±
rc.add_data('value', value_expr)  # size Í∑∏Î£π ÎÇ¥ÏóêÏÑú value Í≥ÑÏÇ∞

# Boolean Expression ÌÜµÌï©
small_value = (rc.data['size'] == 'small') & (rc.data['value'] == 'high')
```

## 3.4. Property Accessor Íµ¨ÌòÑ ‚úÖ **IMPLEMENTED**

```python
from alpha_canvas.core.expression import Field


class DataAccessor:
    """Returns Field Expressions for lazy evaluation."""
    
    def __getitem__(self, field_name: str) -> Field:
        """Return Field Expression (not raw data!)"""
        if not isinstance(field_name, str):
            raise TypeError(
                f"Field name must be string, got {type(field_name).__name__}"
            )
        return Field(field_name)
    
    def __getattr__(self, name: str):
        """Prevent attribute access - item access only."""
        raise AttributeError(
            f"DataAccessor does not support attribute access. "
            f"Use rc.data['{name}'] instead of rc.data.{name}"
        )


# AlphaCanvas ÌÜµÌï©
class AlphaCanvas:
    def __init__(self, ...):
        self._data_accessor = DataAccessor()
    
    @property
    def data(self) -> DataAccessor:
        """Access data fields as Field Expressions."""
        return self._data_accessor
```

**ÏÇ¨Ïö© ÏòàÏãú**:

```python
# Basic field access
field = rc.data['size']  # Returns Field('size')

# Comparison creates Expression
mask = rc.data['size'] == 'small'  # Returns Equals Expression

# Evaluate
result = rc.evaluate(mask)  # Boolean DataArray with universe masking
```

---

## 3.4.2. Signal Assignment (Lazy Evaluation) ‚úÖ **IMPLEMENTED**

### Í∞úÏöî

**Signal Assignment**Îäî Expression Í∞ùÏ≤¥Ïóê Í∞íÏùÑ Ìï†ÎãπÌïòÏó¨ ÏãúÍ∑∏ÎÑêÏùÑ Íµ¨ÏÑ±ÌïòÎäî Í∏∞Îä•ÏûÖÎãàÎã§. Fama-French Ìå©ÌÑ∞ÏôÄ Í∞ôÏùÄ Î≥µÏû°Ìïú ÏãúÍ∑∏ÎÑêÏùÑ ÏßÅÍ¥ÄÏ†ÅÏù∏ Î¨∏Î≤ïÏúºÎ°ú ÏÉùÏÑ±Ìï† Ïàò ÏûàÏäµÎãàÎã§.

**ÌïµÏã¨ ÏÑ§Í≥Ñ ÏõêÏπô**:
- **Lazy Evaluation**: Ìï†ÎãπÏùÄ Ï†ÄÏû•Îßå ÌïòÍ≥† Ï¶âÏãú Ïã§ÌñâÌïòÏßÄ ÏïäÏùå
- **Implicit Canvas**: Î≥ÑÎèÑÏùò Ï∫îÎ≤ÑÏä§ ÏÉùÏÑ± ÏóÜÏù¥ Expression Í≤∞Í≥ºÍ∞Ä Ï∫îÎ≤ÑÏä§ Ïó≠Ìï†
- **Traceability**: Base resultÏôÄ final resultÎ•º Î≥ÑÎèÑÎ°ú Ï∫êÏã±ÌïòÏó¨ Ï∂îÏ†Å Í∞ÄÎä•
- **DRY Principle**: Lazy initializationÏúºÎ°ú Î™®Îì† ExpressionÏóêÏÑú ÏûêÎèô ÏûëÎèô

### Expression.__setitem__ Íµ¨ÌòÑ (DRY Lazy Initialization)

```python
class Expression(ABC):
    """Base class for all Expressions.
    
    Supports lazy assignment via __setitem__:
        signal[mask] = value  # Stores assignment, does not evaluate
    """
    
    def __setitem__(self, mask, value):
        """Store assignment for lazy evaluation.
        
        Uses lazy initialization - _assignments list is created on first use.
        This follows the DRY principle: no __post_init__ needed in subclasses.
        
        Args:
            mask: Boolean Expression or DataArray indicating where to assign
            value: Scalar value to assign where mask is True
        
        Note:
            Assignments are stored as (mask, value) tuples and applied sequentially
            during evaluation. Later assignments overwrite earlier ones for overlapping
            positions.
        """
        # Lazy initialization - create _assignments if it doesn't exist
        if not hasattr(self, '_assignments'):
            self._assignments = []
        
        self._assignments.append((mask, value))
```

**Lazy InitializationÏùò Ïû•Ï†ê**:
1. ‚úÖ **No Boilerplate**: Î™®Îì† Expression ÏÑúÎ∏åÌÅ¥ÎûòÏä§Ïóê `__post_init__` Î∂àÌïÑÏöî
2. ‚úÖ **DRY Principle**: Ï§ëÎ≥µ ÏΩîÎìú Ï†úÍ±∞
3. ‚úÖ **Automatic**: Î™®Îì† ExpressionÏóêÏÑú ÏûêÎèôÏúºÎ°ú ÏûëÎèô
4. ‚úÖ **Efficient**: Ìï†ÎãπÏù¥ ÏóÜÏúºÎ©¥ `_assignments` ÏÜçÏÑ±ÎèÑ ÏÉùÏÑ±ÎêòÏßÄ ÏïäÏùå

### Visitor Integration

```python
class EvaluateVisitor:
    def evaluate(self, expr: Expression) -> xr.DataArray:
        """Evaluate expression and apply assignments if present."""
        # Step 1: Evaluate base expression (tree traversal)
        base_result = expr.accept(self)
        
        # Step 2: Check if expression has assignments (lazy initialization)
        assignments = getattr(expr, '_assignments', None)
        if assignments:
            # Cache base result for traceability
            base_name = f"{expr.__class__.__name__}_base"
            self._cache[self._step_counter] = (base_name, base_result)
            self._step_counter += 1
            
            # Apply assignments sequentially
            final_result = self._apply_assignments(base_result, assignments)
            
            # Apply universe masking to final result
            if self._universe_mask is not None:
                final_result = final_result.where(self._universe_mask)
            
            # Cache final result
            final_name = f"{expr.__class__.__name__}_with_assignments"
            self._cache[self._step_counter] = (final_name, final_result)
            self._step_counter += 1
            
            return final_result
        
        # No assignments, return base result as-is
        return base_result
    
    def _apply_assignments(self, base_result: xr.DataArray, assignments: list) -> xr.DataArray:
        """Apply assignments sequentially to base result."""
        result = base_result.copy(deep=True)
        
        for mask_expr, value in assignments:
            # If mask is an Expression, evaluate it
            if hasattr(mask_expr, 'accept'):
                mask_data = mask_expr.accept(self)
            else:
                # Already a DataArray or numpy array
                mask_data = mask_expr
            
            # Ensure mask is boolean (required for ~ operator)
            mask_bool = mask_data.astype(bool)
            
            # Apply assignment: replace values where mask is True
            result = result.where(~mask_bool, value)
        
        return result
```

### Constant Expression (Blank Canvas)

```python
from dataclasses import dataclass
import numpy as np
import xarray as xr
from alpha_canvas.core.expression import Expression


@dataclass(eq=False)
class Constant(Expression):
    """Expression that produces a constant-valued DataArray.
    
    Creates a universe-shaped (T, N) DataArray filled with the specified
    constant value. Serves as a "blank canvas" for signal construction.
    
    Example:
        >>> signal = Constant(0.0)  # Blank canvas (all zeros)
        >>> signal[mask1] = 1.0     # Assign long positions
        >>> signal[mask2] = -1.0    # Assign short positions
    """
    value: float
    
    def accept(self, visitor):
        return visitor.visit_constant(self)
```

### ÏÇ¨Ïö© ÏòàÏãú: Fama-French 2√ó3 Factor

```python
# Step 1: Create size and value classifications
rc.add_data('size', CsQuantile(Field('market_cap'), bins=2, labels=['small', 'big']))
rc.add_data('value', CsQuantile(Field('book_to_market'), bins=3, labels=['low', 'medium', 'high']))

# Step 2: Create selector masks
is_small = rc.data['size'] == 'small'
is_big = rc.data['size'] == 'big'
is_low = rc.data['value'] == 'low'
is_high = rc.data['value'] == 'high'

# Step 3: Construct signal with lazy assignments
signal = Constant(0.0)                # Implicit blank canvas
signal[is_small & is_high] = 1.0      # Small/High-Value (long)
signal[is_big & is_low] = -1.0        # Big/Low-Value (short)

# Step 4: Evaluate (assignments applied here)
result = rc.evaluate(signal)

# Result: universe-shaped (T, N) array
#  - 1.0 where size=='small' AND value=='high'
#  - -1.0 where size=='big' AND value=='low'
#  - 0.0 elsewhere (neutral)
#  - NaN outside universe
```

### Overlapping Masks (Sequential Application)

```python
signal = Constant(0.0)
signal[is_small] = 0.5                # All small caps = 0.5
signal[is_small & is_high] = 1.0      # Small/High overwrites to 1.0

# Result: Later assignment wins for overlapping positions
#  - Small/High: 1.0 (overwritten)
#  - Small/Other: 0.5 (from first assignment)
#  - Others: 0.0 (from Constant)
```

### Traceability

```python
# After evaluation, check cached steps
for step_idx in sorted(rc._evaluator._cache.keys()):
    name, data = rc._evaluator._cache[step_idx]
    print(f"Step {step_idx}: {name}")

# Output:
#   Step 0: Constant_0.0_base             # Base constant array
#   Step 1: Field_size                     # Size classification
#   Step 2: Field_value                    # Value classification
#   Step 3: Equals                         # is_small mask
#   Step 4: Equals                         # is_high mask
#   Step 5: And                            # is_small & is_high
#   Step 6: Constant_0.0_with_assignments  # Final signal with assignments
```

**ÌïµÏã¨ Ïû•Ï†ê**:
- Base resultÏôÄ final resultÍ∞Ä Î≥ÑÎèÑ Îã®Í≥ÑÎ°ú Ï∫êÏã±Îê®
- PnL trackingÏóê ÌïÑÏàòÏ†ÅÏù∏ Í∏∞Îä•
- Í∞Å Ìï†ÎãπÏùò ÏòÅÌñ•ÏùÑ Îã®Í≥ÑÎ≥ÑÎ°ú Ï∂îÏ†Å Í∞ÄÎä•

### Implementation Checklist

- ‚úÖ `Expression.__setitem__` with lazy initialization (DRY)
- ‚úÖ `Visitor.evaluate()` handles assignments
- ‚úÖ `Visitor._apply_assignments()` sequential application
- ‚úÖ `Constant` Expression for blank canvas
- ‚úÖ `visit_constant()` in Visitor
- ‚úÖ Boolean mask conversion (`.astype(bool)`)
- ‚úÖ Universe masking integration
- ‚úÖ Traceability (separate base/final caching)
- ‚úÖ Tests: storage, evaluation, overlapping masks, caching
- ‚úÖ Showcase: Fama-French 2√ó3 factor construction

---

## 3.4.3. Portfolio Weight Scaling üìã **PLANNED**

### Í∞úÏöî

**Portfolio Weight Scaling**ÏùÄ ÏûÑÏùòÏùò ÏãúÍ∑∏ÎÑê Í∞íÏùÑ Ï†úÏïΩ Ï°∞Í±¥ÏùÑ ÎßåÏ°±ÌïòÎäî Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ Í∞ÄÏ§ëÏπòÎ°ú Î≥ÄÌôòÌïòÎäî Î™®ÎìàÏûÖÎãàÎã§. Strategy PatternÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Îã§ÏñëÌïú Ïä§ÏºÄÏùºÎßÅ Ï†ÑÎûµÏùÑ ÌîåÎü¨Í∑∏Ïù∏ Î∞©ÏãùÏúºÎ°ú ÏßÄÏõêÌï©ÎãàÎã§.

**ÌïµÏã¨ ÏÑ§Í≥Ñ ÏõêÏπô**:
- **Stateless**: Ïä§ÏºÄÏùºÎü¨Îäî ÏÉÅÌÉúÎ•º Ï†ÄÏû•ÌïòÏßÄ ÏïäÏùå (Ìï≠ÏÉÅ Î™ÖÏãúÏ†Å ÌååÎùºÎØ∏ÌÑ∞Î°ú Ï†ÑÎã¨)
- **Strategy Pattern**: Îã§ÏñëÌïú Ïä§ÏºÄÏùºÎßÅ Ï†ÑÎûµÏùÑ ÏâΩÍ≤å ÍµêÏ≤¥ Í∞ÄÎä•
- **Cross-Sectional**: Í∞Å ÏãúÏ†ê ÎèÖÎ¶ΩÏ†ÅÏúºÎ°ú Ï≤òÎ¶¨ (`groupby('time').map()` Ìå®ÌÑ¥)
- **NaN-Aware**: Ïú†ÎãàÎ≤ÑÏä§ ÎßàÏä§ÌÇπ ÏûêÎèô Î≥¥Ï°¥

### WeightScaler Î≤†Ïù¥Ïä§ ÌÅ¥ÎûòÏä§

```python
from abc import ABC, abstractmethod
import xarray as xr


class WeightScaler(ABC):
    """Abstract base class for weight scaling strategies.
    
    Converts arbitrary signal values to portfolio weights by applying
    constraints cross-sectionally (independently for each time period).
    
    Philosophy:
    - Operates on (T, N) signal DataArray
    - Returns (T, N) weight DataArray
    - Each time slice processed independently
    - NaN-aware (respects universe masking)
    - Strategy pattern: subclasses define scaling logic
    """
    
    @abstractmethod
    def scale(self, signal: xr.DataArray) -> xr.DataArray:
        """Scale signal to weights.
        
        Args:
            signal: (T, N) DataArray with arbitrary signal values
        
        Returns:
            (T, N) DataArray with portfolio weights
        
        Note:
            - Must handle NaN values (preserve them in output)
            - Must process each time slice independently
            - Should validate inputs (not all NaN, etc.)
        """
        pass
    
    def _validate_signal(self, signal: xr.DataArray):
        """Validate signal before scaling."""
        if signal.dims != ('time', 'asset'):
            raise ValueError(
                f"Signal must have dims ('time', 'asset'), got {signal.dims}"
            )
        
        # Check if any time slice has non-NaN values
        non_nan_counts = (~signal.isnull()).sum(dim='asset')
        if (non_nan_counts == 0).all():
            raise ValueError(
                "All signal values are NaN across all time periods"
            )
```

### GrossNetScaler (ÌÜµÌï© ÌîÑÎ†àÏûÑÏõåÌÅ¨)

```python
class GrossNetScaler(WeightScaler):
    """Unified weight scaler based on gross and net exposure targets.
    
    Uses the unified framework:
        L_target = (G + N) / 2
        S_target = (G - N) / 2
    
    Where:
        G = target_gross_exposure = sum(abs(weights))
        N = target_net_exposure = sum(weights)
        L = sum of positive weights
        S = sum of negative weights (negative value)
    
    Args:
        target_gross: Target gross exposure (default: 2.0 for 200% gross)
        target_net: Target net exposure (default: 0.0 for dollar-neutral)
    
    Example:
        >>> # Dollar neutral: L=1.0, S=-1.0
        >>> scaler = GrossNetScaler(target_gross=2.0, target_net=0.0)
        >>> 
        >>> # Net long 10%: L=1.1, S=-0.9
        >>> scaler = GrossNetScaler(target_gross=2.0, target_net=0.2)
        >>> 
        >>> # Crypto futures: L=0.5, S=-0.5
        >>> scaler = GrossNetScaler(target_gross=1.0, target_net=0.0)
    """
    
    def __init__(self, target_gross: float = 2.0, target_net: float = 0.0):
        self.target_gross = target_gross
        self.target_net = target_net
        
        # Validate constraints
        if target_gross < 0:
            raise ValueError("target_gross must be non-negative")
        if abs(target_net) > target_gross:
            raise ValueError(
                "Absolute net exposure cannot exceed gross exposure"
            )
        
        # Calculate target long and short books
        self.L_target = (target_gross + target_net) / 2.0
        self.S_target = (target_net - target_gross) / 2.0  # Negative value
    
    def scale(self, signal: xr.DataArray) -> xr.DataArray:
        """Scale signal using fully vectorized gross/net exposure constraints.
        
        Key innovation: NO ITERATION - pure vectorized operations.
        Always meets gross target, even for one-sided signals.
        """
        self._validate_signal(signal)
        
        # Step 1: Separate positive/negative (vectorized)
        s_pos = signal.where(signal > 0, 0.0)
        s_neg = signal.where(signal < 0, 0.0)
        
        # Step 2: Sum along asset dimension (vectorized)
        sum_pos = s_pos.sum(dim='asset', skipna=True)  # Shape: (time,)
        sum_neg = s_neg.sum(dim='asset', skipna=True)  # Shape: (time,)
        
        # Step 3: Normalize (vectorized, handles 0/0 ‚Üí nan ‚Üí 0)
        norm_pos = (s_pos / sum_pos).fillna(0.0)
        norm_neg_abs = (np.abs(s_neg) / np.abs(sum_neg)).fillna(0.0)
        
        # Step 4: Apply L/S targets (vectorized)
        weights_long = norm_pos * self.L_target
        weights_short_mag = norm_neg_abs * np.abs(self.S_target)
        
        # Step 5: Combine (subtract to make short side negative)
        weights = weights_long - weights_short_mag
        
        # Step 6: Calculate actual gross per row (vectorized)
        actual_gross = np.abs(weights).sum(dim='asset', skipna=True)  # Shape: (time,)
        
        # Step 7: Scale to meet target gross (vectorized)
        # Use xr.where to avoid inf from 0/0
        scale_factor = xr.where(actual_gross > 0, self.target_gross / actual_gross, 1.0)
        final_weights = weights * scale_factor
        
        # Step 8: Convert computational NaN to 0 (BEFORE universe mask)
        final_weights = final_weights.fillna(0.0)
        
        # Step 9: Apply universe mask (preserves NaN where signal was NaN)
        final_weights = final_weights.where(~signal.isnull())
        
        return final_weights
```

### Ìé∏Ïùò Scaler ÌÅ¥ÎûòÏä§Îì§

```python
class DollarNeutralScaler(GrossNetScaler):
    """Dollar neutral: sum(long) = 1.0, sum(short) = -1.0.
    
    Convenience wrapper for GrossNetScaler(2.0, 0.0).
    
    This is the most common scaler for market-neutral strategies.
    """
    def __init__(self):
        super().__init__(target_gross=2.0, target_net=0.0)


class LongOnlyScaler(WeightScaler):
    """Long-only portfolio: sum(weights) = target_long.
    
    Ignores negative signals, normalizes positive signals to sum to target.
    Uses vectorized operations for efficiency.
    
    Args:
        target_long: Target sum of weights (default: 1.0)
    
    Example:
        >>> scaler = LongOnlyScaler(target_long=1.0)
        >>> # All negative signals become 0, positives sum to 1.0
    """
    
    def __init__(self, target_long: float = 1.0):
        self.target_long = target_long
    
    def scale(self, signal: xr.DataArray) -> xr.DataArray:
        """Scale using vectorized long-only normalization."""
        self._validate_signal(signal)
        
        # Only keep positive values (vectorized)
        s_pos = signal.where(signal > 0, 0.0)
        
        # Sum along asset dimension (vectorized)
        sum_pos = s_pos.sum(dim='asset', skipna=True)  # Shape: (time,)
        
        # Normalize and scale (vectorized, handles 0/0 ‚Üí nan ‚Üí 0)
        weights = (s_pos / sum_pos * self.target_long).fillna(0.0)
        
        # Preserve NaN where signal was NaN (universe masking)
        return weights.where(~signal.isnull())
```

### Facade ÌÜµÌï©

```python
# In AlphaCanvas class (src/alpha_canvas/core/facade.py)

def scale_weights(
    self, 
    signal: Union[Expression, xr.DataArray], 
    scaler: 'WeightScaler'
) -> xr.DataArray:
    """Scale signal to portfolio weights.
    
    Args:
        signal: Expression or DataArray with signal values
        scaler: WeightScaler strategy instance (REQUIRED)
    
    Returns:
        (T, N) DataArray with portfolio weights
    
    Note:
        Scaler is a required parameter - no default.
        This is intentional for explicit, research-friendly API.
    
    Example:
        >>> from alpha_canvas.portfolio import DollarNeutralScaler
        >>> 
        >>> signal = ts_mean(Field('returns'), 5)
        >>> scaler = DollarNeutralScaler()
        >>> weights = rc.scale_weights(signal, scaler)
        >>> 
        >>> # Compare multiple scalers
        >>> w1 = rc.scale_weights(signal, DollarNeutralScaler())
        >>> w2 = rc.scale_weights(signal, LongOnlyScaler(1.0))
    """
    # Evaluate if Expression
    if hasattr(signal, 'accept'):
        signal_data = self.evaluate(signal)
    else:
        signal_data = signal
    
    # Apply scaling strategy
    weights = scaler.scale(signal_data)
    
    return weights
```

### ÏÇ¨Ïö© Ìå®ÌÑ¥ Î∞è ÏòàÏãú

**Ìå®ÌÑ¥ 1: ÏßÅÏ†ë ÏÇ¨Ïö© (Í∞ÄÏû• Î™ÖÏãúÏ†Å)**

```python
from alpha_canvas.portfolio import DollarNeutralScaler

# 1. Signal ÏÉùÏÑ±
signal_expr = ts_mean(Field('returns'), 5)
signal_data = rc.evaluate(signal_expr)

# 2. Scaler ÏÉùÏÑ± Î∞è Ï†ÅÏö©
scaler = DollarNeutralScaler()
weights = scaler.scale(signal_data)

# Í≤ÄÏ¶ù
assert abs(weights[weights > 0].sum() - 1.0) < 1e-6  # Long = 1.0
assert abs(weights[weights < 0].sum() + 1.0) < 1e-6  # Short = -1.0
```

**Ìå®ÌÑ¥ 2: Facade Ìé∏Ïùò Î©îÏÑúÎìú**

```python
from alpha_canvas.portfolio import GrossNetScaler

signal_expr = ts_mean(Field('returns'), 5)
scaler = GrossNetScaler(target_gross=2.0, target_net=0.2)

# evaluate + scale Ìïú Î≤àÏóê
weights = rc.scale_weights(signal_expr, scaler)
```

**Ìå®ÌÑ¥ 3: Ïó¨Îü¨ Ïä§ÏºÄÏùºÎü¨ ÎπÑÍµê (Ïó∞Íµ¨Ïö©)**

```python
from alpha_canvas.portfolio import (
    DollarNeutralScaler,
    GrossNetScaler,
    LongOnlyScaler
)

# ÎèôÏùº ÏãúÍ∑∏ÎÑêÏóê Ïó¨Îü¨ Ïä§ÏºÄÏùºÎßÅ Ï†ÑÎûµ Ï†ÅÏö©
signal = rc.evaluate(my_alpha_expr)

strategies = {
    'dollar_neutral': DollarNeutralScaler(),
    'net_long_10pct': GrossNetScaler(2.0, 0.2),
    'long_only': LongOnlyScaler(1.0),
    'crypto_futures': GrossNetScaler(1.0, 0.0)
}

weights_dict = {
    name: scaler.scale(signal)
    for name, scaler in strategies.items()
}

# Í∞Å Ï†ÑÎûµ ÎπÑÍµê
for name, weights in weights_dict.items():
    print(f"{name}:")
    print(f"  Gross: {abs(weights).sum()}")
    print(f"  Net: {weights.sum()}")
```

### Module Structure

```
src/alpha_canvas/portfolio/
‚îú‚îÄ‚îÄ __init__.py              # Export all scalers
‚îú‚îÄ‚îÄ base.py                  # WeightScaler abstract base class
‚îî‚îÄ‚îÄ strategies.py            # GrossNetScaler, DollarNeutralScaler, LongOnlyScaler
```

### ÌÖåÏä§Ìä∏ Ï†ÑÎûµ

**1. Îã®ÏúÑ ÌÖåÏä§Ìä∏** (`tests/test_portfolio/test_strategies.py`):
- `GrossNetScaler` ÏàòÌïô Í≤ÄÏ¶ù (L_target, S_target Í≥ÑÏÇ∞)
- Í∞Å scalerÏùò constraint Ï∂©Ï°± ÌôïÏù∏
- NaN Î≥¥Ï°¥ Í≤ÄÏ¶ù
- Edge cases: all positive, all negative, zeros

**2. ÌÜµÌï© ÌÖåÏä§Ìä∏**:
- AlphaCanvas.scale_weights() ÌÜµÌï©
- Expression ‚Üí evaluation ‚Üí scaling Ï†ÑÏ≤¥ ÌååÏù¥ÌîÑÎùºÏù∏
- Universe masking Î≥¥Ï°¥ Í≤ÄÏ¶ù

**3. ÏÑ±Îä• ÌÖåÏä§Ìä∏**:
- ÌÅ¨Î°úÏä§-ÏÑπÏÖò ÎèÖÎ¶ΩÏÑ± Í≤ÄÏ¶ù
- Large dataset (T=1000, N=3000) Î≤§ÏπòÎßàÌÅ¨

### Implementation Checklist

- [ ] `WeightScaler` abstract base class
- [ ] `GrossNetScaler` with unified framework (fully vectorized)
- [ ] `DollarNeutralScaler` convenience wrapper
- [ ] `LongOnlyScaler` implementation (fully vectorized)
- [ ] `AlphaCanvas.scale_weights()` facade method
- [ ] Unit tests for each scaler
- [ ] Integration tests with facade
- [x] **Experiment: weight scaling validation** (exp_18_weight_scaling.py - ALL PASS ‚úÖ)
- [ ] Showcase: Fama-French signal ‚Üí weights
- [x] **Documentation: FINDINGS.md updated** ‚úÖ
- [x] **Documentation: architecture.md updated** ‚úÖ
- [x] **Documentation: implementation.md updated** ‚úÖ

**Performance Validated**:
- Small (10√ó6): 7ms
- Medium (100√ó50): 7ms
- 1Y Daily (252√ó100): 8ms
- Large (1000√ó500): 34ms
- **10x-220x speedup** vs iterative approach

---

## 3.5. Í∞úÎ∞ú ÏõêÏπô

### 3.5.1. ÏßÄÏó∞ ÌèâÍ∞Ä (Lazy Evaluation)

- `Expression` Í∞ùÏ≤¥Îäî "Î†àÏãúÌîº"Ïù¥Î©∞ Îç∞Ïù¥ÌÑ∞Î•º Í∞ÄÏßÄÏßÄ ÏïäÏäµÎãàÎã§.
- `EvaluateVisitor`Í∞Ä Ïã§Ï†ú ÌèâÍ∞ÄÎ•º Îã¥ÎãπÌïòÎ©∞, Ïù¥Îïå Ï∫êÏã±Ïù¥ Î∞úÏÉùÌï©ÎãàÎã§.
- Î∂àÌïÑÏöîÌïú Ïû¨Í≥ÑÏÇ∞ÏùÑ Î∞©ÏßÄÌïòÏó¨ ÏÑ±Îä•ÏùÑ ÏµúÏ†ÅÌôîÌï©ÎãàÎã§.

### 3.5.2. Î†àÏù¥Î∏î Ïö∞ÏÑ† (Label-first)

- Î™®Îì† Î≤ÑÌÇ∑ Ïó∞ÏÇ∞ÏùÄ Ï†ïÏàò Ïù∏Îç±Ïä§ ÎåÄÏã† **ÏùòÎØ∏ ÏûàÎäî Î†àÏù¥Î∏î**ÏùÑ Î∞òÌôòÌï¥Ïïº Ìï©ÎãàÎã§.
- Ïòà: `cs_quantile(..., labels=['small', 'mid', 'big'])`
- Ïù¥Îäî PRDÏùò ÌïµÏã¨ Î¨∏Ï†ú 2Î•º Ìï¥Í≤∞ÌïòÎäî ÏÑ§Í≥Ñ ÏõêÏπôÏûÖÎãàÎã§.

### 3.5.3. Ï∂îÏ†ÅÏÑ± Ïö∞ÏÑ† (Traceability-first)

- Î™®Îì† Ï§ëÍ∞Ñ Í≥ÑÏÇ∞ Í≤∞Í≥ºÎäî **Ï†ïÏàò step Ïù∏Îç±Ïä§**Î°ú Ï∫êÏãúÎêòÏñ¥Ïïº Ìï©ÎãàÎã§.
- ÏÇ¨Ïö©ÏûêÎäî Ïû¨Í≥ÑÏÇ∞ ÏóÜÏù¥ Î™®Îì† Ï§ëÍ∞Ñ Îã®Í≥ÑÎ•º Í≤ÄÏÇ¨Ìï† Ïàò ÏûàÏñ¥Ïïº Ìï©ÎãàÎã§.
- Ïù¥Îäî PRDÏùò ÌïµÏã¨ Î¨∏Ï†ú 3ÏùÑ Ìï¥Í≤∞ÌïòÎäî ÏÑ§Í≥Ñ ÏõêÏπôÏûÖÎãàÎã§.

### 3.5.4. Pythonic Ïö∞ÏÑ† (Pythonic-first)

- Î¨∏ÏûêÏó¥ DSL ÎåÄÏã† PythonÏùò ÎÑ§Ïù¥Ìã∞Î∏å Î¨∏Î≤ïÏùÑ ÌôúÏö©Ìï©ÎãàÎã§.
- Ïòà: `&`, `|`, `[]`, `=` Ïó∞ÏÇ∞Ïûê Ïò§Î≤ÑÎ°úÎî©
- IDE ÏûêÎèôÏôÑÏÑ± Î∞è ÌÉÄÏûÖ ÌûåÌä∏Î•º ÏµúÎåÄÌïú ÌôúÏö©Ìï©ÎãàÎã§.

### 3.5.5. Ï¢ÖÏÜç Ï†ïÎ†¨ ÏßÄÏõê (Dependent Sort Support)

- `cs_quantile`ÏùÄ `group_by` ÌååÎùºÎØ∏ÌÑ∞Î°ú Ï¢ÖÏÜç Ï†ïÎ†¨ÏùÑ ÏßÄÏõêÌï¥Ïïº Ìï©ÎãàÎã§.
- Ïù¥Îäî Fama-French Ìå©ÌÑ∞ Ïû¨ÌòÑÏùÑ ÏúÑÌïú ÌïµÏã¨ ÏöîÍµ¨ÏÇ¨Ìï≠ÏûÖÎãàÎã§.
- `mask` ÌååÎùºÎØ∏ÌÑ∞Î°ú Î°úÏö∞Î†àÎ≤® Ïª§Ïä§ÌÑ∞ÎßàÏù¥ÏßïÎèÑ Í∞ÄÎä•Ìï¥Ïïº Ìï©ÎãàÎã§.

## 3.6. ÌÖåÏä§Ìä∏ Ï†ÑÎûµ

### 3.6.1. Îã®ÏúÑ ÌÖåÏä§Ìä∏

- `Expression` Í∞Å ÎÖ∏Îìú ÌÅ¥ÎûòÏä§
- `EvaluateVisitor` Î©îÏÑúÎìúÎ≥Ñ ÌÖåÏä§Ìä∏ (ÌäπÌûà `visit_cs_quantile`Ïùò `group_by` Î°úÏßÅ)
- `ConfigLoader` YAML ÌååÏã± ÌÖåÏä§Ìä∏
- Ï†ïÏàò step Ïù∏Îç±Ïã± Î°úÏßÅ

### 3.6.2. ÌÜµÌï© ÌÖåÏä§Ìä∏

- Ï†ÑÏ≤¥ ÏõåÌÅ¨ÌîåÎ°úÏö∞ (Ï¥àÍ∏∞Ìôî ‚Üí Îç∞Ïù¥ÌÑ∞ Î°úÎìú ‚Üí Expression ÌèâÍ∞Ä ‚Üí PnL Ï∂îÏ†Å)
- ÎìÄÏñº Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ Ï°∞Ìï© ÏãúÎÇòÎ¶¨Ïò§
- ÎèÖÎ¶Ω/Ï¢ÖÏÜç Ïù¥Ï§ë Ï†ïÎ†¨ Í∏∞Î∞ò Ìå©ÌÑ∞ ÏàòÏùµÎ•† Í≥ÑÏÇ∞ Ìå®ÌÑ¥
- Fama-French SMB, HML Ìå©ÌÑ∞ Ïû¨ÌòÑ Í≤ÄÏ¶ù

### 3.6.3. ÏÑ±Îä• ÌÖåÏä§Ìä∏

- ÎåÄÏö©Îüâ Îç∞Ïù¥ÌÑ∞ (T=1000, N=3000) Ï≤òÎ¶¨ ÏãúÍ∞Ñ
- Ï∫êÏã± Ìö®Í≥º Í≤ÄÏ¶ù (stepÎ≥Ñ Ï°∞Ìöå ÏÑ±Îä•)
- Ï¢ÖÏÜç Ï†ïÎ†¨Ïùò Ïò§Î≤ÑÌó§Îìú Ï∏°Ï†ï
- Î©îÎ™®Î¶¨ ÏÇ¨Ïö©Îüâ ÌîÑÎ°úÌååÏùºÎßÅ

## 3.7. ÏΩîÎî© Ïª®Î≤§ÏÖò

- **ÌÉÄÏûÖ ÌûåÌä∏:** Î™®Îì† public Î©îÏÑúÎìúÏóê ÌÉÄÏûÖ ÌûåÌä∏ ÌïÑÏàò
- **Docstring:** Google Ïä§ÌÉÄÏùº docstring ÏÇ¨Ïö©
- **Linting:** `ruff` ÏÇ¨Ïö©
- **Formatting:** `black` ÏÇ¨Ïö©
- **Import ÏàúÏÑú:** ÌëúÏ§Ä ÎùºÏù¥Î∏åÎü¨Î¶¨ ‚Üí ÏÑúÎìúÌååÌã∞ ‚Üí Î°úÏª¨

## 3.8. Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò Í∞ÄÏù¥Îìú

### 3.8.1. Step Ïù∏Îç±Ïã± Î≥ÄÍ≤ΩÏÇ¨Ìï≠

**Ïù¥Ï†Ñ (Î¨∏ÏûêÏó¥ Í∏∞Î∞ò):**

```python
# ‚ùå ÏÇ¨Ïö©ÌïòÏßÄ ÎßàÏÑ∏Ïöî
rc.trace_pnl('alpha1', step='ts_mean')
rc.get_intermediate('alpha1', step='ts_mean')
```

**ÌòÑÏû¨ (Ï†ïÏàò Í∏∞Î∞ò):**

```python
# ‚úÖ Ïò¨Î∞îÎ•∏ ÏÇ¨Ïö©Î≤ï
rc.trace_pnl('alpha1', step=1)  # step 1ÍπåÏßÄ Ï∂îÏ†Å
rc.get_intermediate('alpha1', step=1)  # step 1 Îç∞Ïù¥ÌÑ∞ Ï°∞Ìöå

# Î™®Îì† Îã®Í≥Ñ Ï∂îÏ†Å
rc.trace_pnl('alpha1')  # step=None (Í∏∞Î≥∏Í∞í)
# Î∞òÌôò: {0: {...}, 1: {...}, 2: {...}}
```

### 3.8.2. ÎèÖÎ¶Ω/Ï¢ÖÏÜç Ï†ïÎ†¨ Ìå®ÌÑ¥

**ÎèÖÎ¶Ω Ï†ïÎ†¨ (Î≥ÄÍ≤Ω ÏóÜÏùå):**

```python
# ‚úÖ Í∏∞Ï°¥Í≥º ÎèôÏùºÌïòÍ≤å ÏûëÎèô
rc.add_axis('size', cs_quantile(rc.data.mcap, bins=2, labels=['small','big']))
```

**Ï¢ÖÏÜç Ï†ïÎ†¨ (Ïã†Í∑ú Í∏∞Îä•):**

```python
# ‚úÖ ÏÉàÎ°úÏö¥ Í∏∞Îä•
rc.add_axis('value', cs_quantile(rc.data.btm, bins=3, labels=['low','mid','high'],
                                   group_by='size'))
```

**ÎßàÏä§ÌÅ¨ Í∏∞Î∞ò ÌïÑÌÑ∞ÎßÅ (Ïã†Í∑ú Í∏∞Îä•):**

```python
# ‚úÖ ÏÉàÎ°úÏö¥ Í∏∞Îä•
mask = rc.data.volume > threshold
rc.add_axis('filtered', cs_quantile(rc.data.returns, bins=5, labels=[...],
                                      mask=mask))
```

## 3.9. Íµ¨ÌòÑ ÏÑ±Í≥µ Í∏∞Ï§Ä

### 3.9.1. Step Ïù∏Îç±Ïã± Í≤ÄÏ¶ù

‚úÖ **ÌïÑÏàò ÎèôÏûë:**

- `rc.trace_pnl('alpha', step=2)` ‚Üí step 2ÍπåÏßÄÏùò PnL Î∞òÌôò
- `rc.get_intermediate('alpha', step=2)` ‚Üí step 2Ïùò Ï∫êÏãúÎêú DataArray Î∞òÌôò
- Î≥ëÎ†¨ Expression (Î∏åÎûúÏπòÍ∞Ä ÏûàÎäî Ìä∏Î¶¨)ÏóêÏÑú Ïò¨Î∞îÎ•∏ ÏàúÏÑúÎ°ú Ïù∏Îç±Ïã±
- ÏûòÎ™ªÎêú step Ïù∏Îç±Ïä§ ÏûÖÎ†• Ïãú Î™ÖÌôïÌïú ÏóêÎü¨ Î©îÏãúÏßÄ

### 3.9.2. Ï¢ÖÏÜç Ï†ïÎ†¨ Í≤ÄÏ¶ù

‚úÖ **ÌïÑÏàò ÎèôÏûë:**

- **ÎèÖÎ¶Ω Ï†ïÎ†¨**: `cs_quantile(...)` ‚Üí Ï†ÑÏ≤¥ Ïú†ÎãàÎ≤ÑÏä§ ÎåÄÏÉÅ quantile
- **Ï¢ÖÏÜç Ï†ïÎ†¨**: `cs_quantile(..., group_by='axis')` ‚Üí Í∞Å Í∑∏Î£π ÎÇ¥ quantile
- **ÎßàÏä§ÌÅ¨ ÌïÑÌÑ∞ÎßÅ**: `cs_quantile(..., mask=...)` ‚Üí ÌïÑÌÑ∞ÎßÅÎêú Î∂ÄÎ∂ÑÏßëÌï© ÎåÄÏÉÅ quantile
- ÎèÖÎ¶Ω/Ï¢ÖÏÜç Ï†ïÎ†¨Ïùò Í≤∞Í≥º cutoffÍ∞Ä Î™ÖÌôïÌûà Îã§Î¶Ñ (Í≤ÄÏ¶ù ÌÖåÏä§Ìä∏ ÌïÑÏöî)

### 3.9.3. Fama-French Ïû¨ÌòÑ Í≤ÄÏ¶ù

‚úÖ **ÌïÑÏàò ÎèôÏûë:**

- SMB (ÎèÖÎ¶Ω 2√ó3 Ï†ïÎ†¨) ‚Üí ÏòàÏÉÅÎêú Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ Í∞ÄÏ§ëÏπò ÏÉùÏÑ±
- HML (Ï¢ÖÏÜç 2√ó3 Ï†ïÎ†¨) ‚Üí ÏòàÏÉÅÎêú Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ Í∞ÄÏ§ëÏπò ÏÉùÏÑ±
- ÎèÖÎ¶Ω/Ï¢ÖÏÜç Î∞©ÏãùÏùò cutoff Ï∞®Ïù¥ Í≤ÄÏ¶ù (academic paper Í∏∞Ï§ÄÍ≥º ÏùºÏπò)

## 3.10. Îã§Ïùå Îã®Í≥Ñ

### Phase 1: ÌïµÏã¨ Ïª¥Ìè¨ÎÑåÌä∏ Íµ¨ÌòÑ

- [ ] `Expression` Ï∂îÏÉÅ ÌÅ¥ÎûòÏä§ Î∞è Leaf/Composite Íµ¨ÌòÑ
- [ ] `EvaluateVisitor` Í∏∞Î≥∏ Íµ¨Ï°∞ Î∞è Ï∫êÏã± Î©îÏª§ÎãàÏ¶ò (Ï†ïÏàò step Ïπ¥Ïö¥ÌÑ∞ Ìè¨Ìï®)
- [ ] `ConfigLoader` Î∞è YAML ÌååÏã±
- [ ] `AlphaCanvas` Facade Í∏∞Î≥∏ Íµ¨Ï°∞

### Phase 2: Ïó∞ÏÇ∞Ïûê Íµ¨ÌòÑ

- [ ] Timeseries Ïó∞ÏÇ∞Ïûê (`ts_mean`, `ts_sum`, etc.)
- [ ] Cross-sectional Ïó∞ÏÇ∞Ïûê (`cs_rank`, `cs_quantile` with `group_by` and `mask`)
- [ ] Transform Ïó∞ÏÇ∞Ïûê (`group_neutralize`, etc.)

### Phase 3: Ï∂îÏ†ÅÏÑ± Î∞è Î∂ÑÏÑù

- [ ] `PnLTracer` Íµ¨ÌòÑ
- [ ] ÏÑ†ÌÉùÏ†Å Îã®Í≥Ñ Ï∂îÏ†Å Î°úÏßÅ (Ï†ïÏàò Ïù∏Îç±Ïä§ Í∏∞Î∞ò)
- [ ] ÏÑ±Í≥º ÏßÄÌëú Í≥ÑÏÇ∞
- [ ] PnL Î¶¨Ìè¨Ìä∏Ïóê step Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÌëúÏãú

### Phase 4: Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÏôÑÏÑ±

- [ ] Property accessor (`rc.data`, `rc.axis`)
- [ ] NumPy-style Ìï†Îãπ (`rc[mask] = value`)
- [ ] Ìó¨Ìçº Î©îÏÑúÎìú (`rc.ts_mean()` Îì±)

### Phase 5: Í≤ÄÏ¶ù Î∞è ÌÖåÏä§Ìä∏

- [ ] Ï†ïÏàò step Ïù∏Îç±Ïã± Îã®ÏúÑ ÌÖåÏä§Ìä∏
- [ ] `_quantile_grouped` Î°úÏßÅ Îã®ÏúÑ ÌÖåÏä§Ìä∏
- [ ] Fama-French SMB/HML ÌÜµÌï© ÌÖåÏä§Ìä∏
- [ ] Ï¢ÖÏÜç Ï†ïÎ†¨ ÏÑ±Îä• Î≤§ÏπòÎßàÌÅ¨

---

**Ï∞∏Í≥†:** Ïù¥ Î¨∏ÏÑúÎäî Ïã§Ï†ú Íµ¨ÌòÑ Í≥ºÏ†ïÏóêÏÑú Î∞úÍ≤¨ÎêòÎäî ÏÉàÎ°úÏö¥ Ìå®ÌÑ¥Í≥º ÍµêÌõàÏùÑ ÏßÄÏÜçÏ†ÅÏúºÎ°ú Î∞òÏòÅÌï¥Ïïº Ìï©ÎãàÎã§ (Living Document).
