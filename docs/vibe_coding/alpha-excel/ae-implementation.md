# 3. Implementation Guide

ì´ ë¬¸ì„œëŠ” alpha-excelì˜ ì‹¤ì œ êµ¬í˜„ ì„¸ë¶€ì‚¬í•­, ì½”ë“œ íŒ¨í„´, ì‚¬ìš© ì˜ˆì‹œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.

## 3.0. êµ¬í˜„ ìƒíƒœ ìš”ì•½ (Implementation Status Summary)

**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-10-28

### âœ… **í•µì‹¬ ê¸°ëŠ¥ êµ¬í˜„ ì™„ë£Œ**

| Feature | Status | Location | Description |
|---------|--------|----------|-------------|
| **F1: Auto Data Loading** | âœ… DONE | `core/field_loader.py` | Field auto-loading via FieldLoader |
| **F2: Expression-Only API** | âœ… DONE | `core/facade.py` | Direct evaluate() without add_data() |
| **F3: Triple-Cache** | âœ… DONE | `core/step_tracker.py` | Signal, weight, port_return caching |
| **F4: Portfolio (Weights)** | âœ… DONE | `portfolio/` | Strategy pattern weight scalers |
| **F5: Backtesting** | âœ… DONE | `core/backtest_engine.py` | Shift-mask workflow, position returns |
| **F6: Serialization** | âœ… DONE | `core/serialization.py` | Expression to/from dict, dependencies |
| **Universe Masking** | âœ… DONE | `core/universe_mask.py` | Centralized masking (double masking) |
| **SRP Refactoring** | âœ… DONE | `core/visitor.py` + 4 components | Visitor focuses on tree traversal only |
| **Arithmetic Operators** | âœ… DONE | `ops/arithmetic.py` | Add, Subtract, Multiply, Divide, Negate, Abs |
| **Logical Operators** | âœ… DONE | `ops/logical.py` | Comparisons, And, Or, Not, IsNan |
| **Time-Series Operators** | âœ… CORE DONE | `ops/timeseries.py` | Rolling, shift, stats (15 ops implemented) |

### âœ… **ê²€ì¦ ì™„ë£Œ**

* âœ… **í†µí•© í…ŒìŠ¤íŠ¸ í†µê³¼** (`test_alpha_excel.py`)
* âœ… **Showcase ë™ìž‘** (`showcase_alpha_excel.py`, `showcase_alpha_excel_comprehensive.py`)
* âœ… **Weight caching showcase** (`showcase_weight_caching.py`)
* âœ… **ì™„ì „ ë²¡í„°í™”** (vectorized pandas operations)
* âœ… **Triple-cache ê²€ì¦** (signal, weight, port_return)

### ðŸ”œ **í–¥í›„ êµ¬í˜„ ì˜ˆì •**

| ê¸°ëŠ¥ | ìš°ì„ ìˆœìœ„ | ì„¤ëª… |
|------|---------|------|
| **Label Quantile** | HIGH | Fama-French Factor ìŠ¤íƒ€ì¼ ê·¸ë£¹í•‘ |
| **String Universe** | MEDIUM | 'univ100', 'univ200' ë“± ì‚¬ì „ ì •ì˜ ìœ ë‹ˆë²„ìŠ¤ |
| **Group Operations** | MEDIUM | GroupNeutralize, GroupDemean (industry ë“±) |

---

## 3.1. í”„ë¡œì íŠ¸ êµ¬ì¡°

```text
alpha-canvas/
â”œâ”€â”€ config/                      # ì„¤ì • íŒŒì¼ (alpha-database ì—°ë™)
â”‚   â””â”€â”€ data.yaml               # ë°ì´í„° ì†ŒìŠ¤ ì •ì˜
â”œâ”€â”€ src/
â”‚   â””â”€â”€ alpha_excel/
â”‚       â”œâ”€â”€ __init__.py         # Public API exports
â”‚       â”œâ”€â”€ core/
â”‚       â”‚   â”œâ”€â”€ facade.py       # AlphaExcel (rc) í´ëž˜ìŠ¤ êµ¬í˜„
â”‚       â”‚   â”œâ”€â”€ expression.py   # Expression ê¸°ë³¸ ì¸í„°íŽ˜ì´ìŠ¤
â”‚       â”‚   â”œâ”€â”€ visitor.py      # EvaluateVisitor (SRP ì ìš©, íŠ¸ë¦¬ ìˆœíšŒ ì „ë‹´)
â”‚       â”‚   â”œâ”€â”€ data_model.py   # DataContext (dict-like pandas storage)
â”‚       â”‚   â”œâ”€â”€ serialization.py # Serialization/Deserialization visitors
â”‚       â”‚   â”œâ”€â”€ universe_mask.py # [NEW] UniverseMask (ë§ˆìŠ¤í‚¹ ë¡œì§)
â”‚       â”‚   â”œâ”€â”€ step_tracker.py  # [NEW] StepTracker (triple-cache ê´€ë¦¬)
â”‚       â”‚   â”œâ”€â”€ field_loader.py  # [NEW] FieldLoader (ë°ì´í„° ë¡œë”©/ë³€í™˜)
â”‚       â”‚   â””â”€â”€ backtest_engine.py # [NEW] BacktestEngine (ë°±í…ŒìŠ¤íŠ¸ ê³„ì‚°)
â”‚       â”œâ”€â”€ ops/                # ì—°ì‚°ìž
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ timeseries.py   # TsMean, TsStd, etc.
â”‚       â”‚   â”œâ”€â”€ crosssection.py # Rank, Demean, etc.
â”‚       â”‚   â”œâ”€â”€ group.py        # GroupNeutralize, GroupRank, etc.
â”‚       â”‚   â”œâ”€â”€ arithmetic.py   # Add, Subtract, Multiply, Divide, etc.
â”‚       â”‚   â”œâ”€â”€ logical.py      # And, Or, Not, Equals, etc.
â”‚       â”‚   â””â”€â”€ constants.py    # Constant value operator
â”‚       â””â”€â”€ portfolio/          # í¬íŠ¸í´ë¦¬ì˜¤ ì „ëžµ
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ base.py         # WeightScaler ì¶”ìƒ ë² ì´ìŠ¤ í´ëž˜ìŠ¤
â”‚           â””â”€â”€ strategies.py   # GrossNetScaler, DollarNeutralScaler, LongOnlyScaler
â”œâ”€â”€ showcase/                   # Showcase ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ tests/                      # í…ŒìŠ¤íŠ¸
â””â”€â”€ docs/
    â””â”€â”€ vibe_coding/
        â””â”€â”€ alpha-excel/
            â”œâ”€â”€ ae-prd.md
            â”œâ”€â”€ ae-architecture.md
            â””â”€â”€ ae-implementation.md   # ì´ ë¬¸ì„œ
```

**í•µì‹¬ ì„¤ê³„ ì›ì¹™ (SRP ì ìš©):**
* `core/expression.py`, `ops/*`: Expression ì¸í„°íŽ˜ì´ìŠ¤ì™€ ì—°ì‚°ìž ì •ì˜
* `core/facade.py`: AlphaExcel facade (ë‹¨ì¼ ì§„ìž…ì )
* `core/visitor.py`: Expression íŠ¸ë¦¬ ìˆœíšŒ ì „ë‹´ (SRP)
* **`core/universe_mask.py`**: Universe ë§ˆìŠ¤í‚¹ ë¡œì§ ì¤‘ì•™í™”
* **`core/step_tracker.py`**: Triple-cache ê´€ë¦¬ ì „ë‹´
* **`core/field_loader.py`**: ë°ì´í„° ë¡œë”© ë° ë³€í™˜ ì „ë‹´
* **`core/backtest_engine.py`**: ë°±í…ŒìŠ¤íŠ¸ ê³„ì‚° ì „ë‹´
* `core/serialization.py`: Expression ì§ë ¬í™”/ì—­ì§ë ¬í™”
* `portfolio/*`: Strategy Pattern ê¸°ë°˜ ê°€ì¤‘ì¹˜ ìŠ¤ì¼€ì¼ë§

---

## 3.2. í•µì‹¬ ì»´í¬ë„ŒíŠ¸ êµ¬í˜„ ìƒì„¸

### 3.2.1. AlphaExcel ì´ˆê¸°í™”

```python
from alpha_database import DataSource
from alpha_excel import AlphaExcel

# DataSource ì´ˆê¸°í™” (alpha_database ì‚¬ìš©)
ds = DataSource('config')

# AlphaExcel ì´ˆê¸°í™” (returns ìžë™ ë¡œë”©)
rc = AlphaExcel(
    data_source=ds,
    start_date='2024-01-01',
    end_date='2024-12-31'
)

# ì»¤ìŠ¤í…€ universe ì§€ì •
universe_mask = (price > 5.0) & (volume > 100000)
rc = AlphaExcel(
    data_source=ds,
    start_date='2024-01-01',
    end_date='2024-12-31',
    universe=universe_mask
)
```

**êµ¬í˜„ ì„¸ë¶€ì‚¬í•­:**

* `AlphaExcel.__init__()`ëŠ” `DataSource` ì¸ìŠ¤í„´ìŠ¤ë¥¼ í•„ìˆ˜ë¡œ ë°›ìŠµë‹ˆë‹¤
* `start_date`, `end_date`ëŠ” ë‚ ì§œ ë²”ìœ„ë¥¼ ì§€ì • (ë°ì´í„° ì¿¼ë¦¬ ë²”ìœ„)
* `universe` íŒŒë¼ë¯¸í„°ëŠ” ì„ íƒì  (Noneì´ë©´ returnsì—ì„œ ìžë™ íŒŒìƒ)
* Returns ë°ì´í„°ëŠ” ì´ˆê¸°í™” ì‹œ ìžë™ ë¡œë”© (`_load_returns()`)

---

### 3.2.2. í•µì‹¬ ë°ì´í„° ëª¨ë¸ êµ¬í˜„ (Core Data Model Implementation)

#### A. `DataContext` êµ¬í˜„ (`data_model.py`)

```python
import pandas as pd
from typing import Dict

class DataContext:
    """Dict-like container for pandas DataFrames with shared (dates, assets) index.

    All DataFrames stored in DataContext must have:
    - Index: pd.DatetimeIndex (dates)
    - Columns: pd.Index (assets)

    This ensures all data shares the same coordinate system.
    """

    def __init__(self, dates: pd.DatetimeIndex, assets: pd.Index):
        """Initialize DataContext with shared coordinate system.

        Args:
            dates: Time index (pd.DatetimeIndex)
            assets: Asset index (pd.Index)
        """
        self._dates = dates
        self._assets = assets
        self._data: Dict[str, pd.DataFrame] = {}

    def __getitem__(self, name: str) -> pd.DataFrame:
        """Get DataFrame by name."""
        if name not in self._data:
            raise KeyError(f"Data '{name}' not found in DataContext")
        return self._data[name]

    def __setitem__(self, name: str, value: pd.DataFrame) -> None:
        """Set DataFrame by name.

        Args:
            name: Data variable name
            value: pandas DataFrame with (dates, assets) index/columns
        """
        # Validate shape
        if not isinstance(value, pd.DataFrame):
            raise TypeError(f"Value must be pandas DataFrame, got {type(value)}")

        # Store (coordinates checked by caller)
        self._data[name] = value

    def __contains__(self, name: str) -> bool:
        """Check if name exists in DataContext."""
        return name in self._data

    @property
    def dates(self) -> pd.DatetimeIndex:
        """Get time index."""
        return self._dates

    @property
    def assets(self) -> pd.Index:
        """Get asset index."""
        return self._assets
```

**í•µì‹¬ íŠ¹ì§•:**
* âŒ **No xarray**: ìˆœìˆ˜ dict ê¸°ë°˜ DataFrame ì €ìž¥ì†Œ
* âœ… **Shared coordinates**: ëª¨ë“  DataFrameì´ ë™ì¼í•œ (dates, assets) ì¢Œí‘œê³„
* âœ… **Dict-like interface**: `ctx['name']` í˜•ì‹ìœ¼ë¡œ ì ‘ê·¼
* âœ… **Type safety**: DataFrameë§Œ ì €ìž¥ ê°€ëŠ¥

---

#### B. `AlphaExcel` í´ëž˜ìŠ¤ êµ¬í˜„ (`facade.py`)

**í•µì‹¬ íŒŒíŠ¸ 1: ì´ˆê¸°í™” ë° Returns ë¡œë”©**

```python
class AlphaExcel:
    def __init__(
        self,
        data_source: 'DataSource',
        start_date: str,
        end_date: Optional[str] = None,
        universe: Optional[Union[str, pd.DataFrame]] = None
    ):
        """Initialize AlphaExcel with DataSource and date range.

        Args:
            data_source: DataSource instance (MANDATORY)
            start_date: Start date (MANDATORY)
            end_date: End date (optional, None = all data from start_date)
            universe: Optional universe (str/DataFrame/None)
        """
        # Store parameters
        self._data_source = data_source
        self.start_date = start_date
        self.end_date = end_date

        # Load returns FIRST (mandatory)
        returns_data = self._load_returns()

        # Handle universe parameter
        if universe is not None:
            if isinstance(universe, str):
                raise NotImplementedError(
                    "String universe (e.g., 'univ100') not yet implemented"
                )
            elif isinstance(universe, pd.DataFrame):
                universe_mask = universe
                dates = pd.DatetimeIndex(universe_mask.index)
                assets = pd.Index(universe_mask.columns)
                returns_data = returns_data.reindex(index=dates, columns=assets)
            else:
                raise TypeError(f"universe must be str or DataFrame, got {type(universe)}")
        else:
            # Derive universe from returns
            dates = pd.DatetimeIndex(returns_data.index)
            assets = pd.Index(returns_data.columns)
            universe_mask = ~returns_data.isna()

        # Create data context
        self.ctx = DataContext(dates, assets)
        self.ctx['returns'] = returns_data

        # Initialize evaluator
        self._evaluator = EvaluateVisitor(self.ctx, data_source=data_source)

        # Store universe mask
        self._universe_mask = universe_mask

        # Initialize specialized components in evaluator (SRP ì ìš©)
        self._evaluator.initialize_components(
            universe_mask_df=universe_mask,
            returns_data=returns_data,
            start_date=start_date,
            end_date=end_date,
            buffer_start_date=self._buffer_start_date
        )

    def _load_returns(self) -> pd.DataFrame:
        """Load returns data from DataSource.

        Returns:
            Returns DataFrame with (time, asset) dimensions

        Raises:
            ValueError: If 'returns' field not found
        """
        try:
            loaded_data = self._data_source.load_field(
                'returns',
                start_date=self.start_date,
                end_date=self.end_date if self.end_date else self.start_date
            )
            # Convert to pandas if necessary
            if hasattr(loaded_data, 'to_pandas'):
                returns_data = loaded_data.to_pandas()
            else:
                returns_data = loaded_data
        except KeyError:
            raise ValueError(
                "Return data is mandatory. Missing 'returns' field in config/data.yaml"
            )

        return returns_data
```

**í•µì‹¬ íŒŒíŠ¸ 2: Expression í‰ê°€ (No add_data())**

```python
def evaluate(self, expr: Expression, scaler: Optional['WeightScaler'] = None) -> pd.DataFrame:
    """Evaluate Expression and return result.

    Args:
        expr: Expression to evaluate
        scaler: Optional WeightScaler for portfolio construction

    Returns:
        pandas DataFrame result

    Example:
        >>> result = rc.evaluate(TsMean(Field('returns'), 5))
        >>>
        >>> # With weight scaling and backtesting
        >>> result = rc.evaluate(expr, scaler=DollarNeutralScaler())
    """
    return self._evaluator.evaluate(expr, scaler)
```

**í•µì‹¬ íŒŒíŠ¸ 3: ìºì‹œ ê²°ê³¼ ì ‘ê·¼**

```python
def get_signal(self, step: int) -> tuple[str, pd.DataFrame]:
    """Get cached signal for a specific step."""
    return self._evaluator.get_cached_signal(step)

def get_weights(self, step: int) -> tuple[str, Optional[pd.DataFrame]]:
    """Get cached portfolio weights for a specific step."""
    return self._evaluator.get_cached_weights(step)

def get_port_return(self, step: int) -> tuple[str, Optional[pd.DataFrame]]:
    """Get cached position-level portfolio returns (T, N)."""
    return self._evaluator.get_cached_port_return(step)

def get_daily_pnl(self, step: int) -> Optional[pd.Series]:
    """Get daily PnL (T,) aggregated across assets."""
    _, port_return = self.get_port_return(step)
    if port_return is None:
        return None
    daily_pnl = port_return.sum(axis=1)
    return daily_pnl

def get_cumulative_pnl(self, step: int) -> Optional[pd.Series]:
    """Get cumulative PnL (T,) using cumsum."""
    daily_pnl = self.get_daily_pnl(step)
    if daily_pnl is None:
        return None
    cumulative_pnl = daily_pnl.cumsum()
    return cumulative_pnl
```

**í•µì‹¬ íŠ¹ì§•:**
* âŒ **No add_data()**: Expression í‰ê°€ë§Œìœ¼ë¡œ ì™„ê²°
* âœ… **Auto-loading**: Field ì°¸ì¡° ì‹œ visitorê°€ ìžë™ ë¡œë”©
* âœ… **Universe always set**: None ì²˜ë¦¬ ë¶ˆí•„ìš” (ì´ˆê¸°í™” ì‹œ ë³´ìž¥)
* âœ… **Triple-cache access**: get_signal, get_weights, get_port_return, get_daily_pnl, get_cumulative_pnl

---

#### C. `EvaluateVisitor` êµ¬í˜„ (SRP ì ìš©) (`visitor.py`)

**ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” (initialize_components)**

```python
def initialize_components(
    self,
    universe_mask_df: pd.DataFrame,
    returns_data: pd.DataFrame,
    start_date: str,
    end_date: str,
    buffer_start_date: str
):
    """Initialize specialized components after construction.

    This is called by AlphaExcel facade after visitor creation.
    """
    # Initialize UniverseMask
    self._universe_mask = UniverseMask(universe_mask_df)

    # Initialize StepTracker
    self._step_tracker = StepTracker()

    # Initialize FieldLoader if not already done
    if self._field_loader is not None:
        self._field_loader.set_universe_shape(
            universe_mask_df.index,
            universe_mask_df.columns
        )
        self._field_loader.set_date_range(start_date, end_date, buffer_start_date)

    # Initialize BacktestEngine
    self._backtest_engine = BacktestEngine(returns_data, self._universe_mask)
```

**Field ë°©ë¬¸ (visit_field) - ìœ„ìž„ íŒ¨í„´**

```python
def visit_field(self, node: Field) -> pd.DataFrame:
    """Visit Field node: delegate to FieldLoader.

    Workflow (SRP ì ìš©):
    1. FieldLoaderì— ë¡œë”© ìœ„ìž„ (ìºì‹±, ë³€í™˜, reindex ëª¨ë‘ í¬í•¨)
    2. UniverseMaskì— INPUT MASKING ìœ„ìž„
    3. StepTrackerì— signal ìºì‹± ìœ„ìž„
    4. Return masked result
    """
    # Step 1: FieldLoaderì— ë¡œë”© ìœ„ìž„
    if self._field_loader is None:
        if node.name not in self._ctx:
            raise RuntimeError(f"Field '{node.name}' not found in context")
        result = self._ctx[node.name]
    else:
        result = self._field_loader.load_field(node.name, node.data_type)

    # Step 2: UniverseMaskì— INPUT MASKING ìœ„ìž„
    result = self._universe_mask.apply_input_mask(result)

    # Step 3: StepTrackerì— ìºì‹± ìœ„ìž„
    self._step_tracker.record_signal(f"Field_{node.name}", result)

    # If scaler provided, compute weights and port_return
    if self._scaler is not None:
        self._cache_weights_and_returns(f"Field_{node.name}", result)

    # Step counter increment
    self._step_tracker.increment_step()

    return result
```

**ì—°ì‚°ìž ë°©ë¬¸ (visit_operator) - ìœ„ìž„ íŒ¨í„´**

```python
def visit_operator(self, node: Expression) -> pd.DataFrame:
    """Visit operator node with OUTPUT MASKING.

    Workflow (SRP ì ìš©):
    1. Traverse children (depth-first)
    2. Delegate to operator's compute() method
    3. UniverseMaskì— OUTPUT MASKING ìœ„ìž„
    4. StepTrackerì— ìºì‹± ìœ„ìž„
    5. Return masked result
    """
    # Step 1 & 2: Traverse and compute
    # (traverse logic unchanged - detect child/left/right)
    result = node.compute(child_result, visitor=self)

    # Step 3: UniverseMaskì— OUTPUT MASKING ìœ„ìž„
    result = self._universe_mask.apply_output_mask(result)

    # Step 4: StepTrackerì— ìºì‹± ìœ„ìž„
    operator_name = type(node).__name__
    self._cache_signal_weights_and_returns(operator_name, result)

    return result
```

**ìºì‹± ë¡œì§ (cache_signal_weights_and_returns) - ìœ„ìž„ íŒ¨í„´**

```python
def _cache_signal_weights_and_returns(self, name: str, signal: pd.DataFrame):
    """Cache signal, weights, and returns using StepTracker and BacktestEngine.

    SRP ì ìš©: ìœ„ìž„ë§Œ ìˆ˜í–‰, ì‹¤ì œ ê³„ì‚°ì€ ì „ë¬¸ ì»´í¬ë„ŒíŠ¸ì—ì„œ
    """
    # StepTrackerì— signal ê¸°ë¡
    self._step_tracker.record_signal(name, signal)

    # If scaler provided
    if self._scaler is not None:
        try:
            # Compute weights using scaler
            weights = self._scaler.scale(signal)
            self._step_tracker.record_weights(name, weights)

            # BacktestEngineì— portfolio return ê³„ì‚° ìœ„ìž„
            if self._backtest_engine is not None:
                port_return = self._backtest_engine.compute_portfolio_returns(weights)
                self._step_tracker.record_port_return(name, port_return)
            else:
                self._step_tracker.record_port_return(name, None)

        except Exception as e:
            # If scaling fails, cache None
            self._step_tracker.record_weights(name, None)
            self._step_tracker.record_port_return(name, None)

    # Increment step counter
    self._step_tracker.increment_step()
```

**í•µì‹¬ íŠ¹ì§• (SRP ì ìš© í›„):**
* âœ… **Single Responsibility**: VisitorëŠ” íŠ¸ë¦¬ ìˆœíšŒë§Œ, ë‚˜ë¨¸ì§€ëŠ” ì „ë¬¸ ì»´í¬ë„ŒíŠ¸ì— ìœ„ìž„
* âœ… **Testability**: ê° ì»´í¬ë„ŒíŠ¸ë¥¼ ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
* âœ… **Maintainability**: ì±…ìž„ ë¶„ë¦¬ë¡œ ì½”ë“œ ì´í•´ ë° ìˆ˜ì • ìš©ì´
* âœ… **Reusability**: ì»´í¬ë„ŒíŠ¸ë¥¼ ë‹¤ë¥¸ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ìž¬ì‚¬ìš© ê°€ëŠ¥

---

### 3.2.3. Serialization êµ¬í˜„ (`serialization.py`)

**SerializationVisitor: Expression â†’ dict**

```python
class SerializationVisitor:
    """Convert Expression tree to JSON-compatible dictionary."""

    def visit_field(self, node: Field) -> Dict[str, Any]:
        """Serialize Field node."""
        return {
            'type': 'Field',
            'name': node.name
        }

    def visit_operator(self, node: Expression) -> Dict[str, Any]:
        """Serialize operator node with type dispatch."""
        node_type = type(node).__name__
        result = {'type': node_type}

        # Handle different operator signatures
        if hasattr(node, 'left') and hasattr(node, 'right'):
            # Binary operator
            result['left'] = node.left.accept(self)
            result['right'] = node.right.accept(self)
        elif hasattr(node, 'child'):
            # Unary operator
            result['child'] = node.child.accept(self)

        # Add parameters (window, bins, labels, group_by, mask)
        for attr in ['window', 'bins', 'labels', 'group_by', 'mask']:
            if hasattr(node, attr):
                result[attr] = getattr(node, attr)

        return result

    def visit_constant(self, node: Constant) -> Dict[str, Any]:
        """Serialize Constant node."""
        return {
            'type': 'Constant',
            'value': node.value
        }
```

**DeserializationVisitor: dict â†’ Expression**

```python
class DeserializationVisitor:
    """Reconstruct Expression from dictionary."""

    @staticmethod
    def from_dict(data: Dict[str, Any]) -> Expression:
        """Deserialize dictionary to Expression.

        Args:
            data: Serialized Expression dictionary

        Returns:
            Reconstructed Expression object
        """
        node_type = data['type']

        # Field node
        if node_type == 'Field':
            return Field(data['name'])

        # Constant node
        if node_type == 'Constant':
            return Constant(data['value'])

        # Operator nodes (type dispatch)
        operator_class = _get_operator_class(node_type)

        # Reconstruct children
        if 'left' in data and 'right' in data:
            # Binary operator
            left = DeserializationVisitor.from_dict(data['left'])
            right = DeserializationVisitor.from_dict(data['right'])
            return operator_class(left, right)
        elif 'child' in data:
            # Unary operator
            child = DeserializationVisitor.from_dict(data['child'])

            # Extract parameters
            kwargs = {}
            for key in ['window', 'bins', 'labels', 'group_by', 'mask']:
                if key in data:
                    kwargs[key] = data[key]

            return operator_class(child, **kwargs)
        else:
            raise ValueError(f"Unknown operator structure for {node_type}")
```

**DependencyExtractor: Expression â†’ List[str]**

```python
class DependencyExtractor:
    """Extract field dependencies from Expression tree."""

    @staticmethod
    def extract(expr: Expression) -> List[str]:
        """Extract all Field dependencies.

        Args:
            expr: Expression to analyze

        Returns:
            List of unique field names
        """
        visitor = _DependencyVisitor()
        expr.accept(visitor)
        return sorted(set(visitor.fields))


class _DependencyVisitor:
    """Internal visitor for dependency extraction."""

    def __init__(self):
        self.fields = []

    def visit_field(self, node: Field) -> None:
        """Record field name."""
        self.fields.append(node.name)

    def visit_operator(self, node: Expression) -> None:
        """Recurse into children."""
        if hasattr(node, 'left'):
            node.left.accept(self)
        if hasattr(node, 'right'):
            node.right.accept(self)
        if hasattr(node, 'child'):
            node.child.accept(self)

    def visit_constant(self, node: Constant) -> None:
        """No dependencies for constants."""
        pass
```

**Expression í´ëž˜ìŠ¤ ë©”ì„œë“œ**

```python
class Expression:
    def to_dict(self) -> Dict[str, Any]:
        """Serialize Expression to dictionary."""
        visitor = SerializationVisitor()
        return self.accept(visitor)

    @staticmethod
    def from_dict(data: Dict[str, Any]) -> 'Expression':
        """Deserialize dictionary to Expression."""
        return DeserializationVisitor.from_dict(data)

    def get_field_dependencies(self) -> List[str]:
        """Extract field dependencies."""
        return DependencyExtractor.extract(self)
```

**í•µì‹¬ íŠ¹ì§•:**
* âœ… **JSON-compatible**: ëª¨ë“  Expressionì´ JSONìœ¼ë¡œ ì €ìž¥ ê°€ëŠ¥
* âœ… **Round-trip**: to_dict() â†’ from_dict() ì™•ë³µ ë³€í™˜
* âœ… **Dependency tracking**: í•„ìš”í•œ ë°ì´í„° í•„ë“œ ì¶”ì¶œ
* âœ… **Type safety**: íƒ€ìž… ê¸°ë°˜ ë””ìŠ¤íŒ¨ì¹˜ë¡œ ì•ˆì „í•œ ë³µì›

---

## 3.3. ì—°ì‚°ìž êµ¬í˜„ íŒ¨í„´

### 3.3.1. Arithmetic Operators (`ops/arithmetic.py`)

```python
@dataclass(eq=False)
class Add(Expression):
    """Addition operator (A + B)."""
    left: Expression
    right: Expression

    def accept(self, visitor):
        return visitor.visit_operator(self)

    def compute(self, left_result: pd.DataFrame, right_result: pd.DataFrame) -> pd.DataFrame:
        """Element-wise addition - pandas native."""
        return left_result + right_result
```

**í•µì‹¬ íŒ¨í„´:**
* âœ… **Dataclass**: `@dataclass(eq=False)` for Expression nodes
* âœ… **Generic accept**: All operators use `visitor.visit_operator(self)`
* âœ… **Operator-owned compute**: Calculation logic in `compute()` method
* âœ… **pandas operations**: Direct pandas DataFrame operations

---

### 3.3.2. Time-Series Operators (`ops/timeseries.py`)

```python
@dataclass(eq=False)
class TsMean(Expression):
    """Time-series rolling mean."""
    child: Expression
    window: int

    def accept(self, visitor):
        return visitor.visit_operator(self)

    def compute(self, child_result: pd.DataFrame) -> pd.DataFrame:
        """Apply rolling mean using pandas."""
        return child_result.rolling(window=self.window, min_periods=self.window).mean()
```

**í•µì‹¬ íŒ¨í„´:**
* âœ… **Window parameter**: Stored as dataclass field
* âœ… **Pandas rolling**: Direct use of pandas `.rolling()` API
* âœ… **min_periods**: Set to `window` for consistency (NaN until full window)

---

### 3.3.3. Cross-Sectional Operators (`ops/crosssection.py`)

```python
@dataclass(eq=False)
class Rank(Expression):
    """Cross-sectional rank operator."""
    child: Expression

    def accept(self, visitor):
        return visitor.visit_operator(self)

    def compute(self, child_result: pd.DataFrame) -> pd.DataFrame:
        """Apply cross-sectional rank using pandas."""
        return child_result.rank(axis=1, pct=True)
```

**í•µì‹¬ íŒ¨í„´:**
* âœ… **axis=1**: Cross-sectional operation (across columns)
* âœ… **pct=True**: Percentile ranking [0, 1]
* âœ… **NaN handling**: pandas automatically handles NaN in ranking

---

### 3.3.4. Constant Operator (`ops/constants.py`)

```python
@dataclass(eq=False)
class Constant(Expression):
    """Constant value expression."""
    value: float

    def accept(self, visitor):
        """Accept visitor for evaluation."""
        return visitor.visit_constant(self)
```

**visitor.pyì—ì„œ ì²˜ë¦¬:**

```python
def visit_constant(self, node: Constant) -> pd.DataFrame:
    """Create DataFrame filled with constant value."""
    # Create (T, N) DataFrame filled with constant
    result = pd.DataFrame(
        node.value,
        index=self._ctx.dates,
        columns=self._ctx.assets
    )

    # Apply universe masking
    result = result.where(self._universe_mask, np.nan)

    # Cache
    self._signal_cache[self._step_counter] = (f"Constant_{node.value}", result)
    self._step_counter += 1

    return result
```

---

## 3.4. Portfolio êµ¬í˜„ (`portfolio/strategies.py`)

### 3.4.1. GrossNetScaler (Unified Framework)

```python
class GrossNetScaler(WeightScaler):
    """Gross/Net exposure constraint scaler."""

    def __init__(self, target_gross: float, target_net: float):
        """Initialize with gross/net targets."""
        self.target_gross = target_gross
        self.target_net = target_net
        self.L_target = (target_gross + target_net) / 2
        self.S_target = (target_gross - target_net) / 2

    def scale(self, signal: pd.DataFrame) -> pd.DataFrame:
        """Scale signal to portfolio weights (vectorized)."""
        # Separate positive/negative signals
        s_pos = signal.where(signal > 0, 0)
        s_neg = signal.where(signal < 0, 0)

        # Normalize (handle 0/0 â†’ NaN â†’ 0)
        sum_pos = s_pos.sum(axis=1)
        sum_neg = s_neg.abs().sum(axis=1)

        norm_pos = s_pos.div(sum_pos, axis=0).fillna(0)
        norm_neg = s_neg.div(sum_neg, axis=0).fillna(0)

        # Apply targets
        weights = norm_pos * self.L_target - norm_neg.abs() * self.S_target

        # Scale to meet gross target
        actual_gross = weights.abs().sum(axis=1)
        scale_factor = (self.target_gross / actual_gross).replace([np.inf, -np.inf], 0).fillna(0)
        weights = weights.mul(scale_factor, axis=0)

        return weights
```

### 3.4.2. DollarNeutralScaler (Special Case)

```python
class DollarNeutralScaler(GrossNetScaler):
    """Dollar neutral scaler (Long=1.0, Short=-1.0)."""

    def __init__(self):
        super().__init__(target_gross=2.0, target_net=0.0)
```

---

## 3.5. ì‚¬ìš© ì˜ˆì‹œ ë° Best Practices

### 3.5.1. ê¸°ë³¸ ì›Œí¬í”Œë¡œìš°

```python
from alpha_database import DataSource
from alpha_excel import AlphaExcel, Field
from alpha_excel.ops.timeseries import TsMean
from alpha_excel.ops.crosssection import Rank

# 1. Initialize
ds = DataSource('config')
rc = AlphaExcel(ds, start_date='2024-01-01', end_date='2024-12-31')

# 2. Define Expression
expr = TsMean(Rank(Field('returns')), window=5)

# 3. Evaluate (auto-loading, auto-masking)
result = rc.evaluate(expr)

# 4. Inspect result
print(result.head())
print(result.shape)
```

### 3.5.2. ë°±í…ŒìŠ¤íŠ¸ ì›Œí¬í”Œë¡œìš°

```python
from alpha_excel.portfolio import DollarNeutralScaler

# 1. Evaluate with scaler (auto-backtest)
result = rc.evaluate(expr, scaler=DollarNeutralScaler())

# 2. Access cached results
for step in range(len(rc._evaluator._signal_cache)):
    name, signal = rc._evaluator.get_cached_signal(step)
    _, weights = rc._evaluator.get_cached_weights(step)
    _, port_return = rc._evaluator.get_cached_port_return(step)

    if weights is not None:
        daily_pnl = port_return.sum(axis=1)
        sharpe = daily_pnl.mean() / daily_pnl.std() * np.sqrt(252)
        print(f"Step {step} ({name}): Sharpe = {sharpe:.2f}")
```

### 3.5.3. Serialization ì›Œí¬í”Œë¡œìš°

```python
import json

# 1. Serialize Expression
expr = TsMean(Rank(Field('returns')), window=5)
expr_dict = expr.to_dict()

# 2. Save to file
with open('expression.json', 'w') as f:
    json.dump(expr_dict, f, indent=2)

# 3. Load from file
with open('expression.json', 'r') as f:
    loaded_dict = json.load(f)

# 4. Reconstruct Expression
from alpha_excel.core.expression import Expression
expr_loaded = Expression.from_dict(loaded_dict)

# 5. Extract dependencies
deps = expr_loaded.get_field_dependencies()
print(f"Required fields: {deps}")  # ['returns']
```

---

## 3.6. í…ŒìŠ¤íŠ¸ ì „ëžµ

### 3.6.1. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

```python
# Test operator compute logic
def test_add_operator():
    left = pd.DataFrame([[1, 2], [3, 4]])
    right = pd.DataFrame([[5, 6], [7, 8]])

    add = Add(Constant(0), Constant(0))  # Dummy children
    result = add.compute(left, right)

    expected = pd.DataFrame([[6, 8], [10, 12]])
    pd.testing.assert_frame_equal(result, expected)
```

### 3.6.2. í†µí•© í…ŒìŠ¤íŠ¸

```python
# Test full workflow
def test_alpha_excel_workflow():
    ds = DataSource('config')
    rc = AlphaExcel(ds, start_date='2024-01-01', end_date='2024-01-31')

    expr = TsMean(Field('returns'), window=5)
    result = rc.evaluate(expr)

    assert isinstance(result, pd.DataFrame)
    assert result.shape[0] > 0  # Has rows
    assert result.shape[1] > 0  # Has columns
```

### 3.6.3. Serialization í…ŒìŠ¤íŠ¸

```python
# Test round-trip
def test_serialization_round_trip():
    expr = TsMean(Rank(Field('returns')), window=5)
    expr_dict = expr.to_dict()
    expr_loaded = Expression.from_dict(expr_dict)

    # Should produce same results
    rc = AlphaExcel(ds, start_date='2024-01-01')
    result1 = rc.evaluate(expr)
    result2 = rc.evaluate(expr_loaded)

    pd.testing.assert_frame_equal(result1, result2)
```

---

## 3.7. ì„±ëŠ¥ ìµœì í™” íŒ

### 3.7.1. Vectorization

```python
# âœ… Good: Vectorized pandas operations
weights = signal.div(signal.abs().sum(axis=1), axis=0)

# âŒ Bad: Python loops
weights = signal.copy()
for i in range(len(signal)):
    row_sum = signal.iloc[i].abs().sum()
    weights.iloc[i] = signal.iloc[i] / row_sum
```

### 3.7.2. Caching

```python
# âœ… Good: Field auto-loading with caching
expr1 = TsMean(Field('returns'), 5)
expr2 = Rank(Field('returns'))
# 'returns' loaded only once, cached in DataContext

# âŒ Bad: Redundant loading (not possible in alpha-excel)
```

### 3.7.3. Memory Management

```python
# âœ… Good: On-demand PnL aggregation
daily_pnl = rc.get_daily_pnl(step)  # Computed when requested

# âŒ Bad: Pre-compute all aggregations
# (alpha-excel stores (T, N) port_return, aggregates on demand)
```

---

## 3.8. í–¥í›„ êµ¬í˜„ ê³„íš

### 3.8.1. Label Quantile (HIGH PRIORITY)

**ìš”êµ¬ì‚¬í•­:**
- Cross-sectional quantile ê¸°ë°˜ ê·¸ë£¹ ë¼ë²¨ë§
- Fama-French Factor ìŠ¤íƒ€ì¼ í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„±

**êµ¬í˜„ ê³„íš:**

```python
# TODO: Implement in ops/crosssection.py
@dataclass(eq=False)
class LabelQuantile(Expression):
    """Cross-sectional quantile labeling for group assignment.

    Example:
        # Size factor: [Small, Big]
        size_labels = LabelQuantile(Field('market_equity'), q=2, labels=['Small', 'Big'])

        # Value factor: [Low, Medium, High]
        value_labels = LabelQuantile(Field('be_me'), q=3, labels=['Low', 'Medium', 'High'])
    """
    child: Expression
    q: int
    labels: List[str]

    def accept(self, visitor):
        return visitor.visit_operator(self)

    def compute(self, child_result: pd.DataFrame) -> pd.DataFrame:
        """Apply cross-sectional qcut with labels.

        Returns:
            DataFrame with categorical labels (T, N)
        """
        # Apply qcut row-by-row (cross-sectional)
        result = child_result.apply(
            lambda row: pd.qcut(row, q=self.q, labels=self.labels, duplicates='drop'),
            axis=1
        )
        return result
```

**ì‚¬ìš© ì˜ˆì‹œ:**
```python
# Fama-French 2x3 portfolio construction
size_labels = LabelQuantile(Field('market_equity'), q=2, labels=['Small', 'Big'])
value_labels = LabelQuantile(Field('be_me'), q=3, labels=['Low', 'Medium', 'High'])

size_groups = rc.evaluate(size_labels)
value_groups = rc.evaluate(value_labels)

# Small-High portfolio
small_high_mask = (size_groups == 'Small') & (value_groups == 'High')

# Long-short strategy
long_mask = small_high_mask
short_mask = (size_groups == 'Big') & (value_groups == 'Low')
signal = long_mask.astype(float) - short_mask.astype(float)
```

---

### 3.8.2. String Universe (MEDIUM PRIORITY)

**êµ¬í˜„ ê³„íš:**

```python
# TODO: Implement in facade.py
if isinstance(universe, str):
    # Load universe from DataSource
    universe_data = self._data_source.load_field(universe, ...)
    universe_mask = universe_data.astype(bool)
```

**ì‚¬ìš© ì˜ˆì‹œ:**
```python
# Pre-defined universes in config/data.yaml
rc = AlphaExcel(ds, start_date='2024-01-01', universe='univ100')
```

---

### 3.8.3. Group Operations (MEDIUM PRIORITY)

**ìš”êµ¬ì‚¬í•­:**
- GroupNeutralize: ê·¸ë£¹ë³„ neutralization (industry neutral ë“±)
- GroupDemean: ê·¸ë£¹ë³„ demean

**êµ¬í˜„ ê³„íš:**

```python
# TODO: Implement in ops/group.py
@dataclass(eq=False)
class GroupNeutralize(Expression):
    """Group-wise neutralization.

    Example:
        # Industry-neutral signal
        signal = GroupNeutralize(
            Rank(Field('returns')),
            group_by=Field('industry')
        )
    """
    child: Expression
    group_by: Expression  # Group labels (e.g., industry)

    def accept(self, visitor):
        return visitor.visit_operator(self)

    def compute(self, child_result: pd.DataFrame, group_labels: pd.DataFrame) -> pd.DataFrame:
        """Apply group-wise neutralization."""
        # Implementation requires groupby logic
        pass
```

---

## 3.9. ìš”ì•½

### êµ¬í˜„ ì™„ë£Œ í•­ëª©

âœ… **Core Infrastructure**
- DataContext (dict-like pandas storage)
- AlphaExcel Facade (auto-loading, no add_data)
- EvaluateVisitor (auto-loading, double masking, triple-cache)

âœ… **Expression System**
- Expression interface
- Field, Constant nodes
- Arithmetic, Logical, Time-Series, Cross-Sectional operators

âœ… **Portfolio System**
- WeightScaler base class
- GrossNetScaler, DollarNeutralScaler, LongOnlyScaler
- Shift-mask backtesting workflow

âœ… **Serialization**
- Expression to/from JSON dict
- Dependency extraction

### í–¥í›„ êµ¬í˜„ ì˜ˆì •

ðŸ”œ **High Priority**
- Label Quantile (Fama-French Factor)

ðŸ”œ **Medium Priority**
- String Universe ('univ100', 'univ200')
- Group Operations (GroupNeutralize, GroupDemean)

---
